{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to UAB Research Computing","text":"<p>The Research Computing System (RCS) provides a framework for sharing research data, accessing computing power, and collaborating with peers on campus and around the globe. Our goal is to construct a dynamic \"network of services\" that you can use to organize, study and share your research data. Research Computing provides services to researchers in these core areas:</p> <ul> <li>Data Analysis: using the High Performance Computing (HPC) fabric Cheaha for batch data processing.</li> <li>Data Sharing: supporting trusted information exchange to spark new ideas via our Storage system.</li> <li>Application Development: providing virtual machines and web-hosted development tools empowering researcher via our cloud.rc fabric.</li> </ul> <p>Announcement</p> <p>We have released new A100 gpus on Cheaha! For more information please see GPUs.</p> <p>We have released new CUDA and cuDNN modules! For more information please see CUDA Modules.</p> <p>Also see our A100 GPU Frequently Asked Questions (FAQ)</p>"},{"location":"#how-to-contact-us","title":"How to Contact Us","text":"<p>Please reach out to us via email at support@listserv.uab.edu to create a support ticket.</p> <p>For face-to-face support please visit us in our Zoom office hours held weekly:</p> <ul> <li> <p>Mondays 10:00 AM to 12:00 PM: Zoom</p> </li> <li> <p>Thursdays 10:00 AM to 12:00 PM: Zoom</p> </li> </ul> <p>For additional information please see Support.</p>"},{"location":"#support-and-development","title":"Support and Development","text":"<p>RCS is developed and supported by UAB IT's Research Computing Group. We are developing a core set of applications to help you to easily incorporate our services into your research processes and this documentation collection to help you leverage the resources already available. We follow the best practices of the Open Source community and develop our software in an open-source fashion.</p> <p>RCS is an out growth of the UABgrid pilot, launched in September 2007 which has focused on demonstrating the utility of unlimited analysis, storage, and application for research. RCS is built on the same technology foundations used by major cloud vendors and decades of distributed systems computing research, technology that powered the last ten years of large scale systems serving prominent national and international initiatives like the Open Science Grid, XSEDE, the LHC Computing Grid, and NCIP.</p>"},{"location":"#outreach","title":"Outreach","text":"<p>The UAB IT Research Computing Group has collaborated with a number of prominent research projects at UAB to identify use cases and develop requirements. Our collaborators include, but are not limited to, the Center for Clinical and Translational Science (CCTS), Heflin Genomics Center, the Comprehensive Cancer Center (CCC), the Department of Computer and Information Sciences (CIS), the Department of Mechanical Engineering (ME), Lister Hill Library, the School of Optometry's Center for the Development of Functional Imaging, and Health System Information Services (HSIS).</p> <p>As part of the process of building RC, the UAB IT Research Computing Group has hosted an annual campus symposium on research computing and cyber-infrastructure (CI) developments and accomplishments. Starting as CyberInfrastructure (CI) Days in 2007, the name was changed to <code>UAB Research Computing Day</code> in 2011 to reflect the broader mission to support research. IT Research Computing also participates in other campus wide symposiums including UAB Research Core Day.</p>"},{"location":"policies/","title":"UAB Research Computing Policies","text":""},{"location":"policies/#uab-it-policies","title":"UAB IT Policies","text":"<p>All users of UAB IT systems, including UAB Research Computing systems, must agree to all UAB relevant UAB IT policies. UAB IT policies may be found at https://www.uab.edu/it/home/policies.</p> <p>The list of policies below are those that may be most relevant to typical use cases on UAB Research Computing systems. The list below is not an exhaustive list, and it is the responsibility of each individual using the system to be aware of and follow all relevant policies, whether listed here or not.</p>"},{"location":"policies/#acceptable-use-policy-aup","title":"Acceptable Use Policy (AUP)","text":"<p>Acceptable Use Policy</p>"},{"location":"policies/#data-classification","title":"Data Classification","text":"<p>Data Classification</p>"},{"location":"account_management/","title":"Account Management","text":""},{"location":"account_management/#user-accounts-and-responsibilities","title":"User Accounts and Responsibilities","text":"<p>Research Computing at UAB provides shared resources for researchers, where responsible and professional use of Cheaha accounts, storage, and computational resources is expected to maintain system integrity and support collaborative research. Please refer below for the eligibility requirements and responsibilities of Research Computing services users.</p> Category User Group Requirements Responsibilities UAB Employees UAB Campus and UAB Medicine Faculty/Staff and Postdocs Must have an active BlazerID Refer to All User Responsibilities.For Lab PIs and Core Directors please refer Lab PIs responsibilities. UAB Students All UAB Campus and UAB Medicine Students Must have an active BlazerID Refer to All User Responsibilities. External Collaborators All sponsored by UAB Employee Must be sponsored by UAB employee Must have XIAS email address Collaborators' Responsibility:\u00a0  - Refer to All User Responsibilities.\u00a0  - Create a XIAS Guest Account. Sponsor's Responsibility:\u00a0  - Sponsor is accountable for collaborators actions on UAB systems.\u00a0  - Please refer to external collaborators page. Lab PIs All Lab PIs and Core Directors Must have an active BlazerID Data Management and Storage:\u00a0  - Periodically check group membership is correct.\u00a0  - Periodically check access controls to directories/buckets are correct.\u00a0  - Moving unused data to LTS or external archival solutions.\u00a0  - Managing backup plans.OpenStack Projects:\u00a0  - Periodically check group membership is correct.\u00a0  - Periodically check unused resources are released.PIs are also responsible to:\u00a0  - Regularly reviewing membership permissions and access control.\u00a0  - Ensure students are aware about FERPA-protected project metadata.\u00a0  - Refer to All User Responsibilities."},{"location":"account_management/#all-users-responsibilities","title":"All Users Responsibilities","text":"<ul> <li>Regularly clearing <code>/scratch</code>.</li> <li>Adherence to UAB IT policies.</li> <li>Request computational resources reasonably. Refer to our compute resource estimation.</li> <li>Responsible Conduct of Research training.</li> <li>Annual account certification of Cheaha account.</li> <li>All Cheaha account users working with Protected Health Information (PHI) data are responsible to review HIPAA policies and HIPAA training.</li> </ul>"},{"location":"account_management/#how-do-i-create-a-research-computing-account","title":"How do I create a Research Computing account?","text":"<ul> <li>Cheaha Account: Creating a Cheaha account is currently a self-service process. Both UAB employees, students, and external collaborators can create a Cheaha account by visiting creating a Cheaha account page.</li> <li>GitLab Account: If you are a UAB-affiliated researcher and have a BlazerID, you may create an account by visiting GitLab registration steps for UAB Employees and Students. If you are a collaborator with a XIAS account, you will need to follow the procedure in External Collaborators registration.</li> <li>Cloud.rc Account: To get your Cloud.rc account, please contact Support. For detail information on how to access it, please visit our Cloud.rc page.</li> </ul>"},{"location":"account_management/#how-do-i-login-to-research-computing-services","title":"How do I login to Research Computing Services?","text":"<ul> <li> <p>Cheaha: To login to Cheaha:</p> <ul> <li>UAB Employees and Students: use your BlazerID.</li> <li>External Collaborators: use your XIAS email address.</li> <li>For detail login information, please visit accessing Cheaha page.  If accessing through Open OnDemand, our online portal, Duo 2FA may be required.</li> </ul> </li> <li> <p>GitLab: To login  to the GitLab, the UAB Employees and Students require a BlazerID, while External Collaborators require XIAS email. For detail login information visit the UAB Employees and Students GitLab Registration and External Collaborators GitLab Registration pages respectively.</p> </li> <li> <p>Cloud.rc: To access the Cloud.rc, you must be on the campus network. For off-campus access, use the UAB Campus VPN, which requires Duo 2FA. UAB employees and students can log in using their BlazerID, while External Collaborators use their XIAS email. For login details, visit our Cloud.rc page.</p> </li> </ul> <p>If you are unable to find what you need, please contact our team here.</p>"},{"location":"account_management/cheaha_account/","title":"Cheaha Account Management","text":"<p>These instructions are intended to guide researchers on creating new accounts and managing existing accounts.</p>"},{"location":"account_management/cheaha_account/#creating-a-new-account","title":"Creating a New Account","text":"<p>Creating a new account is a simple, automated, self-service process. To start, navigate to https://rc.uab.edu, our Open OnDemand web portal, and authenticate. The authentication process differs depending on your affiliation. Accounts are available to researchers with the following situations.</p> <ul> <li>If you are affiliated with UAB and have a BlazerID, please authenticate using Single Sign-On (SSO).</li> <li>If you are affiliated with UAB Medicine, you will need to use your BlazerID to authenticate via Single Sign-On (SSO) instead of your UABMC authentication process.</li> <li>If you are an external collaborator and have a XIAS account with access to Cheaha, please authenticate using your XIAS email address as the ID, not automatically generated <code>xias-XXXXXX-1</code> ID.</li> <li>If you are an external collaborator and do not have a XIAS account, you will need a UAB-affiliated sponsor and will need to follow our XIAS Guest Account Instructions. Your sponsor will need to follow our XIAS Site Management and XIAS Guest Management documentation pages.</li> </ul> <p>Once you have authenticated, you should see a page that looks like the following.</p> <p></p> <p>The form should be prefilled with your BlazerID or XIAS ID, full name, and email address. If any of these details are incorrect please Contact Support. Please also fill out the reason you wish to create an account. To create a Cheaha account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable.</p> <p>When you are ready, click \"Create Account\" to start the account creation process. You should see a popup notification that looks like the following. After a few moments you should be redirected to our Open OnDemand web portal. If not, please Contact Support.</p> <p></p> <p>Welcome to Cheaha and to Research Computing!</p>"},{"location":"account_management/cheaha_account/#managing-an-existing-account","title":"Managing an Existing Account","text":"<p>If you already have an account and wish to check it's status, please visit your account status page at https://rc.uab.edu/account.</p>"},{"location":"account_management/cheaha_account/#account-in-good-standing","title":"Account in Good Standing","text":"<p>If your account is in good standing you should see a page like the following.</p> <p></p>"},{"location":"account_management/cheaha_account/#account-requires-certification","title":"Account Requires Certification","text":"<p>We review accounts periodically to ensure the system is being used fairly and as intended. Part of this process is to certify that researchers with accounts still wish to make use of Cheaha. Once per year every researcher will be required to certify their account before making use of Cheaha. If your account requires certification, when logging in to our Open OnDemand web portal you will see the following notification page.</p> <p>.</p> <p>To certify your account, click the button to be taken to the certification form, which should look like the following.</p> <p></p> <p>While the certification form looks similar to the new account creation form, please be sure to review all of the information carefully. To certify your account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable.</p> <p>When you are ready, click \"Certify Account\" to start the account creation process. You should see a popup notification confirming the process is working. After a few moments your account should be certified and you will be free to use Cheaha again. If not, please Contact Support.</p>"},{"location":"account_management/cheaha_account/#account-on-hold","title":"Account on Hold","text":"<p>Mistakes happen, and sometimes what we thought we programmed wasn't quite what we actually programmed. When these kinds of mistakes occur, excess resources may get used. If this impacts performance or other users excessively, we may put a hold on your account. We may also put a hold on your account if you do not complete Account Certification when required.</p> <p>Other reasons for holds include, but are not limited to:</p> <ul> <li>Misuse (intentional or not) of Research Computing resources</li> <li>IT Policy violations</li> <li>HIPAA or FERPA violations related to use of Cheaha</li> <li>As part of a required investigation</li> </ul> <p>In rare circumstances, we may also place a hold on your account if you possess the sole copy of data not owned by you.</p> <p>If your account is on hold, you will see a page like the following.</p> <p></p> <p>If you SSH into the cluster while your account is on hold you will see the following text in your terminal.</p> <p></p> <p>If your account is on hold and we have not already contacted you, or you believe the hold to be in error, please Contact Support.</p>"},{"location":"account_management/cheaha_account/#authorization-error","title":"Authorization Error","text":"<p>Periodically, we review all researcher accounts to ensure they are authorized to use Cheaha based on affiliation status. If we find a researcher is no longer affiliated with UAB, we may disable the account. If you are not authorized to use Cheaha, you will see a page like the following.</p> <p></p> <p>If you believe this to be in error, please Contact Support.</p>"},{"location":"account_management/cheaha_account/#what-can-i-do-with-my-account","title":"What can I do with my account?","text":"<p>Research Computing offers services addressing a wide range of needs for researchers at UAB, including students, staff, and faculty, as well as for both Labs and research cores.</p> <p>We're always happy to provide support for your Research Computing needs, you need only Contact Support.</p>"},{"location":"account_management/cheaha_account/#for-students-staff-and-faculty","title":"For Students, Staff, and Faculty","text":"<ul> <li>Get Started with Open OnDemand</li> <li>Additional Learning Resources</li> <li>Data Science Journal Club Course</li> </ul>"},{"location":"account_management/cheaha_account/#for-lab-pis-and-core-directors","title":"For Lab PIs and Core Directors","text":"<ul> <li>No-cost storage offerings<ul> <li>GPFS: Hot storage, compute adjacent, directly accessible from Cheaha</li> <li>LTS: Cool storage, large capacity</li> <li>Transfer data with Globus</li> </ul> </li> <li>Batch computing<ul> <li>Desktop, Jupyter, RStudio, Matlab, and more</li> <li>GPUs</li> </ul> </li> <li>On-prem cloud computing<ul> <li>Tutorial</li> <li>Web servers</li> </ul> </li> </ul> <p>If you are unable to find what you need, please feel free to Contact Support.</p>"},{"location":"account_management/cheaha_account/#cheaha-account-and-group-membership-faq","title":"Cheaha Account and Group Membership FAQ","text":"<p>Our Cheaha system is robust, but errors may occur due to general platform connectivity issues or missing components. Below are FAQs for self-service Cheaha account creation and a troubleshooting guide for common issues:</p> <ul> <li>Which credentials should I use? Please visit How Do I Login to Research Computing Services.</li> <li> <p>What do I do if I'm waiting for it to finish for longer than a couple of minutes?</p> <ul> <li>Try closing and restarting your browser, then trying again.</li> <li>Try clearing site data for https://rc.uab.edu, then trying again.</li> <li>Try logging in on a Private Browsing window, then trying again.</li> <li>Try waiting a few hours, then trying again.</li> </ul> </li> <li> <p>What should I do to access shared storages and recognize my group membership after being added to a group on Cheaha?</p> <ul> <li> <p>Do you have any processes/connections on <code>cheaha.rc.uab.edu</code>?</p> <ul> <li>Please exit and log back in.</li> <li>If you have active Tmux/Screen sessions, you will need to terminate those as well, log out, log back in and start Tmux.</li> </ul> </li> <li> <p>Do you have an active Open OnDemand session?</p> <ul> <li>In Open OnDemand (https://rc.uab.edu), navigate to the green navigation bar in the top right corner. Look for the <code>Help</code> or <code>Developer</code> dropdown menu and click on it. Then, click <code>Restart Web Server</code>. Once the restart is complete, please try again.</li> </ul> </li> <li> <p>Do you have one or more OOD HPC Desktops running?</p> <ul> <li>Terminate the desktops and start new ones.</li> </ul> </li> </ul> </li> </ul>"},{"location":"account_management/gitlab_account/","title":"UAB GitLab Overview and Registration","text":"<p>Welcome to UAB GitLab! This is a UAB-specific GitLab. GitLab is similar to GitHub, but hosted here at UAB on secure servers.</p>"},{"location":"account_management/gitlab_account/#gitlab-use-cases","title":"GitLab Use Cases","text":""},{"location":"account_management/gitlab_account/#for-researchers","title":"For Researchers","text":"<p>GitLab can be used:</p> <ul> <li>For reproducibility<ul> <li>Analysis and software code can be kept in one, central repository everyone can use instead of spread across multiple computers/places.</li> <li>Code can be versioned and tracked as it changes over time.</li> <li>Software versions can be recorded, virtual environments can be documented, and containers can be recorded to help future-proof analyses.</li> </ul> </li> <li>Collaboration<ul> <li>GitLab is a central place to create code, edit, and track needed code changes (issues) with your lab and collaborators.</li> <li>Multiple people can use, modify, and merge changes in code while communicating with the broader team all along the way.</li> </ul> </li> <li>Security<ul> <li>Unlimited private repositories for internal code projects.</li> <li>Set behind UAB authentication.</li> </ul> </li> </ul>"},{"location":"account_management/gitlab_account/#for-software-developers-and-researchers","title":"For Software Developers (and Researchers!)","text":"<p>UAB GitLab is useful for software developers. It is a single application for the entire software development lifecycle. From project planning and source code management to continuous integration (CI) and continuous deployment (CD), monitoring, and security.</p> <p>Our GitLab instance may be found at https://gitlab.rc.uab.edu.</p>"},{"location":"account_management/gitlab_account/#uab-gitlab-registration","title":"UAB GitLab Registration","text":""},{"location":"account_management/gitlab_account/#uab-affiliated-researcher-registration","title":"UAB-Affiliated Researcher Registration","text":"<p>If you are a UAB affiliated researcher and have a BlazerID, you may create an account by logging in at the site above using the <code>ldap</code> tab. Please use your single sign-on (SSO) credentials.</p> <p>Please use BlazerID and password instead of UABMC credentials</p> <p>Please use your BlazerID and BlazerID password for UAB GitLab. UABMC credentials are a different sign in system and will likely not work. Central IT groups like Research Computing do not have a way to access UABMC credentials.</p> <p></p>"},{"location":"account_management/gitlab_account/#uabmc-researcher-registration","title":"UABMC Researcher Registration","text":"<p>Please use your BlazerID and BlazerID credentials to sign in following the directions for UAB-Affiliated Researchers. UABMC credentials should not be used for UAB GitLab.</p>"},{"location":"account_management/gitlab_account/#xias-external-collaborator-registration","title":"XIAS External Collaborator Registration","text":"<p>If you are a collaborator with a XIAS account you'll need to follow a different procedure.</p> <ol> <li>Ensure that your sponsor has included <code>https://gitlab.rc.uab.edu</code> in the list of approved URIs on the XIAS configuration page.</li> <li>Email support@listserv.uab.edu providing your full name, XIAS account email address, and sponsor.</li> <li>UAB Research Computing will create the account.</li> <li>You will recieve an email from gitlab.rc.uab.edu with a link to create a password.</li> <li>Navigate to https://gitlab.rc.uab.edu.</li> <li>Click the <code>Standard</code> tab.</li> <li>In the <code>Username or email</code> field type the part of your XIAS email address before the <code>@</code> symbol. Do not include the <code>@</code> symbol or anything after it.</li> <li>Fill out the <code>Password</code> field with the GitLab password you created in Step #4.</li> <li>Click <code>Sign in</code>.</li> </ol> <p></p> <p>Warning</p> <p>XIAS account researchers can only be granted access if their sponsor adds the GitLab URL to the list of approved URIs. Please see XIAS Sites for more information.</p>"},{"location":"account_management/xias/","title":"External Collaborator (XIAS) Accounts","text":"<p>This segment of the docs has instructions for managing and creating XIAS accounts for external collaborators.</p> <p>External collaborators require XIAS accounts to access Cheaha. Obtaining an account requires a UAB-employed sponsor, typically a research PI, who will claim responsibility for the external collaborator. The XIAS account creation process is initiated by the sponsor, not by the external collaborator.</p> <p>An overview of creating an external collaborator account:</p> <ol> <li>The sponsor must first:<ol> <li>Create and manage XIAS Sites</li> <li>Create and manage XIAS Guests</li> </ol> </li> <li>The guest must then Create a XIAS Guest Account</li> </ol> <p>We recommend the sponsor and guest stay in close contact during the process in case anything unexpected occurs. If you encounter difficulties with any part of the XIAS website process, please contact AskIT. If you encounter difficulties with the Cheaha account creation portion at the Open OnDemand web portal, please contact Support</p>"},{"location":"account_management/xias/#what-is-next-and-how-to-login-to-services","title":"What is next and how to login to services?","text":"<p>To login to the Cheaha and GitLab services, you need to use your XIAS email address and the password used to create your XIAS account. To access the Cloud.rc, you first need to request an account. Please follow Cloud.rc to initiate the request.</p> <p>For further guidance on how to proceed and what steps to take next, please refer to the following documentation pages.</p> <p>Warning</p> <p>Before proceeding with any of the resources below, be sure you have been granted authorization to those particular resources by your Sponsor.</p> <ul> <li>How do I create a Cheaha account?</li> <li>How do I get access to GitLab?</li> <li>How do I get access to Cloud.rc (OpenStack)?</li> </ul>"},{"location":"account_management/xias/guest_instructions/","title":"Guest Instructions","text":"<p>These instructions are for guests who have been registered by UAB faculty and staff to use internal UAB resources. Once a request for a XIAS account has been made by your UAB sponsor, you will need to follow these instructions to complete the XIAS registration and obtain access to UAB resources. All of the links used on this page are available at the UAB XIAS Guest Users page.</p>"},{"location":"account_management/xias/guest_instructions/#create-account","title":"Create Account","text":"<ol> <li> <p>The first email you receive should be a notification that a request has been made to add you as a XIAS user. This email will include the project(s)/site(s) you're being added to.</p> <p></p> </li> <li> <p>The next email you receive should contain instructions on how to register your account. This email may take an hour or so to arrive after the first. It will contain an invite code that you must enter at the XIAS website, along with the email address used to register you.</p> <p></p> </li> <li> <p>Navigate to the link in the email. Please practice good internet hygiene and copy the link text, instead of clicking the link! As of the time of writing the link will be to the UAB XIAS Guest Users page. Once at the main page, click the \"Enter Invite or Reset Code\" link.</p> <p></p> </li> <li> <p>You will be taken to the \"Register XIAS Account\" page. Enter the email address used to register you for a XIAS account, and the code from the email you received with registration instructions. Then click proceed.</p> <p></p> </li> <li> <p>Enter your first and last names, then click proceed.</p> <p></p> </li> <li> <p>Enter a password that will be used with your XIAS account. This password can be changed later, and your account can be recovered if the password is lost. Click proceed.</p> <p></p> </li> <li> <p>You will be taken to a confirmation page. If everything is acceptable, click proceed. Otherwise click edit next to the incorrect field. Your XIAS email cannot be changed. If the email is not correct you will need to communicate with your sponsor to start the entire process over from the beginning.</p> <p></p> </li> <li> <p>You should be taken to a page indicating success. Please carefully read the page and follow any instructions. If you do not see a success page, please contact your sponsor about next steps.</p> <p></p> <p>Following this step, your account registration is complete and you should be able to access the resources you have been granted permission to use. Most internal UAB systems use a Single Sign-On (SSO) to simplify and standardize logging in. For those sites that don't you will need to activate your account manually.</p> </li> <li> <p>To manually activate accounts for resources that do not use SSO click the \"Activate (Sync) Accounts\" link on the left hand navigation pane. Fill out the form using the email used to register the XIAS account and the current password.</p> <p></p> </li> </ol>"},{"location":"account_management/xias/guest_instructions/#required-software-for-research-computing-access","title":"Required Software for Research Computing Access","text":"<p>Research computing software requires security software be installed on your devices in order to login. Duo two-factor authentication (2FA) software is required on your mobile to device to access any Single Sign-on services. VPN access software is required for some services when connected from outside the UAB internal network. Accessing the VPN also requires Duo 2FA.</p> <p>Below is a list of Research Computing services and their required software.</p> <ul> <li>Cheaha: 2FA</li> <li>Cloud.rc: VPN and 2FA if off-campus</li> </ul>"},{"location":"account_management/xias/guest_instructions/#change-password-and-recover-from-lost-password","title":"Change Password and Recover From Lost Password","text":"<p>To change your password, or recover your account in case of a lost password, please click the \"Change XIAS Password\" link in the left hand panel of the main page. Once there, follow the instructions on the form.</p> <p></p>"},{"location":"account_management/xias/guest_instructions/#resend-invite-code","title":"Resend Invite Code","text":"<p>If your invite code has expired, you can have a new invite code sent to you by clicking the \"Resend Invite Code\" link in the left hand panel of the main page. Once there, follow the instructions on the form.</p> <p></p>"},{"location":"account_management/xias/guest_instructions/#guest-it-info","title":"Guest IT Info","text":"<p>For more information on UAB IT policies and other useful and helpful information, please click the \"UABIT Guest User info\" link.</p> <p></p>"},{"location":"account_management/xias/pi_guest_management/","title":"Managing UAB XIAS Users","text":"<p>Note</p> <p>These instructions are intended for use by UAB-employed PIs to organize external collaborators, also known as guests. UAB PIs: Please direct guests here for instructions on creating their accounts.</p> <p>Important</p> <p>To complete these steps you will first need to createa a Project/Site</p> <p>UAB XIAS User management allow UAB faculty and staff to grant external collaborators access to specific resources on the internal UAB Campus Network. All XIAS users must be connected with at least one site, so you'll need to create one at the UAB XIAS User Management Webpage. All XIAS Users must also have an expiration date.</p>"},{"location":"account_management/xias/pi_guest_management/#adding-users","title":"Adding Users","text":"<p>Before adding users, have a list of user emails handy for the site you wish to add users to, as well as expiration dates for each user. You will need to create a Project/Site before you can add external collaborators.</p> <ol> <li> <p>To start go to the UAB XIAS User Management Webpage and click Manage Users in the left menu.</p> </li> <li> <p>Select the Project/Site you wish to add users to from the drop down box.</p> <p></p> </li> <li> <p>Click \"Register\" to open a form for adding new users.</p> <p></p> </li> <li> <p>Fill in the form. All fields are required.</p> <ol> <li>Checkbox list - Leave the site checked.</li> <li>End date - An expiration date for the users being added. Cannot be longer than the end date for the selected Project/Site.</li> <li>Text box - Enter a list of e-mail addresses for users to add.</li> </ol> <p></p> </li> <li> <p>Click \"Submit\" to move to a confirmation page.</p> <p></p> </li> <li> <p>Check the emails are correct and click \"Add\" to submit the information Emails will be sent to all email addresses for next steps. You will be redirected to the UAB XIAS User Management Webpage, which should now have the text \"Registration successful.\" near the top.</p> <p></p> </li> <li> <p>To complete their registration, please direct your external collaborators to the UAB XIAS Guest Users page. When they have completed their registration, you should receive an email like the following.</p> <p></p> </li> <li> <p>Once the guest XIAS account has been created, the guest will need to login at https://rc.uab.edu and follow the automated Cheaha Account Creation Process to create a Cheaha account. They will need to use the same email and password they used when creating their XIAS account.</p> </li> </ol>"},{"location":"account_management/xias/pi_guest_management/#discovering-and-managing-users","title":"Discovering and Managing Users","text":"<p>There are two ways to discover XIAS users you are currently sponsoring. The first is to search by email address. The second is to list all users associated with a site.</p>"},{"location":"account_management/xias/pi_guest_management/#discovering-users","title":"Discovering Users","text":"<ol> <li> <p>To locate users by e-mail address: type their email into the \"Locate specific user(s) by e-mail address\" text field on the \"Manage Users\" page.</p> <p></p> </li> <li> <p>To manage users by site: select the site from the drop-down box and click the \"List\" button. The page will reload with a table containing name, email, and start and end dates. The end date is when the XIAS user registration expires. To change the end date for user(s), click the \"Sel\" checkbox next to their names, enter a date in the \"Change end date for selected users to\" text field, and click \"Update\".</p> <p></p> </li> </ol>"},{"location":"account_management/xias/pi_guest_management/#revoking-user-privileges","title":"Revoking User Privileges","text":"<p>Warning</p> <p>THIS INFORMATION IS PENDING TESTING</p> <p>Users cannot have their XIAS account deleted. However, privileges may be revoked. To revoke user privileges, follow the instructions for managing users by site. Update the desired user(s)' end date to a date earlier than the current date.</p> <p>Important</p> <p>If you need to urgently revoke privileges, please also notify UAB IT by emailing AskIT@uab.edu as soon as possible. Please be clear about what is needed and when.</p>"},{"location":"account_management/xias/pi_site_management/","title":"Creating a UAB XIAS Project/Site","text":"<p>Note</p> <p>These instructions are intended for use by UAB-employed PIs to organize external collaborators, also known as guests. UAB PIs: Please direct guests here for instructions on creating their accounts.</p> <p>XIAS Project/Sites, or simply sites, tie external users to specific resources at UAB. By connecting people to the resource they use, UAB can maintain security and accountability. Creating a site is the first step to giving access to external collaborators, and the process can be thought of as \"create once, use many times\". All sites must have an expiration date for security reasons. To create a site you'll need at least one Uniform Resource Identifier (URI) relating to resources used by the site. If you aren't sure what URI(s) to list for your site, please contact UserServices@uab.edu.</p> <ol> <li> <p>To start go to the UAB XIAS Project/Site Management Webpage.</p> <p></p> </li> <li> <p>Click \"New\" to open a form for creating a new Project/Site.</p> <p></p> </li> <li> <p>Fill in the form. All fields are required.</p> <ol> <li>Short name for project/site - A memorable name for your project or site.</li> <li>Longer description - A complete yet concise description of the project or site and its resources.</li> <li>Start date - The start date, can be today.</li> <li>End date - An expiration date for the project or site.</li> <li> <p>URIs - One or more uniform resource locators (URIs) associated with the site, to increase accountability.</p> <ol> <li>Cheaha URI: <code>https://rc.uab.edu</code></li> <li>Cloud URI: <code>https://cloud.rc.uab.edu</code></li> <li>GitLab URI: <code>https://gitlab.rc.uab.edu</code></li> </ol> </li> </ol> <p></p> </li> <li> <p>Click \"Add\" to submit the form. You should be taken to a page summarizing the created Project/Site.</p> <p></p> </li> <li> <p>When you visit the \"Manage Projects/Sites\" page in the future, you will see a table with the newly created Project/Site listed. Click \"View\" to return to the page seen in the previous step. Click \"Edit\" to return to the form from [link]. Click \"Users\" to manage users for this site.</p> <p></p> </li> </ol>"},{"location":"cheaha/getting_started/","title":"Getting Started","text":"<p>Cheaha is a High Performance Computing (HPC) resource intended primarily for batch processing of research computing software. We offer a user-friendly portal website Open OnDemand with graphical interfaces to the most common features, all in one place. Read on to learn more about our resources and how to access them.</p>"},{"location":"cheaha/getting_started/#getting-help","title":"Getting Help","text":"<p>Please Contact Us with requests for support. Tips on getting effective support are here, and our frequently asked questions are here.</p>"},{"location":"cheaha/getting_started/#account-creation","title":"Account Creation","text":"<p>Please visit our Account Creation page for detailed instructions on creating a Cheaha account.</p>"},{"location":"cheaha/getting_started/#accessing-cheaha","title":"Accessing Cheaha","text":"<p>The primary method for accessing Cheaha is through our online portal website, Open OnDemand. To login to our portal, navigate to our https://rc.uab.edu, which does not require an on-campus connection nor the UAB Campus VPN. You should be presented with UAB's Single Sign-on page, which will require use of Duo 2FA. Login using the appropriate credentials laid out at our Account Creation page.</p> <p>SSH may be used to access Cheaha. Connect to host <code>cheaha.rc.uab.edu</code> on port <code>22</code>.</p>"},{"location":"cheaha/getting_started/#with-vscode","title":"With VSCode","text":"<p>An alternative method suited for developers using VSCode, is to use the \"Remote - Tunnels\" extension to connect to an HPC Desktop Interactive Job. More details on this process are available in the VSCode Tunnel section.</p> <p>Important</p> <p>Please do not use VSCode \"Remote - SSH\" to connect to Cheaha. All processes happen on the login node. Use the link above to use \"Remote - Tunnel\" instead.</p>"},{"location":"cheaha/getting_started/#open-ondemand-features","title":"Open OnDemand Features","text":"<p>The Open OnDemand portal features a file browser and various interactive applications including a remote desktop, Jupyter, RStudio and MATLAB, among others. There is also a terminal usable directly in the browser for very basic functions such as file management. More detailed documentation may be found on our Open OnDemand page.</p>"},{"location":"cheaha/getting_started/#hardware","title":"Hardware","text":"<p>A full list of the available hardware can be found on our hardware page.</p>"},{"location":"cheaha/getting_started/#storage","title":"Storage","text":"<p>All researchers are granted 5 TB of individual storage when they create their Research Computing account.</p> <p>Shared storage is available to all Lab Groups and Core Facilities on campus. Shared storage is also available to UAB Administration groups.</p> <p>Please visit our Storage page for detailed information about our individual and shared storage options.</p>"},{"location":"cheaha/getting_started/#partitions","title":"Partitions","text":"<p>Compute nodes are divided into groups called partitions each with specific qualities suitable for different kinds of workflows or software. In order to submit a compute job, a partition must be chosen in the Slurm options. The partitions can be roughly grouped as such:</p> Use Partition Names Notes GPU Processing pascalnodes, pascalnodes-medium, amperenodes, amperenodes-medium These are the only partitions with GPUs All Purpose amd-hdr100 Runs AMD CPUs compared to all other CPU partitions running Intel. Contact us with issues running on this partition Shorter time express, short, intel-dcb Medium-long time medium, long Very large memory largemem, largemem-long <p>Please visit our hardware for more details about the partitions.</p>"},{"location":"cheaha/getting_started/#etiquette","title":"Etiquette","text":"<p>Quality-of-Service (QoS) limits are in place to ensure any one user can't monopolize all resources.</p>"},{"location":"cheaha/getting_started/#why-you-should-avoid-running-jobs-on-login-nodes","title":"Why you should avoid running jobs on Login Nodes","text":"<p>To effectively manage and provide high-performance computing (HPC) resources to the University community provided via the clusters, kindly use the terminal from compute nodes in created jobs rather than the terminal from login nodes. Our clusters are essential for conducting this large and complex scientific computations that often times require a significant amount of computing power. These clusters are shared environments, where multiple users execute their research and computing tasks simultaneously. It is important to utilize the structure of these environments properly for efficient and respectful use of the shared resources, so everyone gets a fair chance at using these resources.</p>"},{"location":"cheaha/getting_started/#login-vs-compute-nodes","title":"Login vs. Compute Nodes","text":"<p>Like with most HPC clusters, cheaha nodes are divided into two, the login node and compute nodes. The login node acts as the gateway for users to access the cluster, submit jobs, and manage files. Compute nodes, on the other hand, are like the engines of the cluster, designed to perform the heavy lifting of data processing and computation.</p> <p>The Login node can be accessed from the Cheaha landing page or through the <code>$HOME</code> directory. You can see in the images below, how to identify if you\u2019re within a login node or compute node.</p> <p>You are on the login node if:</p> <ul> <li>terminal prompt looks like <code>[$USER@login004 ~]$</code></li> </ul> <p></p> <p>You are on compute nodes if:</p> <ul> <li>using Open OnDemand Interactive Apps</li> <li>using Interactive Jobs with <code>srun</code></li> <li>terminal prompt looks like <code>[$USER@c0112 ~]$</code></li> </ul> <p></p> <p>Important</p> <p>If the terminal prompt appears as <code>bash-4.2$</code> instead of the user prompt <code>[$USER@login004]</code>, please refer to the FAQ below to resolve the issue.</p>"},{"location":"cheaha/getting_started/#how-to-restore-default-terminal-prompt-from-bash-42-to-userlogin004","title":"How to Restore Default Terminal Prompt from <code>bash-4.2$</code> to <code>$USER@login004</code>?","text":"<p>There might be scenarios where you see the terminal prompt display <code>bash-4.2$</code> like below, instead of the user prompt <code>[$USER@login004]</code>.</p> <pre><code>bash-4.2$\n</code></pre> <p>The <code>bash-4.2$</code> prompt indicates that the files <code>$HOME/.bashrc</code> and/or <code>$HOME/.bash_profile</code> are missing or corrupted. To resolve this issue, we recommend following these steps:</p> <p>(i) If you have made any changes to these files earlier, it's advisable to create backups. For instance, you can rename <code>.bashrc</code> to <code>.bashrc.backup</code>, and you can verify if the backed-up files are listed.</p> <pre><code>bash-4.2$ mv $HOME/.bashrc $HOME/.bashrc.backup\nbash-4.2$ mv $HOME/.bash_profile $HOME/.bash_profile.backup\n\nbash-4.2$ ls .bash*\n.bash_profile.backup  .bashrc.backup\n</code></pre> <p>(ii) After you have taken the backup of those files, run the following command to copy the default versions from <code>/etc/skel</code> to <code>$HOME</code>. Doing this will clobber, or remove, any changes you may have made, so be sure to make a backup first, as shown in the step (ii), if you wish to keep any changes. You will see the copied files listed in the directory.</p> <pre><code>bash-4.2$ cp /etc/skel/.bash* $HOME\n\nbash-4.2$ ls .bash*\n.bash_profile  .bash_profile.backup  .bashrc  .bashrc.backup\n</code></pre> <p>(iii) You can exit the terminal or source your files using the commands below to apply the changes, after which you will see the user prompt.</p> <pre><code>bash-4.2$ source ~/.bashrc\n[$USER@login004 ~]$\n</code></pre>"},{"location":"cheaha/getting_started/#slurm-and-slurm-jobs","title":"Slurm and Slurm Jobs","text":"<p>Slurm Workload Manager is a widely used open-source job scheduler that manages the queue of jobs submitted to the compute nodes. It ensures efficient use of the cluster by allocating resources to jobs, prioritizing tasks, and managing queues of pending jobs. Starting Slurm jobs can be done in two primary ways: using Open OnDemand (OOD) or through the terminal. For more details on how to use Slurm on cheaha, please see our slurm docs.</p>"},{"location":"cheaha/getting_started/#what-should-run-in-jobs","title":"What Should Run in Jobs?","text":"<p>Ideally, only non-intensive tasks like editing files, or managing job submissions should be performed on the login node. Compute-intensive tasks, large data analyses, and simulations should be submitted as Slurm jobs to compute nodes. This approach ensures that the login node remains responsive and available for all users to manage their tasks and submissions. Submitting compute-intensive tasks as Slurm jobs to compute nodes helps to prevent overloading the login node, ensuring a smoother experience for all users of the cluster.</p>"},{"location":"cheaha/getting_started/#how-to-start-slurm-jobs","title":"How to start SLURM Jobs?","text":"<p>There are two straightforward ways to start SLURM jobs on cheaha, and they are detailed below.</p>"},{"location":"cheaha/getting_started/#open-ondemand-ood","title":"Open OnDemand (OOD)","text":"<p>UAB uses the OOD platform, a web-based interface for providing access to cluster resources without the need for command-line tools. Users can easily submit jobs, manage files, and even use interactive applications directly from their browsers. One of the standout features of OOD is the ability to launch interactive applications, such as a virtual desktop environment. This feature allows users to work within the cluster as if they were on a local desktop, providing a user-friendly interface for managing tasks and running applications. For an overview of how the page works, and to read more details see our docs on Navigating Open OnDemand. After logging into OOD, users can access various applications designed for job management, file editing, and more.</p>"},{"location":"cheaha/getting_started/#terminal-sbatch-jobs","title":"Terminal (sbatch Jobs)","text":"<p>For users comfortable with the command line, submitting jobs via scripts using <code>sbatch</code> is a straightforward process. An <code>sbatch</code> script contains the job specifications, such as the number of nodes, execution time, and the command to run. This method provides flexibility and control over job submission and management. For more information on this, please see our docs on Submitting Jobs with Slurm.</p> <p>Important</p> <p>If you are doing more than minor file management, you will need to use a compute node. Please request an interactive session at https://rc.uab.edu or through a job submitted using Slurm.</p>"},{"location":"cheaha/getting_started/#slurm","title":"Slurm","text":"<p>Slurm is our job queueing software used for submitting any number of job scripts to run on the cluster. We have documentation on how to set up job scripts and submit them further in. More complete documentation is available at https://slurm.schedmd.com/.</p>"},{"location":"cheaha/getting_started/#software","title":"Software","text":"<p>A large variety of software is available on Cheaha as modules. To view and use these modules see the following documentation.</p> <p>For new software installation, please try searching Anaconda for packages first. If you still need help, please send a support ticket</p>"},{"location":"cheaha/getting_started/#conda-packages","title":"Conda Packages","text":"<p>A significant amount of open-source software is distributed as Anaconda or Python libraries. These libraries can be installed by the user without permission from Research Computing using Anaconda environments. To read more about using Anaconda virtual environments see our Anaconda page.</p> <p>If the software installation instructions tell you to use either <code>conda install</code> or <code>pip install</code> commands, the software and its dependencies can be installed using a virtual environment.</p>"},{"location":"cheaha/getting_started/#how-to-get-help","title":"How to Get Help","text":"<p>For questions, you can reach out via our various channels.</p>"},{"location":"cheaha/hardware/","title":"Hardware Information","text":"<p>The following hardware summaries may be useful for selecting partitions for workflows and for grant proposal writing. If any information is missing that would be helpful to you, please be sure to contact us or create an issue on our tracker.</p> <p>Tip</p> <p>The tables in this section are wide and can be scrolled horizontally to display more information.</p>"},{"location":"cheaha/hardware/#cheaha-hpc-cluster","title":"Cheaha HPC Cluster","text":""},{"location":"cheaha/hardware/#summary","title":"Summary","text":"<p>The table below contains a summary of the computational resources available on Cheaha and relevant Quality of Service (QoS) Limits. QoS limits allow us to balance usage and ensure fairness for all researchers using the cluster. QoS limits are not a guarantee of resource availability.</p> <p>In the table, Slurm partitions are grouped by shared QoS limits on cores, memory, and GPUs. Node limits are applied to partitions independently. All limits are applied to researchers independently.</p> <p>Examples of how to make use of the table:</p> <ul> <li>Suppose you submit 30 jobs to the \"express\" partition, and suppose each job needs 10 cores each. Hypothetically, in order for all of the jobs to start at once, 300 cores would be required. The QoS limit on cores is 264 on the \"express\" partition, so at most 26 jobs (260 cores) can start at once. The remaining 4 jobs will be held in queue, because starting one more would go beyond the QoS limit (270 &gt; 264).</li> <li>Suppose you submit 5 jobs to the \"medium\" partition and 5 to the \"long\" partition, each requiring 1 node. Then, 10 total nodes would be needed. In this case, it is possible that all 10 jobs can start at once because partition node limits are separate. If all 5 jobs start, jobs on the \"medium\" partition.</li> <li>Suppose you submit 5 jobs to the \"amperenodes\" partition and 5 to \"amperenodes-medium\", for a total of 10 A100 GPUs. Additionally, you also submit 4 jobs to the \"pascalnodes\" partition totaling 8 P100 GPUs. Then 4 of the \"gpu: ampere\" group jobs can start at once, because the QoS limit is 4 GPUs there. Additionally, all 4 of the \"gpu: pascal\" group jobs, because the QoS limit is 8 GPUs there. In this case, the QoS for each group is separate.</li> </ul> Partition Time Limit in Hours Nodes (Limit/Partition) Cores/Node (Limit/Person) Mem GB/Node (Limit/Person) GPU/Node (Limit/Person) cpu: amd amd-hdr100 150 34 (5) 128 (264) 504 (3072) cpu: intel express 2 51 (~) 48 (264) 754 (3072) short 12 51 (44) 48 (264) 754 (3072) medium 50 51 (44) 48 (264) 754 (3072) long 150 51 (5) 48 (264) 754 (3072) gpu: ampere amperenodes 12 20 (TBD) 32 (64) 189 (384) 2 (4) amperenodes-medium 48 20 (TBD) 32 (64) 189 (384) 2 (4) gpu: pascal pascalnodes 12 18 (~) 28 (56) 252 (500) 4 (8) pascalnodes-medium 48 7 (~) 28 (56) 252 (500) 4 (8) mem: large largemem 50 13 (10) 24 (290) 755 (7168) largemem-long 150 5 (10) 24 (290) 755 (7168) <p>The full table can be downloaded here.</p>"},{"location":"cheaha/hardware/#details","title":"Details","text":"<p>Detailed hardware information, including processor and GPU makes and models, core clock frequencies, and other information for current hardware are in the table below.</p> Generation Compute Type Total Cores Total Memory Gb Total Gpus Cores Per Node Cores Per Die Dies Per Node Die Brand Die Name Die Frequency Ghz Memory Per Node Gb Gpu Per Node Gpu Brand Gpu Name Gpu Memory Gb Nodes 1 cpu: amd 128 1024 2 1 2 AMD Opteron 242 1.6 16 64 2 cpu: intel 192 1152 8 4 2 Intel Xeon E5450 3 48 24 3 cpu: intel 384 1536 12 6 2 Intel Xeon X5650 2.66 48 32 3 cpu: intel 192 1536 12 6 2 Intel Xeon X5650 2.66 96 16 4 cpu: intel 48 1152 16 8 2 Intel Xeon X5650 2.7 384 3 5 cpu: intel 192 1152 16 8 2 Intel Xeon E2650 2 96 12 6 cpu: intel 336 5376 24 12 2 Intel Xeon E5-2680 v3 2.5 384 14 6 cpu: intel 912 9728 24 12 2 Intel Xeon E5-2680 v3 2.5 256 38 6 cpu: intel 1056 5632 24 12 2 Intel Xeon E5-2680 v3 2.5 128 44 7 gpu: pascal 504 4608 72 28 14 2 Intel Xeon E5-2680 v4 2.4 256 4 NVIDIA Tesla P100 16 18 8 cpu: intel 504 4032 24 12 2 Intel Xeon E5-2680 v4 2.5 192 21 8 mem: large 240 7680 24 12 2 Intel Xeon E5-2680 v4 2.5 768 10 8 mem: large 96 6144 24 12 2 Intel Xeon E5-2680 v4 2.5 1536 4 9 cpu: intel 2496 39936 48 24 2 Intel Xeon Gold 6248R 3 768 52 10 cpu: amd 4352 17408 128 64 2 AMD Epyc 7713 Milan 2 512 34 11 gpu: ampere 2560 10240 40 128 64 2 AMD Epyc 7763 Milan 2.45 512 2 NVIDIA A100 80 20 1 cpu: intel 240 960 48 12 4 Intel Xeon Gold 6248R 3 192 5 1 gpu: ampere 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 1 cpu: intel 144 576 48 12 4 Intel Xeon Gold 6248R 3 192 3 1 gpu: ampere 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 <p>The full table can be downloaded here.</p> <p>The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice.</p> Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 7 1.08 17.06 18.14 18 326.43 8 0.96 0.96 21 20.16 8 0.96 0.96 10 9.60 8 0.96 0.96 4 3.84 9 2.30 2.30 52 119.81 10 4.10 4.10 34 139.26 11 5.02 15.14 20.15 20 403.10 Total 1,022.20 <p>The full table can be downloaded here.</p> <p>For information on using Cheaha, see our dedicated section.</p>"},{"location":"cheaha/hardware/#cloud-service-at-cloudrc","title":"Cloud Service at cloud.rc","text":"<p>The Cloud service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table.</p> Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info cloud 1 cpu 240 960 48 192 5 Intel Xeon Gold 6248R 3.00 GHz cloud 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB Total 752 5056 32 9 <p>The full table can be downloaded here.</p> <p>The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice.</p> Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 5 11.52 1 4.61 77.97 82.58 4 330.3 Total 341.82 <p>The full table can be downloaded here.</p> <p>For information on using our Cloud service at cloud.rc, see our dedicated section.</p>"},{"location":"cheaha/hardware/#kubernetes-container-service","title":"Kubernetes Container Service","text":"<p>Important</p> <p>The Kubernetes fabric is still in deployment and not ready for researcher use. We will be sure to inform you when the service is ready. The following information is planned hardware.</p> <p>The Kubernetes container service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table.</p> Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info container 1 cpu 144 576 48 192 3 Intel Xeon Gold 6248R 3.00 GHz container 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB Total 656 4672 32 7 <p>The full table can be downloaded here.</p> <p>The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice.</p> Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 3 6.91 1 4.61 77.97 82.58 4 330.3 Total 337.21 <p>The full table can be downloaded here.</p>"},{"location":"cheaha/job_efficiency/","title":"Job Efficiency","text":"<p>Efficient jobs save you time. Many factors go into queue wait time, but you can control your job requests. Optimizing queue wait times relies on getting resource requests close to actual resource usage. For example, if your task runs as fast with 2 cores as with 4 cores, requesting 4 cores will increase your wait time for no benefit. Other researchers time will be wasted due to locked up, unused resources. So please read this page to learn how to increase your efficiency and save time.</p> <p>As with any new skill, developing an intuition for efficiency and resource estimation requires experimentation, practice, and feedback.</p> <p>Please DO:</p> <ul> <li>run subsets of your data with varying resource requests to develop intuition</li> <li>make use of <code>seff</code> to validate</li> <li>contact us for advice if you're lost</li> </ul>"},{"location":"cheaha/job_efficiency/#estimating-compute-resources","title":"Estimating Compute Resources","text":"<p>Being able to estimate the resources a job will need is critical. Requesting substantially more resources than necessary bottlenecks the cluster by preventing jobs from using resources that are reserved, but going unused. Of course requesting too few resources may cause the tasks to perform unacceptably slowly, or to fail altogether.</p> <p>Questions to ask yourself before requesting resources:</p> <ol> <li> <p>Can my scripts take advantage of multiple cores?</p> <ul> <li>If yes, then request more cores.</li> <li>If no, then request only one core.</li> <li>Example: RStudio generally runs on a single thread. Any cores beyond the first will go unused and unusable.</li> </ul> </li> <li> <p>How large is the data I'm working with?</p> <ul> <li>Start by requesting memory equal to double the size of one file, no less than 2 GB per core.</li> <li>If that isn't enough, increase the request by 50% until there are no more memory errors.</li> <li>Example: If your data file is 4 GB, try starting out by requesting 8 GB of memory, then 12 GB, 16 GB, etc.</li> </ul> </li> <li> <p>Do my pipelines keep large amounts of data in memory?</p> <ul> <li>If yes, you may need to request even more memory than above.</li> <li>Example: Without careful programming, MATLAB code will often make and retain copies of data until finished.</li> </ul> </li> <li> <p>How long should my job take?</p> <ul> <li>Example: If my laptop is able to run the code on one data file in 2 hours, it will take about that long on Cheaha.</li> <li>Example: Requesting 50 hours of time for a 15 hour process will lengthen the queue time.</li> <li>Don't request too little! Include a buffer to account for scheduler and network issues.</li> </ul> </li> <li> <p>How is the software I'm using programmed?</p> <ul> <li>Can it use a GPU? Request one.</li> <li>Can it use multiple cores? Request more than one core.</li> <li>Is it single-threaded? Request only one core.</li> <li>Does it use MPI? Request multiple nodes.</li> <li>Be sure to check all of the flags, configuration, and options for the software, or these changes may not work.</li> </ul> </li> <li> <p>Which partition is most appropriate?</p> <ul> <li>More than 40 GB memory and queue wait times are long? Try <code>largemem*</code>.</li> <li>Need a GPU? Use <code>pascalnodes*</code> or <code>amperenodes*</code>.</li> <li>Software works with AMD? Try <code>amd-hdr100</code>.</li> </ul> </li> </ol> <p>Note</p> <p>Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. We are happy to help guide you to an efficient usage of the cluster.</p> <p>Use <code>seff</code> to verify that your code is as efficient as possible.</p>"},{"location":"cheaha/job_efficiency/#verifying-job-efficiency","title":"Verifying Job Efficiency","text":"<p>It's important to evaluate the efficiency of your job in terms of resource usage after it completes. Remember that Cheaha is a shared resource, so requesting resources that sit unused during a job prevents others from using those resources. As well, because each researcher has a maximum amount of resources they can use at a given time, having inefficient jobs can increase analysis runtime across many jobs, and increase queue wait times.</p> <p>In order to look at job efficieny, use the <code>seff</code> command.</p> <pre><code>seff &lt;jobid&gt;\n</code></pre> <p>The output will look like:</p> <p></p> <p>The job had poor CPU efficiency, requesting 2 CPUs which were only busy for 30% of runtime. Requesting only a single core may have made more sense here. The job also had poor memory efficiency, using less than 1 GB total memory of the requested 16 GB (5.73%). For subsequent jobs using a similar analysis and dataset size, decreasing the requested memory to about 1200 MB and a single CPU would be more efficient, and get the job queued faster.</p> <p>Tip</p> <p>Aim for between 75% and 90% memory efficiency. Lower than that is a waste of resources, but too close to 100% could result in job failure due to an unexpected out-of-memory issue.</p>"},{"location":"cheaha/open_ondemand/","title":"Open OnDemand","text":"<p>Open OnDemand (OOD) is web portal to access Cheaha. On it, you can submit interactive jobs in easy to use forms. These jobs include a generic desktop as well as specific apps such as RStudio or MATLAB. There is also access to a basic file manager for viewing and moving files.</p> <p>The web portal can be accessed at https://rc.uab.edu and is available both on and off campus.</p>"},{"location":"cheaha/open_ondemand/#quickstart","title":"Quickstart","text":"<p>To start a generic desktop job where any piece of software can run, do the following:</p> <ol> <li>Go to Cheaha's web portal</li> <li>Click Interactive Apps &gt; HPC Desktop at the top.</li> <li>Select the resources you will need for the job (number of CPUs, amount of memory, job runtime, and partition).<ol> <li>As an example, a generic desktop job could use 1 CPU and 8 GB of RAM.</li> <li>See our partition table for to determine which partition fits your job. The requested amount of time should not exceed the partition limit.</li> </ol> </li> <li>Click Launch at the bottom. This will take you to the My Interactive Sessions page and a job card will be created for your interactive job.</li> <li>When the job card is created, the job is in queue. It will remain gray while in queue but will turn green when the job has been allocated resources and is running. Click the <code>Launch Desktop in new tab</code> button to open the interactive job.</li> </ol> <p>Every interactive job requested in OOD is already set on a compute node. This bypasses the login node and is the preferred method for running interactive jobs on Cheaha.</p>"},{"location":"cheaha/open_ondemand/#choosing-resources","title":"Choosing Resources","text":"<p>For a more complete description of how to select resources, go here</p>"},{"location":"cheaha/open_ondemand/#debugging-ood-job-failures","title":"Debugging OOD Job Failures","text":"<p>If your OOD job cards are disappearing after being allocated or during the job, see our documentation for instructions on how to retrieve the logs and submit a ticket to Research Computing support.</p>"},{"location":"cheaha/open_ondemand/hpc_desktop/","title":"HPC Desktop","text":"<p>The HPC Desktop is a general desktop interface for Cheaha. It can run all software installed on Cheaha including those with graphical interfaces which can't be run from a base terminal. It has the same functionality as a standard Centos 7 desktop including a web browser for accessing the internet. The HPC Desktop is a standard tool used for pipeline and analysis development when creating scripts to use in batch jobs. Below, you can see an example of the basic available desktop.</p> <p></p>"},{"location":"cheaha/open_ondemand/hpc_desktop/#copy-paste-into-hpc-desktop","title":"Copy-Paste Into HPC Desktop","text":"<p>The HPC Desktop app is run through noVNC. This setup means it is not possible to conventially copy-paste text between the Desktop tab and other tabs or software on the host machine (i.e. a lab workstation or personal laptop). To copy-paste text, noVNC provides a control panel with a clipboard for passing text between the VNC session and the host machine. The control panel is available by clicking a tab on the far left of the screen, halfway down (see the red rectangle in the image below).</p> <p></p> <p>Once you click the tab, you'll see the control panel appear. The second option from the top will open a clipboard. Paste text from your local machine into the clipboard and then paste into either a terminal or another program in the VNC session. See below for an example.</p> <p></p> <p>To copy from the VNC to your personal machine, highlight the text you want to copy in the VNC session, and that text will appear in the clipboard. Select the text in the clipboard, copy it, and then paste it on your local machine. Images cannot be copy-pasted through this clipboard. Instead, images should be saved as a file and then transferred through tools such as Globus, rclone, or an scp utility.</p>"},{"location":"cheaha/open_ondemand/hpc_desktop/#visual-studio-code-remote-tunnel","title":"Visual Studio Code Remote Tunnel","text":"<p>It is possible to remotely access Cheaha using an HPC Desktop job as a host for Visual Studio Code (VSCode), with the \"Remote - Tunnels\" extension. You can read more about the extension and process at https://code.visualstudio.com/docs/remote/tunnels.</p> <p>To use this method you will need either a GitHub account or Microsoft account. Microsoft accounts can be obtained using your SSO credentials through Microsoft.</p> <p>Danger</p> <p>Do NOT use the remote tunnel extension if you intend to view or work with Restricted/PHI Data while using VSCode.</p> <p>When using a tunnel, all information visible within VSCode is end-to-end encrypted and sent from Cheaha to your local machine through a third-party service (the tunnel). Use of any third party services and encryption for Restricted/PHI Data requires a risk assessment first, on a case-by-case basis.</p> <p>Warning</p> <p>Do not use \"Remote - SSH\" to access Cheaha, as all processes run on the login node. VSCode Server, and associated processes, running on the login node may be shut down at any time to free login node resources. Instead, please use \"Remote - Tunnels\" as described below.</p>"},{"location":"cheaha/open_ondemand/hpc_desktop/#downloading-and-installing-vscode-and-vscode-server","title":"Downloading and Installing VSCode and VSCode Server","text":"<p>Important</p> <p>Current versions of VSCode are not supported on older Linux distributions, including CentOS 7 running on Cheaha. Cheaha is only compatible with VSCode version 1.85.2 or lower. The following instructions will show you how to install this specific version. If VSCode does not work and is giving messages saying <code>GLIBC_2.18 (or greater) not found (required by ./code)</code>, your installation of VSCode is too new and needs to be downgraded to 1.85.2.</p> <p>First, open a terminal on Cheaha. Run the following commands if you have used VSCode before to remove some hidden files. If you are installing VSCode for the first time, skip to the next code block.</p> <pre><code>rm -r ${HOME}/.vscode\nrm -r ${HOME}/.vscode-cli\n</code></pre> <p>Next, use the following commands to install both VSCode and VSCode Server:</p> <pre><code>curl -L -o vscode_cli.tar.gz 'https://update.code.visualstudio.com/1.85.2/cli-alpine-x64/stable'\nmkdir $HOME/bin\ntar -xz -C ${HOME}/bin -f vscode_cli.tar.gz\n\nexport commit_sha=8b3775030ed1a69b13e4f4c628c612102e30a681\ncurl -L \"https://update.code.visualstudio.com/commit:${commit_sha}/server-linux-x64/stable\" -o vscode_server.tar.gz\nmkdir -vp ~/.vscode-server/bin/${commit_sha}\ntar --no-same-owner -xzv --strip-components=1 -C ${HOME}/.vscode-server/bin/\"${commit_sha}\" -f vscode_server.tar.gz\n</code></pre>"},{"location":"cheaha/open_ondemand/hpc_desktop/#adding-code-to-path","title":"Adding code to PATH","text":"<p>To avoid typing <code>./code</code> for commands, try adding <code>${HOME}/bin</code> to <code>$PATH</code> in your <code>~/.bashrc</code> before starting a job. Then you will only need to type <code>code</code> for commands. You can do this from the terminal with the following command:</p> <pre><code>echo \"export PATH=\\$PATH:${HOME}/bin\" &gt;&gt; $HOME/.bashrc\n</code></pre> <p>You only need to have this line or <code>export PATH=$PATH:${HOME}/bin:...</code> in your <code>.bashrc</code> once. You should check your <code>.bashrc</code> to make sure this line isn't already there before adding it.</p>"},{"location":"cheaha/open_ondemand/hpc_desktop/#starting-a-tunnel","title":"Starting a Tunnel","text":"<p>These steps should be performed each time you would like to create a tunnel.</p> <ol> <li>Start an HPC Desktop job on Cheaha with the required resources. Note that all VSCode processing, including debuggers, unit testing, Jupyter Notebook server, file access, etc., all happen in this same job context. Adjust resources in relation to your development needs.</li> <li>Within the HPC Desktop Job, open a terminal and run the command <code>code tunnel</code> (assumes you've added <code>code</code> to <code>$PATH</code>).</li> <li> <p>Select whether you would like to login with a Microsoft or GitHub account. You should then see a URL and code.</p> <p></p> <p></p> </li> <li> <p>In your local browser, navigate to the URL and enter the code. If you see a <code>https://vscode.dev/tunnel/...</code> URL, ignore it. It leads to an online-only instance of VSCode running on a Microsoft Cloud service, not to your local machine.</p> <p></p> <p></p> </li> <li> <p>Open VSCode on your local machine and click the <code>&gt;&lt;</code> button in the lower-left corner of the main VSCode window to open the Command Palette. Select \"Connect to Tunnel...\" to find your tunnel.</p> <p></p> <p></p> </li> <li> <p>Select the same login method as in step (3). You may be asked to login locally.</p> <p></p> </li> <li> <p>Select your tunnel from the list.</p> <p></p> </li> </ol> <p>After the previous step, you should be connected to your tunnel. Now your local VSCode window is acting as a front-end for processing and file access occuring on Cheaha.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/","title":"Jupyter Apps","text":"<p>Jupyter Notebooks and Jupyter Lab are both available as standalone apps in OOD. Jupyter is commonly used with Anaconda environments. If you are unfamiliar with Anaconda environments please see the Working with Anaconda Environments section below before continuing here.</p> <p>To launch the Jupyter notebook, select the menus 'Interactive Apps -&gt; Jupyter Notebook'. The job creation and submission form appears:</p> <p></p> <p>As with all interactive apps, you'll need to select the resources required using the job creation form. Jupyter may also require additional initial setup before the app launches.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#environment-setup","title":"Environment Setup","text":"<p>To modify the environment that Anaconda and Jupyter will run in, please use the Environment Setup field to load modules and modify the environment <code>$PATH</code>. Be aware that any changes to the environment made in this window will be inherited by terminals as well as notebooks opened within Jupyter.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#cuda","title":"CUDA","text":"<p>For GPU applications you'll need to load a <code>CUDA/*</code> module to have the CUDA toolkit available. If working with deep learning workflows, you may also need to load the <code>cuDNN/*-CUDA-*</code> module corresponding to your choice of <code>CUDA/*</code> module version. These are required for popular ML/DL/AI libraries like TensorFlow, Keras, and PyTorch. Use <code>module spider cuda/</code> and <code>module spider cudnn</code> to view the list of appropriate modules. An example of what to put in the Environment Setup field when using a version of Tensorflow compatible with CUDA version 12.2.0 is shown below.</p> <pre><code># ENVIRONMENT SETUP\nmodule load CUDA/12.2.0\nmodule load cuDNN/8.9.2.26-CUDA-12.2.0\n</code></pre> <p>For information on which versions of CUDA to load for Tensorflow and PyTorch, please see Tensorflow Compatibility and PyTorch Compatibility.</p> <p>For information on partition and GPU selection, please review our hardware information page and GPU Page</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#extra-jupyter-arguments","title":"Extra Jupyter Arguments","text":"<p>The <code>Extra Jupyter Arguments</code> field allows you to pass additional arguments to the Jupyter Server as it is being started. It can be helpful to point the server to the folder containing your notebook. To do this, assuming your notebooks are stored in <code>/data/user/$USER</code>, also known as <code>$USER_DATA</code>, put <code>--notebook-dir=$USER_DATA</code> in this field. You will be able to navigate to the notebook if it is in a subdirectory of <code>notebook-dir</code>, but you won't be able to navigate to any other directories. An example is shown below.</p> <p></p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#working-with-other-programming-languages-within-jupyter-notebook","title":"Working with other programming languages within Jupyter Notebook","text":"<p>To work with other programming languages within Jupyter Notebook, you need to install the corresponding kernel for each language, similar to the process used for Python with the <code>ipykernel</code>. This can be done using package managers such as <code>pip</code> or <code>conda</code>, or by following language-specific instructions. For example, to install <code>R kernel</code> for the R language, we can run the <code>conda install -c r r-essentials</code> command. Please ensure that the kernel is installed in your Anaconda environment. Then, select the desired language environment from the kernel dropdown menu.</p> <p>Once the necessary kernels are installed, if you wish, you can write and run multiple code cells in different languages within a single notebook. Easily switch between kernels and select the preferred one for each language, and then proceed to run the code cells in their respective languages.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#working-with-anaconda-environments","title":"Working with Anaconda Environments","text":"<p>By default, Jupyter notebooks will use the base environment that comes with the Anaconda3 module. This environment contains a large number of popular packages and may useful for something quick, dirty, and simple. However, for any analysis needing specific package versions or special packages, you will need to create your own environment and select it from the <code>Kernel</code> menu. For information on creating and managing Anaconda environments please see our Using Anaconda page. Then please review our Cheaha-specific Anaconda page for important tips and how to avoid common pitfalls.</p> <p>To change the kernel, use the <code>Kernel</code> dropdown and select <code>Change Kernel</code>. From the list, choose the kernel corresponding to your desired Anaconda environment (see below for an example). If your environment isn't appearing, you may be missing the ipykernel package. To do so, use <code>conda install ipykernel</code> to get ipykernel packgae installed into your environment, so Jupyter can recognize your environment.</p> <p></p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#creating-an-environment-for-use-with-jupyter-notebook","title":"Creating an Environment for use with Jupyter Notebook","text":"<p>We can create a new environment, that houses all of the packages, modules, and libraries we need for our current Jupyter Notebook to implement functions and operations, run all of its cells and deliver desired outputs. Follow the steps below to accomplish this;</p> <ol> <li> <p>Access the terminal using your preferred method.</p> <ul> <li>OOD Terminal. Be sure to run the following steps in a job!</li> <li>OOD HPC Desktop Job Terminal. This method will ensure terminal commands are run in a job.</li> </ul> </li> <li> <p>Create and activate your new environment, following the linked steps.</p> </li> <li> <p>Install your desired packages into your activated environment.</p> </li> <li> <p>Remember to install 'ipykernel' in your activated environment, using <code>conda install ipykernel</code>.</p> </li> <li> <p>Go into your working Jupyter Notebook file, and change to the created environment.</p> </li> </ol>"},{"location":"cheaha/open_ondemand/ood_jupyter/#changing-environments-using-jupyter-notebook-gui","title":"Changing Environments using Jupyter Notebook GUI","text":"<ol> <li> <p>When your Jupyter Notebook Job has been created on Cheaha, and you want to load an environment you have already created. Select from the dropdown menu \"New\". You can find this in the top right corner of the Jupyter Notebook landing page. </p> </li> <li> <p>When you click new, you would see a dropdown of environments that are available for you to use. If you do not see your created environment listed, you may need to install <code>ipykernel</code> using <code>conda install ipykernel</code> in your cheaha shell within your activated environment. You may have to refresh the page to see your newly created environment. Select the preferred existing environment you created. </p> <p>On another note, you may want to replicate an environment setup to handle a project, research, or analysis but you are already working on a Jupyter Notebook file. You can select a different environment from the Jupyter Notebook file by;</p> <ol> <li> <p>Selecting the Jupyter Notebook File from your landing page.</p> </li> <li> <p>While in the file, look for the menu option \"Kernel\", select this. In the Kernel dropdown option, select \"Change kernel\". Then select your preferred kernel environment. Wait a few seconds for it to load, and you are ready to use your preferred environment. Selecting this would open a new Jupyter Notebook file with your selected environment. </p> </li> <li> <p>Your selected environment would appear in the top right corner.</p> </li> </ol> </li> </ol>"},{"location":"cheaha/open_ondemand/ood_jupyter/#common-issues-in-ood-jupyter","title":"Common Issues in OOD Jupyter","text":""},{"location":"cheaha/open_ondemand/ood_jupyter/#python-executable-issues","title":"Python Executable Issues","text":"<p>Jupyter Notebook by default loads <code>Anaconda3</code>. Hence do not load any versions of <code>Anaconda3</code> module in the <code>Environment Setup</code> field in the OOD Jupyter Notebook, as it causes Python mismatch, and the errors are hard to diagnose.</p> <p>Having custom installs of Anaconda/Miniconda/ can cause the above similar issue. If you have installations of any of these software in your personal space, delete those directories and instead use the <code>Anaconda3</code> module.</p> <p>To identify a Python mismatch, use the commands <code>which python</code> and <code>python --version</code> to confirm the desired Python executable and version. Within the <code>conda</code> environment, <code>which python</code> prints the path of the Python executable (e.g. <code>~/.conda/envs/remora/bin/python</code>). If it doesn't match the expected version, an unexpected Python version may be in use.</p> <p><code>conda init</code> append an incorrect version of Python to the front of the <code>$PATH</code>, an environment variable containing directories where the operating system looks for executable files. When you attempt to execute a Python-related command, the system will find the first matching executable in the directories listed in the modified <code>$PATH</code>. If the first entry corresponds to the version of Python added by <code>conda init</code>, that specific version will be used which lead to Python mismatch and hard-to-diagnose errors.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#unexpectedsilent-job-failure","title":"Unexpected/Silent Job Failure","text":"<p>Having <code>conda activate</code> and <code>source activate</code> statements in the OOD Jupyter Notebooks' <code>Environment Setup</code> field can cause unexpected and silent job failure. Avoid using <code>conda activate</code> in the <code>Environment Setup</code> field.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#timeout-in-loading-jupyter-notebook","title":"Timeout in Loading Jupyter Notebook","text":"<p>If you encounter a \"Failed to Connect\" message while trying to open the job, and experience a timeout issue in loading the OOD Jupyter Notebook, it is recommended to close the tab and wait for a few minutes. Jupyter is still in the process of initializing and may take some time after the job initially starts running.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#vnc-error-when-launching-ood-jupyter-notebook","title":"VNC Error When Launching OOD Jupyter Notebook","text":"<p>While launching an OOD HPC Desktop Job or any OOD Applications, if the user gets errors, <code>Unable to contact settings server</code> and/or <code>Unable to load a failsafe session</code>, it is recommended to follow the below guidelines.</p> <p> </p> <p>Using <code>conda init</code> causes a block of code automatically inserted into the <code>.bashrc</code> file in your <code>$HOME</code> directory. This code block may interfere with the proper functioning of various OOD applications, resulting in a VNC error. To address this issue, it is recommended to follow the instructions outlined in the FAQ entry.</p>"},{"location":"cheaha/open_ondemand/ood_jupyter/#pip-installs-packages-outside-of-environment","title":"Pip Installs Packages Outside of Environment","text":"<p>When installing packages within a <code>conda</code> environment using <code>pip</code>, it's crucial to ensure that you install <code>pip</code> within the same conda environment and use <code>pip</code> from that environment. If <code>pip</code> is used outside of Anaconda or within an environment without <code>pip</code> installed, the packages are installed to <code>~/.local</code>. This can lead to unexpected package conflicts, as Python loads packages from <code>~/.local</code> before loading from Anaconda environments, and shows the following error,</p> <pre><code>Requirement already satisfied: numpy in /home/$USER/.local/lib/python3.11/site-packages (1.26.3)\n</code></pre> <p>For the above case, resolving errors involve deleting the <code>~/.local</code> directory.</p> <p>Here's an example of the correct procedure for installing <code>pip</code> packages within a <code>conda</code>:</p> <ol> <li>Load the <code>Anaconda3</code> module using <code>module load Anaconda3</code>.</li> <li>Create or activate the desired Anaconda environment. Please refer to the Anaconda documentation</li> <li>Install <code>pip</code> within the <code>conda</code> environment using <code>conda install pip</code> or <code>conda install python</code>. <code>pip</code> and <code>python</code> are packaged together, installing one will always install the other.</li> <li>Use <code>pip</code> when this <code>conda</code> environment is active to install packages. Please refer to Installing packages with <code>pip</code></li> </ol>"},{"location":"cheaha/open_ondemand/ood_jupyter/#tensorflow-and-pytorch-gpu-issues","title":"Tensorflow and PyTorch GPU issues","text":"<p>If you are using Jupyter with TensorFlow or PyTorch and no GPU is found, please see our Slurm GPU page sections on TensorFlow Compatibility and PyTorch Compatibility.</p>"},{"location":"cheaha/open_ondemand/ood_layout/","title":"Navigating Open OnDemand","text":"<p>After creating your Cheaha account, going to rc.uab.edu will take you to the Open OnDemand (OOD) homepage:</p> <p></p> <p>The landing page contains information about current updates to the system, a message of the day with links to our support email and documentation, as well as a helpful partition table to help you determine which partition to submit your jobs to. You can access all of the different features of OOD using the navigation tabs at the top of the screen. The most commonly used features are covered below.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#file-browser","title":"File Browser","text":"<p>You can open a file browser in a new tab by clicking the <code>Files</code> dropdown and selecting which default directory you would like to access from <code>HOME</code>, <code>USER_DATA</code>, or <code>USER_SCRATCH</code>.</p> <p>Note</p> <p><code>USER_SCRATCH</code> is shown as both <code>/scratch</code> and <code>/data/scratch</code>. <code>/data/scratch</code> is just a symbolic link to <code>/scratch</code>. You can use either, but <code>/scratch</code> is preferred since it's an actual folder instead of a symlink.</p> <p></p> <p>You can see the current working directory at the top (green) along with its file and folder list (black). There are also control bars for both working with files (blue) as well as the file browser itself (orange).</p>"},{"location":"cheaha/open_ondemand/ood_layout/#renamingmoving-file","title":"Renaming/Moving File","text":"<p>You can use the <code>Rename/Move</code> button in the control bar to either <code>rename</code> a selected file with a desired name or <code>move</code> it to a preferred location by specifying the full path of the destination.</p> <p></p> <p>Danger</p> <p>Make sure you never leave the box empty while renaming the files! If it's left empty, you risk losing the files, which cannot be recovered. This problem has been noted in OOD version &lt; 2.0.0.</p> <p>Below is an example that shows how leaving the box empty while renaming the folder \"test_rename\" results in the loss of the folder itself.</p> <p></p> <p></p>"},{"location":"cheaha/open_ondemand/ood_layout/#ood-command-menu","title":"OOD Command Menu","text":""},{"location":"cheaha/open_ondemand/ood_layout/#uploading-data","title":"Uploading Data","text":"<p>Data can be uploaded from your local machine using this interface. Use the <code>Upload</code> button in the OOD Command Menu at the top right to select files from your local browser.</p> <p>Important</p> <p>This should be limited to small files only (&lt; 1 MB). For large files or datasets, please use Globus instead.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#opening-a-terminal","title":"Opening a Terminal","text":"<p>You can also open a bash terminal in the current directory using the <code>&gt;_Open in Terminal</code> command. This should only be used for small tasks since the terminal is running on the login node. For compute-intensive tasks, either request an interactive session in the terminal or request an HPC Desktop session through the Interactive Apps and use the terminal there.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#show-dotfiles","title":"Show Dotfiles","text":"<p>Selecting the <code>Show Dotfiles</code> option will show the hidden files (those beginning with <code>.</code>) in the current folder.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#show-owner","title":"Show Owner","text":"<p>Selecting the <code>Show Owner/Mode</code> option will show the permissions for the files in the working directory. These permissions (<code>mode</code>) can be used to investigate permission issues in shared spaces like <code>/data/project</code> directories. The <code>owner</code> column shows the Unix ID for the user who owns the file or directory. There is not a known way to change it to the username so its use is limited.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#interactive-apps","title":"Interactive Apps","text":"<p>There are two tabs used to interact with applications in OOD, the Interactive Apps dropdown, used to select resources and start jobs, and the My Interactive Sessions tab, used to view currently running interactive apps. As shown below. </p>"},{"location":"cheaha/open_ondemand/ood_layout/#creating-an-interactive-job","title":"Creating an Interactive Job","text":"<p>The Interactive Apps dropdown has a list of specific software setup to run on Cheaha that you can interact with through a browser window such as RStudio, MATLAB, and Jupyter. There is also an HPC Desktop app that provides a general VNC desktop to run all available software on Cheaha. See the quickstart for how to create an example HPC Desktop job. Below, you can see a general form for selecting job resources.</p> <p></p> <p>The interactive apps have the following fields to customize the resources for your job:</p> <ol> <li>Number of Hours: the maximum number of hours the job will run for. Interactive apps will stay allocated for this amount of time unless the job is manually deleted or crashes. The selected number of hours should be less than or equal to the max runtime for your selected partition.</li> <li>Partition: the partition the job will be allocated in. See here for more information about which partition to choose for your job.</li> <li>Number of GPUs: Total number of GPUs to request (max of 4 on pascalnodes or 2 on amperenodes)</li> <li>Numer of CPUs: Total number of CPUs to request</li> <li>Memory Per CPU (GB): GB of memory multiplied by the requested number of CPUs.</li> </ol> <p>Note</p> <p>The \"number of GPUs\" field is ignored if a partition is selected that has no GPUs.</p> <p>Tip</p> <p>You can decrease wait time in the queue by choosing resources carefully. The closer your request is to actual usage, the more optimal your wait time will be. Please see our section on Job Efficiency for more information.</p> <p>Every interactive app has resources only allocated on a single node, and resources are shared among all processes running in the app. Make sure the amount of memory you request is less than or equal to the max amount per node for the partition you choose. We have a table with memory available per node for each partition.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#environment-setup-window","title":"Environment Setup Window","text":"<p>In addition to requesting general resources, for some apps you will have the option to add commands to be run during job startup in an Environment Setup Window. See below for an example showing how to load CUDA into a Jupyter job so it can use a GPU.</p> <p></p> <p>For jobs such as RStudio and Jupyter, some modules like CUDA need to be loaded beforehand so the application has access to it. This can also include loading compiler modules such as CMake and GCC for compiling package installations or editing your <code>$PATH</code> specifically for the interactive app without needing to edit your <code>.bashrc</code>. See the software specific pages for more examples on how to use the Environment Setup.</p> <p>Note</p> <p>In the OOD session, the module is automatically reset at the beginning of every session by default. Therefore, avoid using <code>module reset</code> in the 'Environment Setup' box. See best practice for loading modules for more information.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#launching-interactive-sessions","title":"Launching Interactive Sessions","text":"<p>Once you have completed the form fields with the necessary parameter for the job, click on the blue <code>Launch</code> button. The interactive session will be initiated and placed into the scheduling queue, changing the job state to <code>Queued</code>.</p> <p></p> <p>Depending on the resources requested, you may need to wait for some time. After the resources are allocated, the job state will change to <code>Starting</code>.</p> <p></p> <p>Once the job is launched on the compute nodes, the state will switch to <code>Running</code>. You will then see the option <code>Launch Desktop in new tab</code> button. Click this button to open the interactive VNC session in a new tab. Alternatively, you can also click the blue button in the <code>Host</code> field to open a terminal directly. This terminal is opened on the compute node and so can run any commands you need.</p> <p></p>"},{"location":"cheaha/open_ondemand/ood_layout/#my-interactive-sessions","title":"My Interactive Sessions","text":"<p>The My Interactive Sessions page lists the available apps and your current interactive sessions. If you are logged out, disconnected, or lose track of an interactive application (because of a closed tab or computer shutdown) you can reconnect to running applications on this page. The My Interactive Sessions page looks like:</p> <p></p> <p>For each job running via Open OnDemand, there will be a card listed on this page:</p> <ol> <li>Job ID: The jobID assigned by the SLURM scheduler for this specific job.</li> <li>Host: The node on which the job is currently running.</li> <li>Time Remaining: The amount of time remaining from the total requested time.</li> <li>Session ID: This is the unique ID for the OOD session for this job, which can be clicked to access the OOD log directory for troubleshooting.</li> <li>Node, Cores and State: Information about the number of node, cores assignment, and state of the job.</li> <li>Launch Desktop in new tab: Click this button to open your interactive VNC session.</li> <li>Delete: Click this button if you want to cancel/stop a running job, and/or delete the session if the job has already ended.</li> <li>View Only (Share-able Link): Click this button to share the URL of your job with someone. It allows them to watch as you interact with the program and assist you. However, they can only view and cannot control or enter any data.</li> </ol> <p>The Job ID and Session ID are important for diagnosing issues you may encounter on Cheaha while using Open OnDemand. These interactive jobs can be stopped early by clicking the <code>Delete</code> button on the job card.</p> <p>Bug</p> <p>If your job fails to launch, see debugging OOD jobs for instructions on how to access OOD job data and submit a support ticket.</p>"},{"location":"cheaha/open_ondemand/ood_layout/#debugging-ood-job-failures","title":"Debugging OOD Job Failures","text":"<p>On occasion, interactive jobs created in OOD will crash on startup and cause the job card to disappear. Most of these failures are caused by improper environment setup prior to job creation. If you experiencing OOD job failures, retrieve the OOD job info using the following steps:</p> <ol> <li>Create a new job with the same setup as the job that failed.</li> <li> <p>When the job is in queue, click the link in the <code>Session ID</code> field in the job card before the job fails (see the image below for an example). This will open a file browser in a new tab.</p> <p></p> </li> <li> <p>Wait for the job to fail. Afterwards, refresh the file browser, select all of the files (do not include the <code>desktops</code> or <code>..</code> folders), and click <code>Download</code>.</p> </li> </ol> <p></p> <ol> <li>Take all of the files that were downloaded, put them in a new folder, and zip the folder.</li> </ol> <p>Submit a ticket to us explaining the issue with the zip folder created in Step 4 attached to the email, and we will be happy to assist. If you would like to inspect the log yourself for debugging, the <code>output.log</code> typically will contain the relevant error messages.</p>"},{"location":"cheaha/open_ondemand/ood_matlab/","title":"Matlab","text":"<p>Matlab is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below.</p> <p></p> <p>Warning</p> <p>Matlab tends to consume substantial memory at startup. You may experience difficulty with job errors below 20 GB of total memory.</p>"},{"location":"cheaha/open_ondemand/ood_matlab/#using-anaconda-python-from-within-matlab","title":"Using Anaconda Python from within Matlab","text":"<p>Matlab has the ability to interoperate with Python from within Matlab. The official documentation for this featuer may be found at https://www.mathworks.com/help/matlab/call-python-libraries.html.</p> <p>This section is dedicated to using this feature with Anaconda on Cheaha. To use Python contained in an Anaconda Environment within Matlab, please use the following steps.</p> <ol> <li>Create an HPC Interactive Desktop Job.</li> <li>Open a terminal in that job. The following steps should all be run in this terminal unless otherwise specified.</li> <li>Load the Anaconda Module.</li> <li>Create an Environment in Anaconda with the packages needed.</li> <li>Activate the Environment,</li> <li>Load the Matlab Module.</li> <li>Start Matlab by entering the command <code>matlab</code>.</li> <li> <p>Verify success by entering <code>pyenv</code> at the Matlab prompt (not the terminal window). Multiple lines of text will be returned at the prompt. Among them you should see a line like the following, with your environment name in place of <code>&lt;env_name&gt;</code>.</p> <pre><code>Executable: /home/$USER/.conda/envs/&lt;env_name&gt;/bin/python\n</code></pre> </li> </ol> <p>You may optionally verify that Python works correctly by entering <code>py.list([\"hello\", \"world\"])</code>. A python list object should appear in the workspace.</p>"},{"location":"cheaha/open_ondemand/ood_matlab/#using-a-gpu-with-matlab","title":"Using a GPU with MATLAB","text":"<p>Please see the MATLAB Section on our GPU Page.</p>"},{"location":"cheaha/open_ondemand/ood_matlab/#known-issues","title":"Known Issues","text":"<p>There is a known issue with <code>parpool</code> and other related multi-core parallel features such as <code>parfor</code> affecting R2022a and earlier. See our Modules Known Issues section for more information.</p>"},{"location":"cheaha/open_ondemand/ood_rstudio/","title":"RStudio","text":"<p>RStudio is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. You'll also need to select both the version of RStudio you wish to use, and the version of R you wish to use. To adjust the environment, please use the Environment Setup field to load modules besides R and RStudio as seen below. All other modules and paths should be loaded here as it is difficult to load and consistently use modules once RStudio starts.</p> <p></p> <p>Important</p> <p>Unless an older version of R is absolutely necessary, it is highly suggested to always use the newest version of R and RStudio for both updated functionality within those software as well as updated compilers for package installation. Using the newest version of R solves most known package installation errors.</p>"},{"location":"cheaha/open_ondemand/ood_rstudio/#rstudio-and-python","title":"RStudio and Python","text":"<p>If you have a workflow that uses both R and Python, it is strongly recommended to use the reticulate package along with Anaconda environments. Reticulate allows researchers to load Python packages into a native R session as objects. For instance, if someone prefer some functionality of the <code>pandas</code> package but has other code already written in R, they can import <code>pandas</code> to R and use both simultaneously.</p> <p>This also allows researchers to download precompiled command line binaries into an Anaconda environment and easliy use them in their R scripts.</p> <p>For setup, use the following steps:</p> <ol> <li> <p>In a terminal on a compute node, either in an HPC Desktop job or by clicking the blue Host button on any job card:</p> <ol> <li>Load the <code>Anaconda3</code> module</li> <li>Create an Anaconda environment. More information about how to create Anaconda environments can be found in our documentation.</li> <li>Activate your environment and install your requuired python packages using either <code>pip install</code> or <code>conda install</code> depending on the package source.</li> </ol> <p>Note</p> <p>The preceding steps should only need to be run once. If other Python packages need to be installed in the same environment, repeat steps 1 and 3. You will not need to recreate your environment.</p> </li> <li> <p>In RStudio:</p> <ol> <li>Add the command <code>module load Anaconda3</code> to the Environment Setup window when requesting the RStudio job.</li> <li>If not already installed, install the <code>reticulate</code> package using either <code>install.packages</code> or the renv package.</li> <li>Use <code>reticulate::use_condaenv('env_name')</code> to load your conda environment.</li> <li>From here, you will be able to interact with all of the python packages and non-python precompiled binaries in your Anaconda environment using R and RStudio. Please read more about how to do that in reticulate's documentation.</li> </ol> </li> </ol> <p>For cases where your R code only needs access to precompiled binaries or libraries and does not need to import any Python libraries, you can instead create your Anaconda environment and add the following lines into the Environment Setup window:</p> <pre><code>module load Anaconda3\nconda activate &lt;env_name&gt;\n</code></pre> <p>This will add those binaries and libraries to your environment <code>$PATH</code> which RStudio will inherit.</p> <p>Important</p> <p>If you're wanting to directly use any Python package in R, DO NOT include the <code>conda activate</code> command in the Environment Setup. Use <code>reticulate</code> instead as described above.</p>"},{"location":"cheaha/open_ondemand/ood_rstudio/#rstudio-projects-and-renv","title":"RStudio Projects and renv","text":"<p>The most recent versions of RStudio installed on Cheaha support R Projects as well as package management through the <code>renv</code> package. Please read more about improving analysis reproducibility using both of these tools in our workflow solutions</p>"},{"location":"cheaha/open_ondemand/ood_rstudio/#using-pandoc-and-knitr-within-rstudio","title":"Using Pandoc and <code>knitr</code> within RStudio","text":"<p>If you want to use RMarkdown to create reports in RStudio, R modules using version 4.2.0 and later include <code>knitr</code> compatibility. Please use the latest versions of both R and Rstudio for fully integrated <code>knitr</code> functionality.</p>"},{"location":"cheaha/open_ondemand/ood_rstudio/#starting-with-a-clean-session-to-avoid-errors","title":"Starting With a Clean Session to Avoid Errors","text":"<p>By default, RStudio loads the most recently opened project at startup and restores the <code>.RData</code> file into the workspace. If you only work on a single project, this may be helpful. If you frequently change projects then these default settings can create difficult-to-diagnose errors, or you may inadvertently alter a project by adding incorrect packages, for example.</p> <p>To reduce the risk of these kinds of errors, uncheck the highlighted fields below in the RStudio Options menu under the \"General\" selection.</p> <ul> <li>Restore most recently opened project at startup</li> <li>Restore .RData into workspace at startup</li> </ul> <p></p>"},{"location":"cheaha/slurm/gpu/","title":"GPUs","text":""},{"location":"cheaha/slurm/gpu/#available-devices","title":"Available Devices","text":"<p>Cheaha has GPUs available with the following statistics, broken out by Slurm Partition. For more information on all available partitions, see our Hardware Summary.</p> <code>pascalnodes</code> <code>amperenodes</code> Product Name P100 A100 80GB Architecture Pascal Ampere CUDA Compute Capability Version 6.0 8.0 CUDA Cores 3584 6912 Memory (GB) 16 80 Memory Bandwidth (GB/s) 720 2039 NVLink Bandwidth (GB/s) 160 600 FP32 performance (TFLOPs) 10.6 19.5 <p>For more information on these nodes, see <code>Detailed Hardware Information</code>.</p>"},{"location":"cheaha/slurm/gpu/#scheduling-gpus","title":"Scheduling GPUs","text":"<p>To submit a job with one or more GPUs, you will need to set the partition to <code>pascalnodes</code> or <code>amperenodes</code> family of partitions for P100 GPUs or <code>amperenodes</code> family for A100 GPUs.</p> <p>When requesting a job using <code>sbatch</code>, you will need to include the Slurm flag <code>--gres=gpu:#</code>. Replace <code>#</code> with the number of GPUs you need. Quotas and constraints are available on our Hardware Summary</p> <p>Note</p> <p>It is suggested that at least 2 CPUs are requested for every GPU to begin with. The user should monitor and adjust the number of cores on subsequent job submissions if necessary. Look at Managing Jobs for more information.</p>"},{"location":"cheaha/slurm/gpu/#ensuring-io-performance-with-a100-gpus","title":"Ensuring IO Performance With A100 GPUs","text":"<p>If you are using <code>amperenodes</code> and the A100 GPUs, then it is highly recommended to move your input files to <code>/local/$SLURM_JOB_ID</code> prior to running your workflow, to ensure adequate GPU performance. Using <code>$USER_SCRATCH</code>, or other network file locations, will starve the GPU of data, resulting in poor performance.</p> <p>The following script can be used to wrap your existing workflows. It will automatically create a temporary directory <code>$TMPDIR</code> and delete it when your workflow is finished. You'll need to supply the original source of your data as <code>$MY_DATA_DIR</code>. The script is not guaranteed to delete the temporary directory if the job ends before it reaches the final line, so please be mindful and periodically check for any extra temporary directories and delete them as needed.</p> <pre><code>#!/bin/bash\n#SBATCH ...\n#SBATCH --partition=amperenodes\n#SBATCH --gres=gpu:1\n\n# LOAD CUDA MODULES\nmodule load CUDA/12.1.1\nmodule load cuDNN/12.1.1\n\n# CREATE TEMPORARY DIRECTORY\n# WARNING! $TMPDIR will be deleted at the end of the script!\n# Changing the following line can cause permanent, unintended deletion of important data.\nTMPDIR=\"/local/$USER/$SLURM_JOB_ID\"\nmkdir -p \"$TMPDIR\"\n\n# COPY RESEARCH DATA TO LOCAL TEMPORARY DIRECTORY\n# Replace $MY_DATA_DIR with the path to your data folder\ncp -r \"$MY_DATA_DIR\" \"$TMPDIR\"\n\n# YOUR ORIGINAL WORKFLOW GOES HERE\n# be sure to load files from \"$TMPDIR\"!\n\n# CLEAN UP TEMPORARY DIRECTORY\n# WARNING!\n# Changing the following line can cause permanent, unintended deletion of important data.\nrm -rf \"$TMPDIR\"\n</code></pre>"},{"location":"cheaha/slurm/gpu/#open-ondemand","title":"Open OnDemand","text":"<p>When requesting an interactive job through <code>Open OnDemand</code>, selecting the <code>pascalnodes</code> partitions will automatically request access to one GPU as well. There is currently no way to change the number of GPUs for OOD interactive jobs.</p>"},{"location":"cheaha/slurm/gpu/#matlab","title":"MATLAB","text":"<p>To use GPUs with our Open OnDemand MATLAB app, you may need to take a slightly different route than usual.</p> <p>If you are using MATLAB R2022a or newer, then our <code>pascalnodes</code> P100 GPUs and <code>amperenodes</code> A100 GPUs should work without any additional steps.</p> <p>If you are using R2021b and earlier, then follow the instructions below.</p> <ol> <li>Start an HPC Interactive Desktop Job with appropriate resources. Be sure to use one of the <code>pascalnodes*</code> Partitions.</li> <li>Open a terminal.</li> <li>Load the appropriate CUDA Module.<ul> <li>Determine which CUDA Modules are compatible with your required version of MATLAB using the table at the MathWorks Site.</li> <li>Check the <code>Pascal (cc6.x)</code> column for the <code>pascalnodes</code> P100 GPUs and <code>Ampere (cc8.x)</code> column for the <code>amperenodes</code> A100 GPUs.</li> <li>As of September, 2023, <code>module load CUDA/11.6.0</code> and newer should work fine with any version of MATLAB R2021b or older, with possible caveats for some functions.</li> </ul> </li> <li>Load the appropriate MATLAB Module.</li> <li>Start MATLAB by entering the command <code>matlab</code>.</li> <li>When MATLAB loads, enter the command <code>gpuDevice</code> in the MATLAB Command Window to verify it can identify the GPU.</li> </ol> <p>For more information and official MATLAB documentation please see this page: https://www.mathworks.com/help/parallel-computing/gpu-computing-requirements.html.</p>"},{"location":"cheaha/slurm/gpu/#cuda-modules","title":"CUDA Modules","text":"<p>You will need to load a CUDA module to make use of GPUs on Cheaha. Depending on which version of software you are using, different versions of CUDA module may be required. For instance, tensorflow version <code>2.13.0</code> requires the <code>CUDA/11.8.0</code> module. To see which versions are available on Cheaha, use the following command at the terminal.</p> <pre><code>module -r spider 'CUDA/*'\n</code></pre> <p>If a specific version of CUDA is needed but not installed, please send an install request to support@listserv.uab.edu.</p>"},{"location":"cheaha/slurm/gpu/#cudnn-modules","title":"cuDNN Modules","text":"<p>If working with deep neural networks (DNNs, CNNs, LSTMs, LLMs, etc.), you will need to load a <code>cuDNN</code> module as well. The <code>cuDNN</code> modules are built to be compatible with a sibling <code>CUDA</code> module and are named with the corresponding version. For example, if you are loading <code>CUDA/12.2.0</code>, you will also need to load <code>cuDNN/8.9.2.26-CUDA-12.2.0</code>.</p>"},{"location":"cheaha/slurm/gpu/#tensorflow-compatibility","title":"Tensorflow Compatibility","text":"<p>To check which CUDA Module version is required for your version of Tensorflow, see the toolkit requirements chart here https://www.tensorflow.org/install/source#gpu.</p>"},{"location":"cheaha/slurm/gpu/#pytorch-compatibility","title":"PyTorch Compatibility","text":"<p>PyTorch does not maintain a simple compatibility table for CUDA versions. Instead, please manually check their \"get started\" page for the latest PyTorch version compatibility, and their \"previous versions\" page for older PyTorch version compatibility. Assume that a CUDA version is not compatible if it is not listed for a specific PyTorch version.</p> <p>To use GPUs prior to PyTorch version 1.13 you must select a <code>cudatoolkit</code> version from the PyTorch channel when you install PyTorch using Anaconda. It is how PyTorch knows to install a GPU compatible flavor, as opposed to the CPU only flavor. See below for templates of CPU and GPU installs for PyTorch versions prior to 1.13. Be sure to check the compatibility links above for your selected version. Note <code>torchaudio</code> is also available for signal processing.</p> <ul> <li>CPU Version: <code>conda install pytorch==... torchvision==... -c pytorch</code></li> <li>GPU Version: <code>conda install pytorch==... torchvision==... cudatoolkit=... -c pytorch</code></li> </ul> <p>For versions of PyTorch 1.13 and newer, use the following template instead.</p> <ul> <li>CPU Version: <code>conda install pytorch==... torchvision==... cpuonly -c pytorch</code></li> <li>GPU Version: <code>conda install pytorch==... torchvision==... pytorch-cuda=... -c pytorch -c nvidia</code></li> </ul> <p>Note</p> <p>When loading modules, such as CUDA modules for jobs requiring one or more GPUs, always utilize <code>module reset</code> before loading modules, both at the terminal and within <code>sbatch</code> scripts. See best practice for loading modules for more information.</p>"},{"location":"cheaha/slurm/gpu/#reviewing-gpu-jobs","title":"Reviewing GPU Jobs","text":"<p>As with all jobs, use <code>sacct</code> to review GPU jobs. Quantity of GPUs may be reviewed using the <code>reqtres</code> and <code>alloctres</code> fields.</p>"},{"location":"cheaha/slurm/gpu/#frequently-asked-questions-faq-about-a100-gpus","title":"Frequently Asked Questions (FAQ) About A100 GPUs","text":"<ul> <li>I've been using the P100 GPUs on <code>pascalnodes</code> up until now, what is the easiest way to start using the A100 GPUs?<ul> <li>If you are using an <code>sbatch</code> script...<ul> <li>Change <code>--partition=pascalnodes</code> to <code>--partition=amperenodes</code>, or change <code>--partition=pascalnodes-medium</code> to <code>--partition=amperenodes-medium</code>.</li> <li>Also change <code>--gres=gpu:3</code> and <code>--gres=gpu:4</code> to <code>--gres=gpu:2</code>, as there are only two A100 GPUs per node.</li> </ul> </li> <li>If you are using an Open OnDemand Interactive App...<ul> <li>Change the partition from \"pascalnodes\" to \"amperenodes, or change \"pascalnodes-medium\" to \"amperenodes-medium\".</li> </ul> </li> <li>In all cases, be sure to read the section on Ensuring IO Performance With A100 GPUs to be sure disk read speed doesn't limit your performance gains.</li> </ul> </li> <li>How do I access the A100 GPUs?     You can access the A100 GPUs by request jobs in the appropriate partitions. Use <code>amperenodes</code> partition for up to 12 hours or <code>amperenodes-medium</code> partition for up to 48 hours.</li> <li>How many GPUs can I request at once?     Up to four GPUs may be requested by any one researcher at once. However, there are only two GPUs per node, so requesting four GPUs will allocate two nodes. To make use of multiple nodes, your workflow software must know how to communicate between nodes using software like Horovod or OpenMPI. If you are new to GPUs and aren't sure you need multiple nodes, please limit your request to one or two gpus.</li> <li>What performance improvements can I expect over the P100 GPUs?     Performance improvements depend on the software and algorithms being used. Determining optimal configuration will take some experimenting. Swapping a single P100 to a single A100, you can generally expect 3x to 20x improvement. For more information about possible performance improvements, please see the Official NVIDIA A100 page.</li> <li>How can I make the most efficient use of the A100 GPUs?     A100s process data very rapidly compared with previous technology. Ideally, we want the A100 to be the bottleneck during processing, rather than CPUs or I/O operations. Here are two initial possibilities to consider for optimizing efficiency:<ul> <li>All researchers should copy their input data onto <code>/local/$SLURM_JOB_ID</code> (node-specific NVMe drives) before processing to avoid I/O bottlenecks reducing performance. See Ensuring IO Performance With A100 GPUs.</li> <li>Some researchers may benefit from using a larger number of CPU cores for data loading and preprocessing, compared with <code>pascalnodes</code>. Please consider experimenting with different numbers of CPU cores using the same dataset to find what is optimal for you. If you feel that performance should be higher, please contact Support so we can guide you toward an optimal CPU-to-GPU ratio for your application and workflow.</li> </ul> </li> <li>Where are the A100 nodes physically located, and will this impact my workflows?     The A100 nodes are located in the DC BLOX Data Center, west of UAB Campus. Because Cheaha storage (GPFS) is located on campus, there may be slightly higher latency when transferring data between the A100 nodes and GPFS. Impacts will only occur if very small amounts of data are transferred very frequently, which is unusual for most GPU workflows. We strongly recommend copying your input data onto <code>/local/$SLURM_JOB_ID</code> prior to processing, see Ensuring IO Performance With A100 GPUs.</li> <li>What will happen to the P100 GPUs?     We intend to retain all of the 18 existing P100 GPU nodes, of which  9 nodes are available now. The remaining 9 nodes have been temporarily taken offline as we reconfigure hardware, and will be reallocated based on demand and other factors.</li> <li>What else should I be aware of?<ul> <li>Please be sure to clean your data off of <code>/local/$SLURM_JOB_ID</code> as soon as you no longer need it, before the job finishes.</li> <li>We have updated the CUDA and cuDNN modules to improve reliability and ease of use. Please see the section on CUDA Modules for more information.</li> </ul> </li> </ul>"},{"location":"cheaha/slurm/introduction/","title":"Introduction to Slurm","text":"<p>All work on Cheaha must be submitted to the queueing system, Slurm. This doc gives a basic overview of Slurm and how to use it.</p> <p>Slurm is software that gives researchers fair allocation of the cluster's resources. It schedules jobs based using resource requests such as number of CPUs, maximum memory (RAM) required per CPU, maximum run time, and more.</p> <p>The main Slurm documentation can be found at the Slurm site. The Slurm Quickstart can also be helpful for orienting researchers new to queueing systems on the cluster.</p>"},{"location":"cheaha/slurm/introduction/#batch-job-workflow","title":"Batch Job Workflow","text":"<ol> <li>Stage data to <code>$USER_DATA</code>, <code>$USER_SCRATCH</code>, or a <code>/data/project/...</code> directory.</li> <li>Research how to run your directives in 'batch' mode. In other words, how to run your analysis pipeline from the command line, with no GUIs or researcher input.</li> <li>Identify the appropriate resources necessary to run the jobs (CPUs, time, memory, etc)</li> <li>Write a job script specifying these parameters using Slurm directives.</li> <li>Submit the job (<code>sbatch</code>)</li> <li>Monitor the job (<code>squeue</code>)</li> <li>Review the results, and modify/rerun if necessary (<code>sacct</code> and <code>seff</code>)</li> <li>Remove data from <code>$USER_SCRATCH</code></li> </ol> <p>For more details, please see Submitting Jobs.</p> <p>For details on managing and reviewing jobs, please see Job Management.</p>"},{"location":"cheaha/slurm/introduction/#the-slurm-queue","title":"The Slurm Queue","text":"<p>When working on Cheaha and with Research Computing, you will often hear references to the Slurm Queue. By its name, you might think that the Slurm Queue is a first-in-first-out (FIFO) queue like when waiting in line at an event or place of business. And, some institutions use a FIFO queue, as it is the default configuration for Slurm.</p> <p>At UAB Research Computing, we use a multifactor priority queue, meaning that those users with top priority are first to receive service, regardless of when they entered the queue.</p> <p>Slurm measures priority as a single number, and the highest value generally is first to receive service. Multiple factors play into the queue. The most important factors are given in the table below, in no particular order.</p> Factor Description What Gives Higher Priority Example Age Lenght of time job has spent in queue. Longer time in queue Job in queue for 2 days will start faster than for 4 hours. Resources Quantity of resources requested for job. Request fewer resources 1 CPU will start faster than 4 CPUs. 2 hour time limit will start faster than 10 hours. Partition Which partition was requested for job. Lower resource partitions Express partition will start faster than Long. NOT related to Priority Tier. Fair Share What fraction of cluster resources are already being used by you. Fewer jobs currently running Having 1 job already running will start faster than having 10 of the same job. <p>The fastest way to queue a job is to request minimal resources and time, have a smaller share of total resources already used, and use the shortest partition possible.</p> <p>Given two or more jobs with equal priority, the job on the partition with the largest \"Priority Tier\" value goes first.</p> <p>The scheduler cannot predict the future. If a job enters the queue with a higher priority than yours, it will start before yours. This may lead to a situation where your job no longer fits on any of the nodes. If this happens your job will have to wait until sufficient space opens regardless of its priority value. A possible strategy to minimize the risk of preemption is to request fewer resources per node, to more readily fill available space.</p> <p>A lot of intelligent people have worked very hard to design the Slurm scheduler to be both smart and fair. Please allow it to do its job. If you have workloads that are very large and want advice or are unsure of the best queueing strategy for your workflow, please Contact Us for a consultation, we are happy to help.</p> <p>Important</p> <p>Please do not run <code>squeue</code>, or any other Slurm command, in a loop. All Slurm commands increase the load on the Slurm controller. Many commands in a short period of time can make Slurm unresponsive, unstable, or require a restart, which negatively impacts all researchers.</p> <p>Instead, please simply let the Slurm scheduler do its job of managing.</p>"},{"location":"cheaha/slurm/job_management/","title":"Managing Jobs","text":"<p>When jobs are submitted, researchers can monitor their status using Slurm commands. Additionally, researchers can get information about completed jobs regarding their CPU and memory usage during execution for planning future jobs. Both of these cases should be a regular part of using Cheaha for researchers.</p> <p>In case jobs were submitted by accident or the code was written incorrectly, they can also be cancelled.</p>"},{"location":"cheaha/slurm/job_management/#monitoring-queued-jobs-with-squeue","title":"Monitoring Queued Jobs with <code>squeue</code>","text":"<p>Currently running jobs can be monitored using the <code>squeue</code> command. The basic command to list all jobs for a specific researcher is:</p> <pre><code>squeue -u $USER\n</code></pre> <p>The output of <code>squeue</code> will look like:</p> <p></p> <p>By default the fields displayed are <code>jobid</code>, <code>partition</code>, <code>jobname</code> as <code>name</code>, BlazerID as <code>user</code>, job state as <code>st</code>, total run time as <code>time</code>, number of nodes as <code>node</code>, and the list of nodes as <code>nodelist</code>, used for each job a researcher has submitted.</p> <p>For array jobs, the JobID will be formatted as <code>jobid_arrayid</code>.</p> <p>More information is available at the Official Documentation.</p>"},{"location":"cheaha/slurm/job_management/#cancelling-jobs-with-scancel","title":"Cancelling Jobs with <code>scancel</code>","text":"<p>Cancelling queued and currently running jobs can be done using the <code>scancel</code> command. Importantly, this will only cancel jobs that were initiated by the researcher running the command. <code>scancel</code> is very flexible in how it behaves:</p> <pre><code># cancel a single job or an entire job array\nscancel &lt;jobid&gt;\n\n# cancel specific job array IDs, specified as single number or a range\nscancel &lt;jobid_arrayid&gt;\n\n# cancel all jobs on a partition for the user\nscancel -p &lt;partition&gt;\n\n# cancel all jobs for a researcher\nscancel -u $USER\n</code></pre> <p>Warning</p> <p>Cancelling all jobs will also cancel the interactive jobs created on the Open OnDemand portal.</p> <p>More information is available at the Official Documentation.</p>"},{"location":"cheaha/slurm/job_management/#reviewing-past-jobs-with-sacct","title":"Reviewing Past Jobs with <code>sacct</code>","text":"<p>If you are planning a new set of jobs and are estimating resource requests, it is useful to review similar jobs that have already completed. To list past jobs for a researcher, use the <code>sacct</code> command. Common use cases and information are detailed below. Full details are available at the Official Documentation.</p> <p>Tip</p> <p>To minimize queue wait times and make best use of resources, please review job efficiency using <code>seff</code>. See our Job Efficiency page for more information.</p>"},{"location":"cheaha/slurm/job_management/#review-jobs-by-jobid","title":"Review Jobs by JobID","text":"<p>The basic form is to use <code>-j</code> along with a JobID to list information about that job.</p> <pre><code>sacct -j &lt;jobid&gt;\n</code></pre> <p>You can also review multiple jobs using a comma-separated list of JobIDs.</p> <p>This command will output basic information such as the ID, Name, Partition, Allocated CPUs, and State for the given JobID.</p> <p>Jobs can have matching extern and/or batch job entries as well. These are not especially helpful for most researchers. You can remove these entries using the <code>-X</code> flag.</p> <pre><code>sacct -j &lt;jobid&gt; -X\n</code></pre>"},{"location":"cheaha/slurm/job_management/#review-jobs-submitted-between-specific-timepoints","title":"Review Jobs Submitted Between Specific Timepoints","text":"<p>If you do not remember the JobID, you can use the <code>-S</code> and <code>-E</code> flags to retrieve jobs submitted between the given start datetime and end datetime.</p> <p>For example, to retrieve jobs submitted during the month of July 2021, the command could be:</p> <pre><code>sacct -S 070121 -E 073121\nsacct -S 07/01/21 -E 07/31/21\nsacct -S 2021-07-01 -E 2021-07-31\n</code></pre>"},{"location":"cheaha/slurm/job_management/#customizing-the-output","title":"Customizing the Output","text":"<p>You can add <code>-o</code> with a list of output fields to customize the information you see.</p> <pre><code>sacct -o jobid,start,end,state,alloccpu,reqmem\n</code></pre> <p>You may also use the format <code>&lt;field&gt;%&lt;width&gt;</code> to make columns be <code>&lt;width&gt;</code> characters wide. This is sometimes necessary for TRES fields and <code>nodelist</code>, among others. An example might be <code>alloctres%40</code> to make the field 40 characters wide.</p> <p>This command will output the JobID, the start time, end time, the state, the number of allocated CPUs, and the requested memory for the specified job. All potential output fields can be seen using <code>sacct --helpformat</code>. Their descriptions can be found on the sacct documentation under Job Accounting Fields.</p>"},{"location":"cheaha/slurm/job_management/#formatting-the-output","title":"Formatting the Output","text":"<p>You can format the output of <code>sacct</code> using a delimiter with the flags <code>--parsable2</code> and <code>--delimiter=&lt;delim&gt;</code>. Any number of characters may be used as a delimiter. The default is <code>|</code>. It is not recommended to use <code>,</code> as that is used in comma-separated lists throughout <code>sacct</code> fields.</p>"},{"location":"cheaha/slurm/job_management/#sacct-flags","title":"<code>sacct</code> Flags","text":"Flag Short Description Docs FILTERING <code>--user</code> <code>-u</code> Jobs from a specific user. Please only use your own BlazerID. sacct <code>--allocations</code> <code>-X</code> Show jobs only, not steps. sacct <code>--starttime</code> <code>-S</code> Jobs starting at a given time. See Time formatting. sacct <code>--endtime</code> <code>-E</code> Jobs ending at a given time. See Time Formatting. sacct <code>--state</code> <code>-s</code> Jobs with a given state. See States. sacct <code>--jobs</code> <code>-j</code> Show only the jobids supplied in a comma-separated list. sacct FORMATTING <code>--format</code> <code>-o</code> Show only the Fields supplied in a comma-separated list. sacct <code>--helpformat</code> <code>-e</code> Show a list of available Fields. sacct <code>--parsable2</code> <code>-P</code> Output as delimited data with <code>--delimiter</code> if supplied, default is <code>\\|</code>. sacct <code>--delimiter</code> n/a Characters to delimit field values. sacct <code>--json</code> n/a Output as JSON. (Not yet available on Cheaha). sacct <code>--yaml</code> n/a Output as YAML. (Not yet available on Cheaha). sacct <code>--noconvert</code> n/a Keep uniform units, e.g. all M instead of M and G. See Units. sacct <p>A complete list of flags is available at Official Documentation.</p>"},{"location":"cheaha/slurm/job_management/#sacct-fields","title":"<code>sacct</code> Fields","text":"Field Description Same As... Job Step Docs METADATA jobid Slurm assigned job ID number. jobid format yes yes sacct jobname User assigned job name. <code>--job-name</code> yes yes sacct state Current state of the job. states yes yes sacct partition Partition job was submitted to. <code>--partition</code> yes yes sacct ntasks Number of requested tasks. <code>--ntasks</code> yes yes sacct nodelist List of nodes used. <code>--nodelist</code> if supplied yes yes sacct TIME submit Submit time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct start Start time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct end End time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct elapsed Elapsed time as DD-HH:MM:SS n/a yes yes sacct RESOURCE REQUESTED reqcpus CPUs requested. cpu calculation yes yes sacct reqmem Memory requested. Uses 10Gc for per core, 10Gn for per node. <code>--mem-per-cpu</code> or <code>--mem</code> yes no sacct reqnodes Nodes requested. <code>--nodes</code> yes yes sacct reqtres All requested resources. May be used to review GPUs. tres explanation yes yes sacct RESOUCES ALLOCATED alloccpus CPUs allocated. cpu calculation yes yes sacct allocnodes Nodes allocated <code>--nodes</code> yes yes sacct alloctres All allocated resources. May be used to review GPUs. tres explanation yes yes sacct averss Average resident set size (memory) in bytes across tasks. resident set size no yes sacct maxrss Maximum resident set size (memory) in bytes across tasks. resident set size no yes sacct <p>A complete list of fields is available at the Official Documentation.</p>"},{"location":"cheaha/slurm/job_management/#slurm-common-reference","title":"Slurm Common Reference","text":""},{"location":"cheaha/slurm/job_management/#slurm-jobid-formatting","title":"Slurm JobID Formatting","text":"<p>JobID numbers are assigned automatically by the scheduler in the order submissions are received. All jobs have a single, unique JobID number associated with them. Some features will cause JobID numbers to be reported differently than their actual value.</p> <ul> <li>For non-array jobs submitted with <code>sbatch</code>, <code>salloc</code>, or with <code>srun</code> outside of a job context, the unique JobID number is reported directly.</li> <li>For array jobs submitted with <code>sbatch</code>, the array is assigned a master ID like <code>12345678</code>, and each task is reported as <code>&lt;master-job-id&gt;_&lt;task-id&gt;</code>. An example might be <code>12345678_987</code>. Each task still has a unique JobID number.</li> <li>For job steps submitted with <code>srun</code> inside of a job context, the JobID is reported as <code>&lt;job-id&gt;.&lt;task-name&gt;</code>. All jobs submitted generate a <code>.batch</code> step and a <code>.extern</code> step. An example might be <code>12345678.batch</code>.</li> </ul>"},{"location":"cheaha/slurm/job_management/#slurm-time-formatting","title":"Slurm Time Formatting","text":"<p>Slurm formats time in two different ways: (1) time points and (2) durations. Time points are used whenever a single point in time is needed, such as the start or end of a job. Durations are needed for job requests and reported for elapsed times.</p> <p>Units are given a shorthand designations:</p> <ul> <li><code>YYYY</code> four-digit year.</li> <li><code>MM</code> two-digit month or two-digit minutes, depending on placement.</li> <li><code>DD</code> two-digit day.</li> <li><code>HH</code> two-digit hour.</li> <li><code>SS</code> two-digit seconds.</li> <li><code>AM|PM</code> literally AM or PM.</li> </ul> <p>Square brackets <code>[]</code> indicate the contents are optional.</p> <p>Time points may be formatted like any of the following.</p> <pre><code>HH:MM[:SS][AM|PM]\nMMDD[YY][-HH:MM[:SS]]\nMM.DD[.YY][-HH:MM[:SS]]\nMM/DD[/YY][-HH:MM[:SS]]\nYYYY-MM-DD[THH:MM[:SS]]\n</code></pre> <p>Duration requests are made like any of the following.</p> <pre><code>MM[:SS]\n[HH:]MM:SS\nDD-HH[:MM[:SS]]\n</code></pre> <p>Durations are reported like the following.</p> <pre><code>[DD-[HH:]]MM:SS\n</code></pre>"},{"location":"cheaha/slurm/job_management/#slurm-states","title":"Slurm States","text":"<p>Job states report on where the job is in the overall Slurm process. If all goes well, you will see jobs move through the following states:</p> <ol> <li><code>PENDING</code></li> <li><code>RUNNING</code></li> <li>A terminal state depending on what happens<ol> <li><code>COMPLETED</code> if the job finished normally and returns exit code zero</li> <li><code>CANCELLED</code> if the researcher cancels the job</li> <li><code>FAILED</code> if there is a software error or non-zero exit code</li> <li><code>TIMEOUT</code> if the job had insufficient time</li> </ol> </li> </ol> <p>Other states are possible. A complete list of job states is available at the Official Documentation.</p>"},{"location":"cheaha/slurm/job_management/#slurm-units","title":"Slurm Units","text":"<p>Slurm uses flexible units for memory to keep reports compact. It always prefers the shortest possible representation, and will choose the largest units by default. Other units may be used, and there are flags to allow reporting in uniform units.</p> <p>The memory units are <code>KMGT</code> for <code>kilo</code>, <code>mega</code>, <code>giga</code>, <code>tera</code> respectively. All are in bytes. Slurm uses the convention that e.g.</p> \\[ \\begin{aligned} 1\\textrm{T} &amp;=1024\\textrm{G}\\\\ &amp;=1024^{2}\\textrm{M}\\\\ &amp;=1024^{3}\\textrm{K} \\end{aligned} \\]"},{"location":"cheaha/slurm/job_management/#tres-explained","title":"TRES Explained","text":"<p>The abbreviation <code>TRES</code> stands for \"trackable resources\". Any resource made available by Slurm that is trackable is recorded in the Slurm database and can be recovered using sacct. The fields <code>reqtres</code> and <code>alloctres</code> can be used to review CPUs, memory, nodes and GPUs. The data is stored as a comma-separated list of <code>&lt;resource&gt;=&lt;quantity&gt;</code> pairs, and all values are totals across the entire job, not per node or per task. An example might look like:</p> <pre><code>billing=8,cpu=8,gres/gpu=2,mem=64G,node=1\n</code></pre>"},{"location":"cheaha/slurm/job_management/#rss-explained","title":"RSS Explained","text":"<p>The abbreviation <code>RSS</code> stands for \"resident set size\", and is related to memory usage by jobs in Slurm. Memory usage is challenging to record accurately. Recording memory means a request must be made to the operating system to obtain memory usage at a single point in time, which uses computational resources. There is a balance made between resolution in time, and computational overhead.</p> <p>The difficulty with recording memory usage contributes to difficulty diagnosing root causes of out of memory errors, bus errors, and segmentation faults.</p> <p>RSS is recorded by Slurm in the sacct fields <code>averss</code> and <code>maxrss</code>. These values are both reported in bytes, rather than the usual compact memory units.</p>"},{"location":"cheaha/slurm/job_management/#slurm-resource-calculations","title":"Slurm Resource Calculations","text":""},{"location":"cheaha/slurm/job_management/#calculating-cpus","title":"Calculating CPUs","text":"\\[ \\begin{aligned} \\textrm{Total CPUs} &amp;=\\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &amp;=\\left(\\frac{\\textrm{CPU}}{\\textrm{Task}}\\right) \\left(\\frac{\\textrm{Task}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right) \\end{aligned} \\] <p>Example:</p> <p>For a job with <code>--cpus-per-task=16 --ntasks=2 --nodes=3</code>:</p> \\[ \\begin{aligned} \\textrm{Total CPUs} &amp;=16\\times 2\\times 3\\\\ &amp;=96 \\end{aligned} \\]"},{"location":"cheaha/slurm/job_management/#calculating-memory","title":"Calculating Memory","text":"\\[ \\begin{aligned} \\textrm{Total Memory} &amp;=\\left(\\textrm{--mem}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &amp;=\\left(\\frac{\\textrm{Memory}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right)\\\\ \\\\ \\textrm{Total Memory} &amp;=\\left(\\textrm{--mem-per-cpu}\\right) \\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &amp;=\\left(\\frac{\\textrm{Memory}}{\\textrm{CPU}}\\right) \\left(\\frac{\\textrm{CPU}}{\\textrm{Task}}\\right) \\left(\\frac{\\textrm{Task}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right) \\end{aligned} \\] <p>Examples:</p> <p>For a job with <code>--mem=40G --nodes=2</code>:</p> \\[ \\begin{aligned} \\textrm{Total Memory} &amp;=\\left(\\textrm{--mem}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &amp;=40\\textrm{G}\\times 2\\\\ &amp;=80\\textrm{G} \\end{aligned} \\] <p>For a job with <code>--mem-per-cpu=10G --cpus-per-task=8 --ntasks=2 --nodes=2</code>:</p> \\[ \\begin{aligned} \\textrm{Total Memory} &amp;=\\left(\\textrm{--mem-per-cpu}\\right) \\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &amp;=10\\textrm{G}\\times 8\\times 2\\times 2\\\\ &amp;=320\\textrm{G} \\end{aligned} \\]"},{"location":"cheaha/slurm/practical_sbatch/","title":"Practical Examples of <code>sbatch</code> Usage With the <code>--array</code> Flag and Dynamic Indices","text":"<p>Do you find yourself tediously creating many <code>sbatch</code> job scripts for the same type of data set? Or do you modify the same job script? Have you ever experienced frustrating errors or typos while doing this? Would you like to save time by using one script for many similar tasks? If so then please read on for how to use <code>sbatch</code> jobs with the <code>--array</code> flag.</p> <p>The <code>--array</code> flag transforms an <code>sbatch</code> job script for a single task into a collection of tasks that are all scheduled simultaneously. For programmers, the <code>--array</code> flag turns a job script into a parallel loop, or a loop where each iteration is run independently and in no particular order. Naturally, this means that your tasks must be independent and similar. The most common use case is running the same software with different inputs or on different data sets. To get the most out of this page, you'll want to be familiar with Submitting Jobs.</p> <p>We will show how to create and use <code>sbatch</code> jobs with the <code>--array</code> flag, or <code>sbatch --array</code> jobs. We will use a simplified, practical example that parallels the process of a computational scientific experiment. The practical task we will solve is simplified to enhance focus on the structure of the problem, rather than the content of the problem. The structure of the problem is what makes <code>sbatch --array</code> jobs more or less suitable for a particular need. Specifically, whether there are many independent subtasks that all have the same structure, with similar or the same parameters.</p> <p>For other examples of using Slurm and its other tools, please see Submitting Jobs and Managing Jobs.</p>"},{"location":"cheaha/slurm/practical_sbatch/#the-task","title":"The Task","text":"<p>Your task is to determine the statistical properties of dice rolls. To measure these properties, you'll need to simulate rolling the dice many times to obtain a lot of data. Because dice rolls are independent, the task of simulating many dice rolls can be subdivided into many independent subtasks, all with the same parameters. While we could simulate dice rolls one at a time, in sequence, we could instead use an <code>sbatch --array</code> job to simulate dice rolls in parallel.</p> <p>Suppose you've already written some code called <code>simulate</code> that performs these simulations. The code transforms three integer inputs into an output sequence of positive integers. Based on the usual rules for tabletop role-playing game (TTRPG) dice rolling. The block below shows a sample input requesting ten rolls that are the sum of two six-sided die rolls plus one. In TTRPG notation we would write this as <code>2d6+1</code> rolled ten times. The output, in the case below, is a sequence of ten integers. Each element of the sequence falls in the possible range of the <code>2d6+1</code> die roll, which is <code>[3,13]</code>.</p> Inputs, Outputs, How to Call<pre><code># a tuplet of integers like the following...\n\n# name       | number of rolls | quantity | sides    | modifier\n# properties | positive        | positive | positive | any integer\n               10              , 2        , 6        , 1\n\n# is transformed to sequence of positive integers\n8,9,4,4,3,8,6,6,13,8\n\n# based on a call like\nsimulate $SEED $INPUT_FILE $OUTPUT_FILE\n</code></pre> <p>The input data takes the form of a simple text file (specifically a comma-separated value or CSV file) with four integers as described above. The upstream source of this data puts each simulation in a separate file in a separate folder. This may feel contrived for a simple example, but real experimental data is often structured with one treatment per folder, so we are using it in this example. We do not necessarily know in advance how many data folders will be present when we run the code. Sure, we could count manually and then hardcode that value, but we are trying to automate our process to avoid introducing errors and to save time in the future.</p> <p>All of the above constraints must fit within the framework provided by Slurm and the <code>sbatch --array</code> job style. Now that we have a complete list of requirements, we are ready to start forming a solution.</p>"},{"location":"cheaha/slurm/practical_sbatch/#building-a-solution","title":"Building a Solution","text":"<p>We are going to need three components to effectively use <code>sbatch array</code> jobs given the requirements and constraints of the task.</p> <ol> <li>The <code>simulate</code> code that transforms inputs to outputs. We are assuming this exists and will not discuss the implementation in detail here.</li> <li>A <code>job</code> shell script file that instructs Slurm how to allocate each array job task.</li> <li>A <code>main</code> shell script to automate the <code>--array</code> bounds and call the <code>job</code> script.</li> </ol>"},{"location":"cheaha/slurm/practical_sbatch/#job-script","title":"Job Script","text":"<p>The job shell script file will be very much like a typical <code>sbatch</code> job script. The preamble will contain the details of Slurm scheduler instructions in the form of flags. After the preamble comes the payload, where we instruct the shell what commands need to be run within each task.</p>"},{"location":"cheaha/slurm/practical_sbatch/#preamble","title":"Preamble","text":"<p>The preamble of an <code>sbatch</code> job script instructs Slurm how to queue the job and what resources to allocate. Our preamble is relatively straightforward and should look familiar if you've written job scripts before. For more detailed information on what the flags mean please see Slurm Flags.</p> job script preamble<pre><code>#! /bin/bash\n\n#SBATCH --job-name=simulate_dice_rolls\n#SBATCH --output=%x-%A-%a.log\n#SBATCH --error=%x-%A-%a.log\n\n#SBATCH --partition=express\n#SBATCH --time=00:02:00\n\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n</code></pre> <p>If we had a fixed number of datasets, say ten, we could add the line <code>#SBATCH --array=0-9</code> to the preamble and not require a <code>main.sh</code> file. In that case we would just use <code>sbatch job.sh</code> and be done. However, for this example, we require a dynamic upper bound on the <code>--array</code> flag, which isn't possible in the preamble of a script file.</p>"},{"location":"cheaha/slurm/practical_sbatch/#payload","title":"Payload","text":"<p>For our example, the payload consists of multiple parts. We will need to extract the file from just one of the folders, ensuring that each folder is used exactly once. Dependencies must be loaded, and then the software must be run. The most interesting part will be extracting the files.</p>"},{"location":"cheaha/slurm/practical_sbatch/#file-extraction","title":"File Extraction","text":"<p>The general idea for file extraction, and generally use of the <code>$SLURM_ARRAY_TASK_ID</code> variable, is for each task to pull one unique element out of a list of possibilities. The list of possibilities can take many forms. In our case, we can use a shell array of strings constructed from a glob pattern.</p> <p>Other workflows may extract lines from a manifest file, or even have a computation that transforms the job array index provided by <code>$SLURM_ARRAY_TASK_ID</code> into some other number or collection of numbers.</p> <p>Below is the file extraction portion of <code>job.sh</code>, along with descriptions of each line.</p> Job Script Payload: File Extraction<pre><code>shopt -s nullglob\n\ninput_files=(../inputs/**/dice.csv)\nINPUT_FILE=${input_files[$SLURM_ARRAY_TASK_ID]}\n\nOUTPUT_FILE=${INPUT_FILE/inputs/outputs}\nOUTPUT_FILE=${OUTPUT_FILE/dice/rolls}\n</code></pre> <ol> <li> <p>The line <code>shopt -s nullglob</code> instructs <code>bash</code> to allow empty arrays created with glob patterns.</p> </li> <li> <p>The line <code>input_files=(../inputs/**/dice.csv)</code> uses the glob pattern <code>../inputs/**/dice.csv</code> to create a shell array of paths to files in our dataset. Note that this shell array is distinct from the job <code>--array</code>, though they should have the same total number of elements, one for each task.</p> <p>The glob pattern can be read as:</p> <ul> <li><code>../</code>: move up one folder then...</li> <li><code>inputs/</code>: look in the folder <code>inputs</code> for...</li> <li><code>**/</code>: any number of nested subfolders containing...</li> <li><code>dice.csv</code>: the file <code>dice.csv</code>.</li> </ul> <p>The parentheses around the glob pattern instructs <code>bash</code> to create a shell array of strings from the results of the glob pattern. If there are multiple folders, each with one <code>dice.csv</code> file, then the shell array will have one entry for each of them.</p> <p>The shell array is then stored in the variable <code>input_files</code>.</p> </li> <li> <p>The line <code>INPUT_FILE=${input_files[$SLURM_ARRAY_TASK_ID]}</code> extracts exactly one entry from the <code>input_files</code> shell array and puts it in the variable <code>INPUT_FILE</code>. The value of <code>$SLURM_ARRAY_TASK_ID</code> is set by the Slurm schedule when each task starts. If there are ten tasks, as with <code>--array=0-9</code>, then each task has <code>$SLURM_ARRAY_TASK_ID</code> set to a unique value from <code>[0,9]</code>. We index the shell array <code>input_files</code> using <code>$SLURM_ARRAY_TASK_ID</code> to get a single entry from the shell array. Putting it all together, each task will pull out exactly one file from the set of data folders.</p> <p>Tip</p> <p>Curly braces with a leading dollar sign like <code>${...}</code> are used for evaluating some modification to a variable.</p> </li> <li> <p>The line <code>OUTPUT_FILE=${INPUT_FILE/inputs/outputs}</code> transforms the variable <code>INPUT_FILE</code> so that any instance of the word <code>inputs</code> is replaced with the word <code>outputs</code>. The result is assigned to the variable <code>OUTPUT_FILE</code>. The net result is the output file currently will be <code>../outputs/folder/dice.csv</code>.</p> </li> <li> <p>We're not quite done setting up the output path. The line <code>OUTPUT_FILE=${OUTPUT_FILE/dics/rolls}</code> further replaces <code>dice</code> with <code>rolls</code>. The net result is <code>../outputs/folder/rolls.csv</code>.</p> <p>Lines 4 and 5 result in a parallel folder structure between inputs and outputs. It is possible to use other structures such as all outputs in the same folder, or outputs in the same folders as inputs, but we won't go into detail here on how to achieve those.</p> </li> </ol>"},{"location":"cheaha/slurm/practical_sbatch/#running-the-software","title":"Running the Software","text":"<p>Running the software requires setting up the random number generator seed (for repeatability), loading module dependencies, creating the output directory, and finally running the simulation.</p> Job Script Payload: Running the Software<pre><code>SEED=314159\n\nmodule load ...\n\nOUTPUT_DIRECTORY=$(dirname \"$OUTPUT_FILE\")\nmkdir -p $OUTPUT_DIRECTORY\n\nsimulate $SEED $INPUT_FILE $OUTPUT_FILE\n</code></pre> <ol> <li> <p>The line <code>SEED=314159</code> sets the variable <code>SEED</code> with a static number for repeatability. It is set in a variable, rather than directly on the final line, so future readers of the code will understand the purpose of the number.</p> </li> <li> <p>The line <code>module load ...</code> is where any necessary <code>module load</code> or <code>conda activate</code> executions would go to prepare dependencies.</p> </li> <li> <p>The line <code>OUTPUT_DIRECTORY=$(dirname \"$OUTPUT_FILE\")</code> extracts the directory part of the output file path and assigns it to the variable <code>OUTPUT_DIRECTORY</code>.</p> <p>Tip</p> <p>Single parentheses with a leading dollar sign like <code>$(...)</code> are used for capturing the string output of a command in a variable.</p> </li> <li> <p>The line <code>mkdir -p $OUTPUT_DIRECTORY</code> creates the directory at the path stored in the variable <code>$OUTPUT_DIRECTORY</code>. The flag <code>-p</code> means the <code>mkdir</code> command will not produce an error if the directory already exists.</p> <p>Tip</p> <p>If you know your software will create any necessary output directories, then this and the previous line are not necessary.</p> </li> <li> <p>The line <code>simulate $SEED $INPUT_FILE $OUTPUT_FILE</code> runs the simulation.</p> </li> </ol>"},{"location":"cheaha/slurm/practical_sbatch/#main-script","title":"Main Script","text":"<p>The <code>main</code> shell script will determine the upper bound <code>$N</code> of the <code>--array</code> flag, and then call <code>sbatch --array=1-$N job.sh</code>. It will be up to <code>job.sh</code> to determine how to use <code>$SLURM_ARRAY_TASK_ID</code>. Before we go too much further, it may be helpful to think of <code>sbatch --array=1-$N job.sh</code> as creating an indexed loop, from 1 to <code>$N</code>, and running <code>job.sh</code> on each of those index values. The important point is that the loop indices are run in parallel, so whatever happens in each call to <code>job.sh</code> must be independent. The <code>main.sh</code> file is the same for all languages and is shown in the code block below. The comments describe what each segment of code is doing.</p> main.sh<pre><code>#! /bin/bash\n\nshopt -s nullglob\n\ninput_files=(../inputs/**/dice.csv)\n\nFILE_COUNT=${#input_files[@]}\nFILE_COUNT=$(( $FILE_COUNT - 1 ))\n\nsbatch --array=0-$FILE_COUNT job.sh\n</code></pre> <ol> <li> <p>The line <code>#! /bin/bash</code> instructs the operating system what interpreter to use if called without an explicit interpreter, like <code>./main.sh</code>. It is best practice to have this line for scripts running in <code>bash</code>. Other lines are possible for other interpreters.</p> </li> <li> <p>The line <code>shopt -s nullglob</code> instructs <code>bash</code> to allow empty arrays created with glob patterns.</p> </li> <li> <p>The line <code>input_files=(../inputs/**/dice.csv)</code> uses the glob pattern <code>../inputs/**/dice.csv</code> to create a shell array of paths to files in our dataset. The details of this are discussed above in File Extraction.</p> </li> <li> <p>The line <code>FILE_COUNT=${#input_files[@]}</code> gets all entries from the <code>input_files</code> array using the special index <code>@</code> (for all elements), then counting them with the prefix symbol <code>#</code>.</p> <p>Tip</p> <p>Curly braces with a leading dollar sign like <code>${...}</code> are used for evaluating string modifications to a variable.</p> </li> <li> <p>The line <code>FILE_COUNT=$(( FILE_COUNT - 1 ))</code> subtracts one from the file count. We must do this because array variables, as used in <code>job.sh</code>, start counting at zero (they are zero-indexed). So instead of counting from <code>1</code> to <code>10</code>, we would count from <code>0</code> to <code>9</code>.</p> <p>Tip</p> <p>Double parentheses with a leading dollar sign like <code>$((...))</code> are used for evaluating integer arithmetic to a variable.</p> </li> <li> <p>The line <code>sbatch --array=0-$FILE_COUNT job.sh</code> puts the array tasks in the Slurm queue using the <code>job.sh</code> script. The number of tasks runs from <code>0</code> to <code>$FILE_COUNT</code> as compute above.</p> </li> </ol> <p>To use the script, enter the command <code>bash main.sh</code> at the terminal.</p> <p>Note</p> <p>When writing <code>sbatch</code> scripts for job submission and managing modules, begin your script by resetting the module environment with <code>module reset</code> to ensure a clean environment for subsequent configurations. See best practice for loading modules for more information.</p>"},{"location":"cheaha/slurm/practical_sbatch/#putting-it-all-together","title":"Putting it All Together","text":"<p>We needed three parts to make the <code>sbatch --array</code> job work for our task. Each of these parts has been described above in some detail.</p> <ol> <li><code>simulate</code> program to run a simulation.</li> <li><code>job.sh</code> to instruct the Slurm scheduler what to do in each parallel task.</li> <li><code>main.sh</code> to run everything.</li> </ol> <p>Executing <code>bash main.sh</code> at the terminal will first compute the number of array tasks, then call <code>sbatch --array</code> with that number of tasks on <code>job.sh</code>. The scheduler will then schedule that many jobs to be run. Each job will have a unique task ID, which will be used to access unique input files and write to unique output files. All of them will be run in parallel.</p> <p>The reason <code>sbatch --array</code> could be used on our dice rolling statistics task is because dice rolls are independent. It doesn't matter when I roll the dice or whether I roll them together or sequentially, the results will be statistically the same.</p> <p>Any task that can be broken into independent subtasks with similar input parameters can be used with <code>sbatch --array</code> in this way. Please feel free to use the scripts provided as a template for your own <code>sbatch --array</code> jobs, modifying them as appropriate.</p>"},{"location":"cheaha/slurm/practical_sbatch/#the-example-scripts","title":"The Example Scripts","text":"<p>For reference, here are the full scripts.</p> job.sh<pre><code>#! /bin/bash\n\n#SBATCH --job-name=simulate_dice_rolls\n#SBATCH --output=%x-%A-%a.log\n#SBATCH --error=%x-%A-%a.log\n\n#SBATCH --partition=express\n#SBATCH --time=00:02:00\n\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n\nshopt -s nullglob\n\ninput_files=(../inputs/**/dice.csv)\nINPUT_FILE=${input_files[$SLURM_ARRAY_TASK_ID]}\n\nOUTPUT_FILE=${INPUT_FILE/inputs/outputs}\nOUTPUT_FILE=${OUTPUT_FILE/dice/rolls}\n\nSEED=314159\n\nmodule load ...\n\nOUTPUT_DIRECTORY=$(dirname \"$OUTPUT_FILE\")\nmkdir -p $OUTPUT_DIRECTORY\n\nsimulate $SEED $INPUT_FILE $OUTPUT_FILE\n</code></pre> main.sh<pre><code>#! /bin/bash\n\nshopt -s nullglob\n\ninput_files=(../inputs/**/dice.csv)\n\nFILE_COUNT=${#input_files[@]}\nFILE_COUNT=$(( $FILE_COUNT - 1 ))\n\nsbatch --array=0-$FILE_COUNT job.sh\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/","title":"Writing Slurm Batch Jobs","text":"<p>This Slurm tutorial serves as a hands-on guide for users to create Slurm batch scripts based on their specific software needs and apply them for their respective usecases.  It covers basic examples for beginners and advanced ones, including sequential and parallel jobs, array jobs, multithreaded jobs, GPU utilization jobs, and MPI (Message Passing Interface) jobs. To know which type of batch jobs are suitable for your pipeline/usecase, please refer to the User Guide section.</p>"},{"location":"cheaha/slurm/slurm_tutorial/#structure-of-a-slurm-batch-job","title":"Structure of a Slurm Batch Job","text":"<p>Below is the template for a typical Slurm job submission in the Cheaha high-performance computing (HPC) system. The script begins with <code>#!/bin/bash</code>, indicating it is a bash script. The next step would be to declare Slurm configuration options, specifying the required resources for job execution. This section typically comprises parameters such as CPU count, partition, memory allocation, time limit, etc. Following the configuration, the script may include sections for loading necessary software or libraries required for the job.</p> <pre><code>#!/bin/bash\n# Declaring Slurm configuration options and specifying required resources\n...\n# Loading Software/Libraries\n...\n# Running Code\n...\n</code></pre> <p>The last portion is running the actual code or software. Here, the computational task or program intended for execution is launched using specific commands and processes, which depends on the software used and overall computational workflow. For more detailed specification, refer to Slurm job submission. The following sections present practical examples for writing a Slurm batch script to specific use cases, and prerequisites to start with the tutorial.</p>"},{"location":"cheaha/slurm/slurm_tutorial/#prerequisites","title":"Prerequisites","text":"<p>If you're new to using Unix/Linux commands and bash scripting, we suggest going through the software carpentry lesson, The Unix Shell. Also, we recommend reviewing the Cheaha Hardware Information to help guide you in choosing appropriate partition and resources.</p>"},{"location":"cheaha/slurm/slurm_tutorial/#slurm-batch-job-user-guide","title":"Slurm Batch Job User Guide","text":"<p>This user guide provides comprehensive insight into different types of batch jobs, facilitating in identifying the most suitable job type for your specific tasks. With clear explanations and practical examples, you will gain a deeper understanding of sequential, parallel, array, multicore, GPU, and multi-node jobs, assisting to make informed decisions when submitting jobs on the Cheaha system.</p> <ol> <li> <p>A Simple Slurm Batch Job is ideal for Cheaha users who are just starting with Slurm batch job submission. It uses a simple example to introduce new users to requesting resources with <code>sbatch</code>, printing the <code>hostname</code>, and monitoring batch job submission.</p> </li> <li> <p>Sequential Job is used when tasks run one at a time sequentially. Adding more CPUs does not make a sequential job run faster. If you need to run many such sequential jobs simultaneously, you can submit it as an single array job. For instance, a Python or R script that executes a series of steps\u2014such as data loading, extraction, analysis, and output reporting\u2014where each step must be completed before the next can begin.</p> </li> <li> <p>Parallel Jobs is suitable for executing multiple independent tasks/jobs simultaneously and efficiently distributing them across resources. This approach is particularly beneficial for small-scale tasks that cannot be split into parallel processes within the code itself. For example, consider a Python script that operates on different data set, in such a scenario, you can utilize <code>srun</code> to execute multiple instances of the script concurrently, each operating on a different dataset and on different resources.</p> </li> <li> <p>Array Job is used for submitting and running multiple large number of identical tasks in parallel. They share the same code and execute with similar resource requirements. Instead of submitting multiple sequential job, you can submit a single array job, which helps to manage and schedule a large number of similar tasks efficiently. This improves efficiency, resource utilization, scalability, and ease of debugging. For instance, array jobs can be designed for executing multiple instances of the same task with slight variations in inputs or parameters such as perform FastQC processing on 10 different samples.</p> </li> <li> <p>Mutlithreaded or Multicore Job is used when software inherently support multithreaded parallelism i.e  run independent tasks simultaneously on multicore processors. For instance, there are numerous software such as MATLAB, FEBio, Xplor-NIH support running multiple tasks at the same time on multicore processors. Users or programmers do not need to modify the code; you can simply enable multithreaded parallelism by configuring the appropriate options.</p> </li> <li> <p>GPU Job utilizes the parallel GPUs, which contain numerous cores designed to perform the same mathematical operations simultaneously. GPU job is appropriate for pipelines and software that are designed to run on GPU-based systems and efficiently distribute tasks across cores to process large datasets in parallel. Example includes Tensorflow, Parabricks, PyTorch, etc.</p> </li> <li> <p>Multinode Job is for pipeline/software that can be distributed and run across multiple nodes. For example, MPI based applications/tools such as Quantum Expresso, Amber, LAMMPS, etc.</p> </li> </ol>"},{"location":"cheaha/slurm/slurm_tutorial/#example-1-a-simple-slurm-batch-job","title":"Example 1: A Simple Slurm Batch Job","text":"<p>Let us start with a simple example to print <code>hostname</code> of the node where your job is submitted. You will have to request for the required resources to run your job using Slurm parameters (lines 5-10). To learn more about individual Slurm parameters given in the example, please refer to Slurm flag and environment variables and the official Slurm documentation. o To test this example, copy the below script in a file named <code>hostname.job</code>. This job executes the <code>hostname</code> command (line 15) on a single node, using one task, one CPU core, 1 gigabyte of memory, with a time limit of 10 minutes. The output and error logs are directed to separate files with names based on their job name and ID (line 11 and 12). For a more detailed understanding of the individual parameters used in this script, please refer to the section on Simple Batch Job. The following script includes comments, marked with <code>###</code>, describing their functions. We will utilize this notation for annotating comments in subsequent examples.</p> <pre><code>#!/bin/bash\n\n### Declaring Slurm configuration options and specifying required resources\n#SBATCH --job-name=hostname     ### Name of the job\n#SBATCH --nodes=1               ### Number of Nodes\n#SBATCH --ntasks=1              ### Number of Tasks\n#SBATCH --cpus-per-task=1       ### Number of Tasks per CPU\n#SBATCH --mem=1G                ### Memory required, 1 gigabyte\n#SBATCH --partition=express     ### Cheaha Partition\n#SBATCH --time=00:10:00         ### Estimated Time of Completion, 10 minutes\n#SBATCH --output=%x_%j.out      ### Slurm Output file, %x is job name, %j is job id\n#SBATCH --error=%x_%j.err       ### Slurm Error file, %x is job name, %j is job id\n\n### Running the command `hostname`\nhostname\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#submitting-and-monitoring-the-job","title":"Submitting and Monitoring the Job","text":"<p>Now submit the script <code>hostname.job</code> for execution on Cheaha cluster using <code>sbatch hostname.job</code>. Slurm processes the job script and schedules the job for execution on the cluster. The output you see, \"Submitted batch job 26035322,\" indicates that the job submission was successful, and Slurm has assigned a unique job ID <code>26035322</code>.</p> <pre><code>$sbatch hostname.job\n\nSubmitted batch job 26035322\n</code></pre> <p>After submitting the job, Slurm will create the output and error files with job name <code>hostname</code> and id <code>26035322</code>  as,</p> <pre><code>$ ls\n\nhostname_26035322.err  hostname_26035322.out  hostname.sh\n</code></pre> <p>The submitted job will be added to the Slurm queue and will wait for available resources based on the specified job configuration and the current state of the cluster. You can use <code>squeue -j job_id</code> to monitor the status of your job.</p> <pre><code>$squeue -j 26035322\n\nJOBID      PARTITION    NAME        USER    ST       TIME  NODES NODELIST(REASON)\n26035322   express      hostname    USER    CG       0:01      1 c0156\n</code></pre> <p>The above output provides a snapshot of the job's status, resource usage,  indicating that it is currently running on one node (c0156). The term <code>CG</code> refers to completing its execution. For more details refer to Managing Slurm jobs. If the job is successful, the <code>hostname_26035322.err</code> file will be empty/without error statement. You can print the result using,</p> <pre><code>$ cat hostname_26035322.out\nc0156\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#example-2-sequential-job","title":"Example 2: Sequential Job","text":"<p>This example illustrate a Slurm job that runs a Python script involving NumPy operation. This python script is executed sequentially using the same resource configuration as Example 1. Let us name the below script as <code>numpy.job</code>.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=numpy            ### Name of the job\n#SBATCH --nodes=1                   ### Number of Nodes\n#SBATCH --ntasks=1                  ### Number of Tasks\n#SBATCH --cpus-per-task=1           ### Number of Tasks per CPU\n#SBATCH --mem=4G                    ### Memory required, 4 gigabyte\n#SBATCH --partition=express         ### Cheaha Partition\n#SBATCH --time=01:00:00             ### Estimated Time of Completion, 1 hour\n#SBATCH --output=%x_%j.out          ### Slurm Output file, %x is job name, %j is job id\n#SBATCH --error=%x_%j.err           ### Slurm Error file, %x is job name, %j is job id\n\n### Loading Anaconda3 module to activate `pytools-env` conda environment\nmodule load Anaconda3\nconda activate pytools-env\n\n### Run the script `python_test.py`\npython python_test.py\n</code></pre> <p>The batch job requires an input file <code>python_test.py</code> (line 17) for execution. Copy the input file from the Containers page. Place this file in the same folder as the <code>numpy.job</code>. This python script performs numerical integration and data visualization tasks, and it relies on the following packages: numpy, matplotlib, scipy for successful execution. These dependencies can be installed using Anaconda within a <code>conda</code> environment named <code>pytools-env</code>. Prior to running the script, load the <code>Anaconda3</code> module and activate the <code>pytools-env</code> environment (line 13 and 14).  Once job is successfully completed, check the slurm output file for results. Additionally, a plot named <code>testing.png</code> will be generated.</p> <pre><code>$ ls\n\nnumpy_26127143.err  numpy_26127143.out  numpy.job  python_test.py  testing.png\n</code></pre> <pre><code>$cat numpy_26127143.out\n\n[ 0 10 20 30 40]\n[-5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5  1.   1.5\n  1.   2.5  3.   3.5  4.   4.5]\n[ 0.   0.5  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5  6.   6.5\n  1.   7.5  8.   8.5  9.   9.5 10.  10.5 11.  11.5 12.  12.5 13.  13.5\n 1.   14.5 15.  15.5 16.  16.5 17.  17.5 18.  18.5 19.  19.5 20. ]\n(2.0, 2.220446049250313e-14)\n</code></pre> <p>You can review detailed information about finished jobs using <code>sacct</code> command for a specific job id as shown below. For instance, this job was allocated with one CPU and has been successfully completed. The lines with \".ba+\" and \".ex+\" refer to batch step and external step within a job, but we will ignore them for simplicity in this and future examples. The exit code <code>0:0</code> signifies a normal exit with no errors.</p> <pre><code>$ sacct -j 26127143\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n26127143          numpy    express      USER          1  COMPLETED      0:0\n26127143.ba+      batch                 USER          1  COMPLETED      0:0\n26127143.ex+     extern                 USER          1  COMPLETED      0:0\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#example-3-parallel-jobs","title":"Example 3: Parallel Jobs","text":"<p>Multiple jobs or tasks can be executed simultaneously using <code>srun</code> within a single batch script. In this example, the same executable <code>python_script_new.py</code> is run in parallel with distinct inputs (line 17-19). The <code>&amp;</code> symbol at the end of each line run these commands in background. The <code>wait</code> command (line 20) performs synchronization and ensures that all background processes and parallel tasks are completed before finishing. In Line 4, three tasks are requested as there are three executables to be run in parallel. The overall job script is allocated with three CPUs, and in lines(17-19), each <code>srun</code> script utilizes 1 CPU to perform their respective task. Copy the batch script into a file named <code>multijob.job</code>. Use the same <code>conda</code> environment <code>pytools-env</code> shown in example2.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=multijob             ### Name of the job\n#SBATCH --nodes=1                       ### Number of Nodes\n#SBATCH --ntasks=3                      ### Number of Tasks\n#SBATCH --cpus-per-task=1               ### Number of Tasks per CPU\n#SBATCH --mem=4G                        ### Memory required, 4 gigabyte\n#SBATCH --partition=express             ### Cheaha Partition\n#SBATCH --time=01:00:00                 ### Estimated Time of Completion, 1 hour\n#SBATCH --output=%x_%j.out              ### Slurm Output file, %x is job name, %j is job id\n#SBATCH --error=%x_%j.err               ### Slurm Error file, %x is job name, %j is job id\n\n### Loading Anaconda3 module to activate `pytools-env` conda environment\nmodule load Anaconda3\nconda activate pytools-env\n\n### Runs the script `python_test.py` in parallel with distinct inputs and ensures synchronization\nsrun --nodes=1 --ntasks=1 python python_script_new.py 1 100000 &amp;\nsrun --nodes=1 --ntasks=1 python python_script_new.py 100001 200000 &amp;\nsrun --nodes=1 --ntasks=1 python python_script_new.py 200001 300000 &amp;\nwait\n</code></pre> <p>Copy the following python script and call it as <code>python_script_new.py</code>. The input file takes two command-line arguments i.e. the <code>start</code> and <code>end</code> values. The script uses these values to creates an array and compute the sum of its elements using numpy. The above batch script runs three parallel instances of this Python script with different inputs.</p> <pre><code>import sys\nimport numpy as np\n\n### Verify if correct number of command-line arguments is provided\nif len(sys.argv) != 3:\n  print(\"Usage: python python_script_new.py &lt;start&gt; &lt;end&gt;\")\n  sys.exit(1)\n\n### Passing start and end values from command-line arguments\nstart = int(sys.argv[1])\nend = int(sys.argv[2])\n\n### Create an array from start to end using numpy\ninput_array = np.arange(start, end)\n\n### Perform addition on the array elements using numpy's sum function\nsum_result = np.sum(input_array)\n\n### Print Input Range and Sum\nprint(\"Input Range: {} to {}, Sum: {}\".format(start, end, sum_result))\n</code></pre> <p>The below output shows that each line corresponds to the output of one parallel execution of python script with specific input ranges. Note that the results are in out of order. This is because each <code>srun</code> script runs independently, and their completion times may vary based on factors such as system load, resource availability, and the nature of their computations. If the results must be in order to be correct, you will need to modify your script to explicitly collect and organize them. One possible approach can be found in the section srun for running parallel jobs (refer to example 2).</p> <pre><code>$cat multijob_27099591.out\n\nInput Range: 1 to 100000, Sum: 4999950000\nInput Range: 200001 to 300000, Sum: 24999750000\nInput Range: 100001 to 200000, Sum: 14999850000\n</code></pre> <p>The <code>sacct</code> report indicates that three CPUs have been allocated. The python script executes with unique task IDs 27099591.0,27099591.1,27099591.2.</p> <pre><code>$ sacct -j 27099591\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n27099591       multijob    express      USER          3  COMPLETED      0:0\n27099591.ba+      batch                 USER          3  COMPLETED      0:0\n27099591.ex+     extern                 USER          3  COMPLETED      0:0\n27099591.0       python                 USER          1  COMPLETED      0:0\n27099591.1       python                 USER          1  COMPLETED      0:0\n27099591.2       python                 USER          1  COMPLETED      0:0\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#example-4-array-job","title":"Example 4: Array Job","text":"<p>Array jobs are more effective when you have a larger number of similar tasks to be executed simultaneously with varied input data, unlike <code>srun</code> parallel jobs which are suitable for running a smaller number of tasks concurrently (e.g. less than 5). Array jobs are easier to manage and monitor multiple tasks through unique identifiers.</p> <p>The following Slurm script is an example of how you might convert the previous <code>multijob</code> script to an array job. To start, copy the below script to a file named, <code>slurm_array.job</code>. The script requires the input file <code>python_script_new.py</code> and the <code>conda</code> environment <code>pytools-env</code>, similar to those used in example2 and example 3. Line 11 specifies the script as an array job, treating each task within the array as an independent job. For each task, lines 18-19 calculates the input range. <code>SLURM_ARRAY_TASK_ID</code> identifies the task executed using indexes, and is automatically set for array jobs. The python script (line 22) runs individual array task concurrently on respective input range. The command <code>awk</code> is used to prepend each output line with the unique task identifier and then append the results to the file, <code>output_all_tasks.txt</code>. For more details on on parameters of array jobs, please refer to Batch Array Jobs and Practical Batch Array Jobs.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=slurm_array       ### Name of the job\n#SBATCH --nodes=1                    ### Number of Nodes\n#SBATCH --ntasks=1                   ### Number of Tasks\n#SBATCH --cpus-per-task=1            ### Number of Tasks per CPU\n#SBATCH --mem=4G                     ### Memory required, 4 gigabyte\n#SBATCH --partition=express          ### Cheaha Partition\n#SBATCH --time=01:00:00              ### Estimated Time of Completion, 1 hour\n#SBATCH --output=%x_%A_%a.out        ### Slurm Output file, %x is job name, %A is array job id, %a is array job index\n#SBATCH --error=%x_%A_%a.err         ### Slurm Error file, %x is job name, %A is array job id, %a is array job index\n#SBATCH --array=1-3                  ### Number of Slurm array tasks, 3 tasks\n\n### Loading Anaconda3 module to activate `pytools-env` conda environment\nmodule load Anaconda3\nconda activate pytools-env\n\n### Calculate the input range for each task\nstart=$((($SLURM_ARRAY_TASK_ID - 1) * 100000 + 1))\nend=$(($SLURM_ARRAY_TASK_ID * 100000))\n\n### Run the python script with input arguments and append the results to a .txt file for each task\npython python_script_new.py $start $end 2&gt;&amp;1 | awk -v task_id=$SLURM_ARRAY_TASK_ID '{print \"array task \" task_id, $0}' &gt;&gt; output_all_tasks.txt\n</code></pre> <p>The output shows the sum of different input range computed by individual task, making it easy to track using a task identifier, such as array task 1/2/3.</p> <pre><code>$ cat output_all_tasks.txt\n\narray task 2 Input Range: 100001 to 200000, Sum: 14999850000\narray task 3 Input Range: 200001 to 300000, Sum: 24999750000\narray task 1 Input Range: 1 to 100000, Sum: 4999950000\n</code></pre> <p>The <code>sacct</code> report indicates that the job <code>27101430</code> consists of three individual tasks, namely <code>27101430_1</code>, <code>27101430_2</code>, and <code>27101430_3</code>. Each task has been allocated one CPU resource.</p> <pre><code>$ sacct -j 27101430\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n27101430_3   slurm_arr+    express      USER          1  COMPLETED      0:0\n27101430_3.+      batch                 USER          1  COMPLETED      0:0\n27101430_3.+     extern                 USER          1  COMPLETED      0:0\n27101430_1   slurm_arr+    express      USER          1  COMPLETED      0:0\n27101430_1.+      batch                 USER          1  COMPLETED      0:0\n27101430_1.+     extern                 USER          1  COMPLETED      0:0\n27101430_2   slurm_arr+    express      USER          1  COMPLETED      0:0\n27101430_2.+      batch                 USER          1  COMPLETED      0:0\n27101430_2.+     extern                 USER          1  COMPLETED      0:0\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#example-5-multithreaded-or-multicore-job","title":"Example 5: Multithreaded or Multicore Job","text":"<p>This Slurm script illustrates execution of a MATLAB script in a multithread/multicore environemnt. Save the script as <code>multithread.job</code>. The <code>%</code> symbol in this script denotes comments within MATLAB code. Line 16 runs the MATLAB script <code>parfor_sum_array</code>, with an input array size <code>100</code> passed as argument, using 4 CPU cores (as specified in Line 5).</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=multithread          ### Name of the job\n#SBATCH --nodes=1                       ### Number of Nodes\n#SBATCH --ntasks=1                      ### Number of Tasks\n#SBATCH --cpus-per-task=4               ### Number of Tasks per CPU\n#SBATCH --mem=16G                       ### Memory required, 16 gigabyte\n#SBATCH --partition=express             ### Cheaha Partition\n#SBATCH --time=01:00:00                 ### Estimated Time of Completion, 1 hour\n#SBATCH --output=%x_%j.out              ### Slurm Output file, %x is job name, %j is job id\n#SBATCH --error=%x_%j.err               ### Slurm Error file, %x is job name, %j is job id\n\n### Loading required MATLAB module\nmodule load rc/matlab/R2023b\n\n### Executing the matlab script with input arguments\nmatlab -nosplash -nodesktop -r \"parfor_sum_array(100); quit;\"\n</code></pre> <p>Copy the below MATLAB script as <code>parfor_sum_array.m</code>. At the beginning, the script defines a function <code>sum_array</code> and variable <code>array_size</code> is passed as an input argument. This function uses multithreading with the <code>parfor</code> option to calculate the sum of elements in an array. On Line 10, the number of workers (<code>num_workers</code>) is set to the value of the environment variable <code>SLURM_CPUS_PER_TASK</code> i.e. 4. The script then creates a parallel pool using lines 13-17, utilizing the specified number of workers. The parallel computation of summing up of array elements is performed using a <code>parfor</code> loop in lines 23-27. By using <code>parfor</code> with a pool of workers, operations are run in parallel for improved performance. More insights on usage of <code>parfor</code> can be found in the official MATLAB page.</p> <p>Important</p> <p>Make sure that the <code>SLURM_CPUS_PER_TASK &gt; 1</code> in order to take advantage of multithreaded performance. It is important that the  <code>SLURM_CPUS_PER_TASK</code> does not exceed the number of workers and physical cores (i.e. CPU cores) available on the node. This is to prevent high context switching, where individual CPUs are constantly switching between multiple running processes, which can negatively impact job performance of all jobs running on the node. It may also lead to overhead during job execution and result in poorer performance. Please refer to our Hardware page to learn more about resource limits and selecting appropriate resources.</p> <p>Bug</p> <p>There is a known issue with <code>parpool</code> and other related multi-core parallel features such as <code>parfor</code> affecting R2022a and earlier. See our Modules Known Issues section for more information.</p> <pre><code>% Function to calculate the sum of an array in parallel. This function takes array_size as input from command-line arguments\nfunction sum_array(array_size)\n    % Check if input array size is provided\n    if nargin &lt; 1\n        disp('error: pass input array size as arguments');\n        return;\n    end\n\n    % The number of workers is set based on Slurm parameter i.e. number of CPUS per task\n    num_workers = str2double(getenv('SLURM_CPUS_PER_TASK'));\n\n    % Create parallel pool\n    poolobj = gcp('nocreate');\n    if isempty(poolobj) || poolobj.NumWorkers ~= num_workers\n        delete(poolobj);\n        parpool(num_workers);\n    end\n\n    % Initialization of Array\n    A = zeros(1, array_size);\n\n    % Perform parallel computation for each element of the array\n    sum_result = 0;\n    parfor i = 1:array_size\n        A(i) = i;\n        sum_result = sum_result + A(i);\n    end\n\n    % Display the total sum\n    disp(['Sum of array is: ' num2str(sum_result)]);\nend\n</code></pre> <p>The below result summarizes the parallel pool initialization and its utilization of 4 workers for  computation of sum of an array. Followed by, the <code>sacct</code> report illustrates that the multithreaded job was allocated with 4 CPUs and was successfully completed.</p> <pre><code>$ cat multithread_27105035.out\n\nMATLAB is selecting SOFTWARE OPENGL rendering.\n\n                            &lt; M A T L A B (R) &gt;\n                  Copyright 1984-2023 The MathWorks, Inc.\n             R2023b Update 6 (23.2.0.2485118) 64-bit (glnxa64)\n                             December 28, 2023\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\nStarting parallel pool (parpool) using the 'Processes' profile ...\nConnected to parallel pool with 4 workers.\nSum of array is: 5050\nParallel pool using the 'Processes' profile is shutting down.\n</code></pre> <pre><code>$ sacct -j 27105035\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n27105035     multithre+    express      USER          4  COMPLETED      0:0\n27105035.ba+      batch                 USER          4  COMPLETED      0:0\n27105035.ex+     extern                 USER          4  COMPLETED      0:0\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#example-6-gpu-job","title":"Example 6: GPU Job","text":"<p>This slurm script shows the execution of Tensorflow job using GPU resources. Let us save this script as <code>gpu.job</code>. The Slurm parameter <code>--gres=gpu:2</code> in line 6, requests for 2 GPUs. In line 8, note that in order to run GPU-based jobs, either the <code>amperenodes</code> or <code>pascalnodes</code> partition must be used (please refer to our GPU page for more information). Lines 14-15 loads the necessary CUDA modules, while lines 18-19 load the Anaconda module and activate a <code>conda</code> environment called <code>tensorflow</code>. Refer to Tensorflow official page for installation. The last line executes a python script that utilizes Tensorflow library to perform matrix multiplication across multiple GPUs.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu              ### Name of the job\n#SBATCH --nodes=1                   ### Number of Nodes\n#SBATCH --ntasks=1                  ### Number of Tasks\n#SBATCH --cpus-per-task=1           ### Number of Tasks per CPU\n#SBATCH --gres=gpu:2                ### Number of GPUs, 2 GPUs\n#SBATCH --mem=16G                   ### Memory required, 16 gigabyte\n#SBATCH --partition=amperenodes     ### Cheaha Partition\n#SBATCH --time=01:00:00             ### Estimated Time of Completion, 1 hour\n#SBATCH --output=%x_%j.out          ### Slurm Output file, %x is job name, %j is job id\n#SBATCH --error=%x_%j.err           ### Slurm Error file, %x is job name, %j is job id\n\n### Loading the required CUDA and cuDNN modules\nmodule load CUDA/12.2.0\nmodule load cuDNN/8.9.2.26-CUDA-12.2.0\n\n### Loading the Anaconda module and activating the `tensorflow` environment\nmodule load Anaconda3\nconda activate tensorflow\n\n### Executing the python script\npython matmul_tensorflow.py\n</code></pre> <p>Let us now create a file named <code>matmul_tensorflow.py</code> and copy the following script into it. This python script demonstrates the utilization of Tensorflow library to distribute computational tasks among multiple GPUs, in order to perform matrix multiplication in parallel (Lines 11-19). Lines 8-9 retrieve the logical GPUs and enable device placement logging, which helps to analyze which device is used for each operation. The final results are aggregated and the sum is computed on the CPU device (lines 22-23).</p> <pre><code>import tensorflow as tf\n\n### Print Tensorflow version and check for available number of GPUs\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\n### Get the logical GPUs and enable device placement\ngpus = tf.config.list_logical_devices('GPU')\ntf.debugging.set_log_device_placement(True)\n\nif gpus:\n    ### Create tensors on each GPU and Perform matrix multiplication on multiple GPUs\n    c = []\n    for gpu in gpus:\n        with tf.device(gpu.name):\n            a = tf.random.uniform(shape=(4, 3))\n            b = tf.random.uniform(shape=(3, 4))\n            c.append(tf.matmul(a, b))\n            print(f\"Computation on GPU: {gpu.name}\")\n\n    ### Calculate the Sum on CPU device\n    with tf.device('/CPU:0'):\n        matmul_sum = tf.add_n(c)\n\n    ### Print the result\n    print(matmul_sum)\n</code></pre> <p>The results indicate that the Tensorflow version utilized is 2.15. The segments <code>/device:GPU:0</code> and <code>/device:GPU:1</code> specify that the computations were executed on two GPUs. The final results is a 4x4 matrix obtained by summing the matrix multiplication results. In the <code>sacct</code> report, the column <code>AllocGRES</code> shows that 2 GPUs are allocated for this job.</p> <pre><code>$ cat gpu_27107694.out\n\nTensorFlow version: 2.15.0\nNum GPUs Available:  2\nComputation on GPU: /device:GPU:0\nComputation on GPU: /device:GPU:1\ntf.Tensor(\n[[1.6408134 0.9900811 1.3046092 0.9307438]\n [1.5603762 1.6812123 1.8867838 1.0662912]\n [2.481688  1.8107605 2.0444224 1.5500932]\n [2.415476  1.9280369 2.020216  1.4872619]], shape=(4, 4), dtype=float32)\n</code></pre> <pre><code>$ sacct -j 27107694 --format=JobID,JobName,Partition,Account,AllocCPUS,allocgres,State,ExitCode\n\n       JobID    JobName  Partition    Account  AllocCPUS    AllocGRES      State ExitCode\n------------ ---------- ---------- ---------- ---------- ------------ ---------- --------\n27107694            gpu amperenod+      USER          1        gpu:2  COMPLETED      0:0\n27107694.ba+      batch                 USER          1        gpu:2  COMPLETED      0:0\n27107694.ex+     extern                 USER          1        gpu:2  COMPLETED      0:0\n</code></pre>"},{"location":"cheaha/slurm/slurm_tutorial/#example-7-multinode-job","title":"Example 7: Multinode Job","text":"<p>The below Slurm script runs a Quantum Expresso job using the <code>pw.x</code> executable on multiple nodes. In this example, we request for 2 nodes on <code>amd-hdr100</code> partition in lines 4 and 7. The suitable Quantum Expresso module is loaded in line 13. The last line is configured for a parallel computation of Quantum Expresso simulation across 2 nodes <code>N 2</code> and 4 MPI processes <code>-nk 4</code> for the input parameters in <code>pw.scf.silicon.in</code>. The input file <code>pw.scf.silicon.in</code> and psuedo potential file is taken from the github page. However this input is subject to change, hence according to your use case you can change the inputs.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=mpijob               ### Name of the job\n#SBATCH --nodes=2                       ### Number of Nodes\n#SBATCH --ntasks 4                      ### Number of Tasks\n#SBATCH --mem=64G                       ### Memory required, 64 gigabyte\n#SBATCH --partition=amd-hdr100          ### Cheaha Partition\n#SBATCH --time=12:00:00                 ### Estimated Time of Completion, 12 hour\n#SBATCH --output=%x_%j.out              ### Slurm Output file, %x is job name, %j is job id\n#SBATCH --error=%x_%j.err               ### Slurm Error file, %x is job name, %j is job id\n\n### Load the suitable Quantum Expresso module\nmodule load QuantumESPRESSO/6.3-foss-2018b\n\n### Executes the executable \"pw.x\" across 2 nodes and 4 processes/CPU cores for the input `pw.scf.silicon.in`\nsrun --mpi=pmix_v3 -N 2 pw.x -nk 4 -i pw.scf.silicon.in\n</code></pre> <p>The below output shows that the workflow has been distributed across 2 nodes, with a total of 4 pools. The computations are performed based on these above-mentioned parallel execution configuration. Also, displays the metrics such as parallelization, overall performance, and successful job completion status. Note that the results only display essential information to aid in understanding the execution of this multi-node job. And, the <code>sacct</code> report indicates that the job is allocated with 4 CPUs across 2 nodes, and was completed successfully.</p> <pre><code>$ cat multinode_27108398.out\n\nProgram PWSCF v.6.3MaX starts on  8Mar2024 at 13:18:37\n\n     This program is part of the open-source Quantum ESPRESSO suite\n     for quantum simulation of materials; please cite\n         \"P. Giannozzi et al., J. Phys.:Condens. Matter 21 395502 (2009);\n         \"P. Giannozzi et al., J. Phys.:Condens. Matter 29 465901 (2017);\n          URL http://www.quantum-espresso.org\",\n     in publications or presentations arising from this work. More details at\n     http://www.quantum-espresso.org/quote\n\n     Parallel version (MPI &amp; OpenMP), running on       4 processor cores\n     Number of MPI processes:                 4\n     Threads/MPI process:                     1\n\n     MPI processes distributed on     2 nodes\n     K-points division:     npool     =       4\n     Reading input from pw.scf.silicon.in\n\n     Current dimensions of program PWSCF are:\n     Max number of different atomic species (ntypx) = 10\n     Max number of k-points (npk) =  40000\n     Max angular momentum in pseudopotentials (lmaxx) =  3\n     .....\n     .....\n          Parallel routines\n\n     PWSCF        :     1.17s CPU         1.36s WALL\n\n   This run was terminated on:  13:18:38   8Mar2024\n=------------------------------------------------------------------------------=\n   JOB DONE.\n=------------------------------------------------------------------------------=\n</code></pre> <pre><code>$ sacct -j 27108398 --format=JobID,JobName,Partition,Account,AllocCPUS,AllocNodes,State,ExitCode\n\n       JobID    JobName  Partition    Account  AllocCPUS AllocNodes      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- ---------- --------\n27108398      multinode amd-hdr100      USER          4          2  COMPLETED      0:0\n27108398.ba+      batch                 USER          3          1  COMPLETED      0:0\n27108398.ex+     extern                 USER          4          2  COMPLETED      0:0\n27108398.0         pw.x                 USER          4          2  COMPLETED      0:0\n</code></pre>"},{"location":"cheaha/slurm/submitting_jobs/","title":"Submitting Jobs with Slurm","text":"<p>Processing computational tasks with Cheaha at the terminal requires submitting jobs to the Slurm scheduler. Slurm offers two commands to submit jobs: <code>sbatch</code> and <code>srun</code>. Always use <code>sbatch</code> to submit jobs to the scheduler, unless you need an interactive terminal. Otherwise only use <code>srun</code> within <code>sbatch</code> for submitting job steps within an <code>sbatch</code> script context.</p> <p>The command <code>sbatch</code> accepts script files as input. Scripts should be written in an available shell language on Cheaha, typically bash, and should include the appropriate Slurm directives at the top of the script telling the scheduler the requested resources. Read on to learn more about how to use Slurm effectively.</p> <p>Important</p> <p>Much of the information and examples on this page require a working knowledge of terminal commands and the shell. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources.</p>"},{"location":"cheaha/slurm/submitting_jobs/#common-slurm-terminology","title":"Common Slurm Terminology","text":"<ul> <li>Node: A self-contained computing devices, forming the basic unit of the cluster. A node has multiple CPUs, memory, and some have GPUs. Jobs requiring multiple nodes must use a protocol such as MPI to communicate between them.<ul> <li>Login nodes: Gateway for reseacher access to computing resources, shared among all users. DO NOT run research computation tasks on the login node.</li> <li>Compute nodes: Dedicated nodes for running research computation tasks.</li> </ul> </li> <li>Core: A single unit of computational processing, not to be confused with a CPU, which may have many cores.</li> <li>Partition: A logical subset of nodes sharing computational features. Different partitions have different resource limits, priorities, and hardware.</li> <li>Job: A collection of commands that require computational resources to perform. Can be interactive with <code>srun</code> or submitted to the scheduler with <code>srun</code> or <code>sbatch</code>.</li> <li>Batch Job: An array of jobs which all have the same plan for execution, but may vary in terms of input and output. Only available in non-interactive batch mode via <code>sbatch</code></li> <li>Job ID: The unique number representing the job, returned by <code>srun</code> and <code>sbatch</code>. Stored in <code>$SLURM_JOB_ID</code> within a job.</li> <li>Job Index Number: For array jobs, the index of the currently running job within the array. Stored in <code>$SLURM_ARRAY_TASK_ID</code> within a job.</li> </ul>"},{"location":"cheaha/slurm/submitting_jobs/#slurm-flags-and-environment-variables","title":"Slurm Flags and Environment Variables","text":"<p>Slurm has many flags a researcher can use when creating a job, but a short list of the most important ones for are described below. It is highly recommended to be as explicit as possible with flags and not rely on system defaults. Explicitly using the flags below makes your scripts more portable, shareable and reproducible.</p> Flag Short Environment Variable Description sbatch srun <code>--job-name</code> <code>-J</code> <code>SBATCH_JOB_NAME</code> Name of job stored in records and visible in <code>squeue</code>. sbatch srun <code>SLURM_JOB_ID</code> Job ID number of running job or array task. May differ from <code>SLURM_ARRAY_JOB_ID</code> depending on array task index sbatch srun <code>--output</code> <code>-o</code> <code>SBATCH_OUTPUT</code> Path to file storing text output. sbatch srun <code>--error</code> <code>-e</code> <code>SBATCH_ERROR</code> Path to file storing error output. sbatch srun <code>--partition</code> <code>-p</code> <code>SBATCH_PARTITION</code> Partition to submit job to. More details below. sbatch srun <code>--time</code> <code>-t</code> <code>SBATCH_TIMELIMIT</code> Maximum allowed runtime of job. Allowed formats below. sbatch srun <code>--nodes</code> <code>-N</code> Number of nodes needed. Set to <code>1</code> if your software does not use MPI or if unsure. sbatch srun <code>--ntasks</code> <code>-n</code> <code>SLURM_NTASKS</code> Number of tasks planned per node. Mostly used for bookkeeping and calculating total cpus per node. If unsure set to <code>1</code>. sbatch srun <code>--cpus-per-task</code> <code>-c</code> <code>SLURM_CPUS_PER_TASK</code> Number of needed cores per task. Cores per node equals <code>-n</code> times <code>-c</code>. sbatch srun <code>SLURM_CPUS_ON_NODE</code> Number of cpus available on this node. sbatch srun <code>--mem</code> <code>SLURM_MEM_PER_NODE</code> Amount of RAM needed per node in MB. Can specify 16 GB using 16384 or 16G. sbatch srun <code>--gres</code> <code>SBATCH_GRES</code> Used to request GPUs per node. For 2 GPUs per node use <code>--gres=gpu:2</code>. sbatch srun <code>--array</code> <code>SBATCH_ARRAY_INX</code> Comma-separated list of similar tasks to run. More details below. sbatch n/a <code>SBATCH_ARRAY_JOB_ID</code> Parent Job ID number of array task. Same for all array tasks submitted with same script. May differ from <code>SLURM_JOB_ID</code> depending on array task index. sbatch n/a <code>SLURM_ARRAY_TASK_COUNT</code> Total number of array tasks. sbatch n/a <code>SLURM_ARRAY_TASK_ID</code> Current array task index. sbatch n/a"},{"location":"cheaha/slurm/submitting_jobs/#available-partitions-for-partition","title":"Available Partitions for <code>--partition</code>","text":"<p>Please see Cheaha Hardware for more information. Remember, the smaller your resource request, the sooner your job will get through the queue.</p>"},{"location":"cheaha/slurm/submitting_jobs/#requesting-gpus","title":"Requesting GPUs","text":"<p>Please see the GPUs page for more information.</p>"},{"location":"cheaha/slurm/submitting_jobs/#dynamic-output-and-error-file-names","title":"Dynamic <code>--output</code> and <code>--error</code> File Names","text":"<p>The <code>--output</code> and <code>--error</code> flags can use dynamic job information as part of the name:</p> <ul> <li><code>%j</code> is the Job ID, equal to <code>$SLURM_JOB_ID</code>.</li> <li><code>%A</code> is the main Array Job ID, equal to <code>$SLURM_ARRAY_JOB_ID</code>.</li> <li><code>%a</code> is the Array job index number, equal to <code>$SLURM_ARRAY_TASK_ID</code>.</li> <li><code>%x</code> is the <code>--job-name</code>, equal to <code>$SLURM_JOB_NAME</code>.</li> </ul> <p>For example if using <code>--job-name=my-job</code>, then to create an output file like <code>my-job-12345678</code> use <code>--output=%x-%j</code>.</p> <p>If also using <code>--array=0-4</code>, then to create an output file like <code>my-job-12345678-0</code> use <code>--output=%x-%A-%a</code>.</p>"},{"location":"cheaha/slurm/submitting_jobs/#batch-jobs-with-sbatch","title":"Batch Jobs with <code>sbatch</code>","text":"<p>Important</p> <p>The following examples assume familiarity with the Linux terminal. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources.</p> <p>Batch jobs are typically submitted using scripts with <code>sbatch</code>. Using <code>sbatch</code> this way is the preferred method for submitting jobs to Slurm on Cheaha. It is more portable, shareable, reproducible and scripts can be version controlled using Git.</p> <p>For batch jobs, flags are typically included as directive comments at the top of the script like <code>#SBATCH --job-name=my-job</code>. Read on to see examples of batch jobs using <code>sbatch</code>.</p>"},{"location":"cheaha/slurm/submitting_jobs/#a-simple-batch-job","title":"A Simple Batch Job","text":"<p>Below is an example batch job script. To test it, copy and paste it into a plain text file <code>testjob.sh</code> in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering <code>cd ~</code> and then entering <code>sbatch testjob.sh</code>. Momentarily, two text files with <code>.out</code> and <code>.err</code> suffixes will be produced in your home directory.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --job-name=test\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH --partition=express\n#SBATCH --time=00:10:00\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\necho \"Hello World\"\necho \"Hello Error\" 1&gt;&amp;2\n</code></pre> <p>There is a lot going on in the above script, so let's break it down. There are three main chunks of this script:</p> <ol> <li>Line 1 is the interpreter directive: <code>#!/bin/bash</code>. This tells the shell what application to use to execute this script. All <code>sbatch</code> scripts on Cheaha should start with this line.</li> <li> <p>Lines 3-11 are the <code>sbatch</code> flags which tell the scheduler what resources you need and how to manage your job.</p> <ul> <li>Line 3: The job name is <code>test</code>.</li> <li>Lines 4-7: The job will have 1 node, with 1 core and 1 GB of memory.</li> <li>Line 8: The job will be on the express partition.</li> <li>Line 9: The job will be no longer than 10 minutes, and will be terminated if it runs over.</li> <li>Line 10: Any standard output (<code>stdout</code>) will be written to the file <code>test_$SLURM_JOB_ID.out</code> in the same directory as the script, whatever the <code>$SLURM_JOB_ID</code> happens to be when the job is submitted. The name comes from <code>%x</code> equal to <code>test</code>, the <code>--job-name</code>, and <code>%j</code> equal to the Job ID.</li> <li>Line 11: Any error output (<code>stderr</code>) will be written to a different file <code>test_$SLURM_JOB_ID.err</code> in the same directory.</li> </ul> </li> <li> <p>Lines 13 and 14 are the payload, or tasks to be run. They will be executed in order from top to bottom just like any shell script. In this case, it is simply writing \"Hello World\" to the <code>--output</code> file and \"Hello Error\" to the <code>--error</code> file. The <code>1&gt;&amp;2</code> Means redirect a copy (<code>&gt;&amp;</code>) of <code>stdout</code> to <code>stderr</code>.</p> </li> </ol>"},{"location":"cheaha/slurm/submitting_jobs/#batch-array-jobs-with-known-indices","title":"Batch Array Jobs With Known Indices","text":"<p>Building on the job script above, below is an array job. Array jobs are useful when you need to perform the same analysis on slightly different inputs with no interaction between those analyses. We call this situation \"pleasingly parallel\". We can take advantage of an array job using the variable <code>$SLURM_ARRAY_TASK_ID</code>, which will have an integer in the set of values we give to the <code>--array</code> flag.</p> <p>To test the script below, copy and paste it into a plain text file <code>testarrayjob.sh</code> in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering <code>cd ~</code> and then entering <code>sbatch testarrayjob.sh</code>. Momentarily, 16 text files with <code>.out</code> and <code>.err</code> suffixes will be produced in your home directory.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --job-name=test\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH --partition=express\n#SBATCH --time=00:10:00\n#SBATCH --output=%x_%A_%a.out\n#SBATCH --error=%x_%A_%a.err\n#SBATCH --array=0-9\n\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\n</code></pre> <p>This script is very similar to the one above, but will submit 10 jobs to the scheduler that all do slightly different things. Each of the 10 jobs will have the same amount and type of resources allocated, and can run in parallel. The 10 jobs come from <code>--array=0-9</code>. The output of each job will be one of the numbers in the set <code>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}</code>, depending on which job is running. The output files will look like <code>test_$(SLURM_ARRAY_JOB_ID)_$(SLURM_ARRAY_TASK_ID).out</code> or <code>.err</code>. The value of <code>$(SLURM_ARRAY_JOB_ID)</code> is the main Job ID given to the entire array submission.</p> <p>Scripts can be written to take advantage of the <code>$SLURM_ARRAY_TASK_ID</code> variable indexing variable. For example, a project could have a list of participants that should be processed in the same way, and the analysis script uses the array task ID as an index to pull out one entry from that list for each job. Many common programming languages can interact with shell variables like <code>$SLURM_ARRAY_TASK_ID</code>, or the values can be passed to a program as an argument.</p> <p>You can override the <code>--array</code> flag stored in the script when you call <code>sbatch</code>. To do so, pass another <code>--array</code> flag along with the script name like below. This allows you to rerun only subsets of your array script.</p> <pre><code># submit jobs with index 0, 3, and 7\nsbatch --array=0,3,7 array.sh\n\n# submit jobs with index 0, 2, 4, and 6\nsbatch --array=0-6:2 array.sh\n</code></pre> <p>For more details on using <code>sbatch</code> please see the official documentation.</p> <p>Note</p> <p>If you are using bash or shell arrays, it is crucial to note they use 0-based indexing. Plan your <code>--array</code> flag indices accordingly.</p>"},{"location":"cheaha/slurm/submitting_jobs/#batch-array-jobs-with-dynamic-or-computed-indices","title":"Batch Array Jobs With Dynamic or Computed Indices","text":"<p>For a practical example with dynamic indices, please visit our Practical <code>sbatch</code> Examples</p>"},{"location":"cheaha/slurm/submitting_jobs/#interactive-jobs-with-srun","title":"Interactive Jobs with <code>srun</code>","text":"<p>Jobs should be submitted to the Slurm job scheduler either using a batch job or an Open OnDemand (OOD) interactive job.</p> <p>You can use <code>srun</code> for working on short interactive tasks such as creating an Anaconda environment and running parallel tasks within an sbatch script.</p> <p>Warning</p> <p>The limitations of <code>srun</code> is that the jobs/execution die if the internet connection is down, and you may have to rerun the job again.</p> <p>We recommend against using <code>srun</code> for any scientific or research computing or data analysis. Use a batch job or an Open OnDemand (OOD) interactive job instead.</p> <p>Let us see how to acquire a compute node quickly using <code>srun</code>. You can run interactive job using <code>srun</code> command with the <code>--pty /bin/bash</code> flag. Here is an example,</p> <pre><code>$srun --ntasks=2 --time=01:00:00 --mem-per-cpu=8G --partition=medium --job-name=test_srun --pty /bin/bash\n\nsrun: job 21648044 queued and waiting for resources\nsrun: job 21648044 has been allocated resources\n</code></pre> <p>The above example allocates a compute node with a 8GB of RAM on a <code>medium</code> partition with <code>--ntasks=2</code> to run short tasks.</p>"},{"location":"cheaha/slurm/submitting_jobs/#srun-for-running-parallel-jobs","title":"<code>srun</code> for running parallel jobs","text":"<p><code>srun</code> is used to run executables in parallel, and is used within <code>sbatch</code> script. Let us see an example where <code>srun</code> is used to launch multiple (parallel) instances of a job.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --job-name=srun_test\n#SBATCH --partition=long\n#SBATCH --time=05:00\n#SBATCH --mem=4G\n\nsrun hostname\n</code></pre> <p>In the script above, we have asked for two nodes --nodes=2, and each node will run a single instance of a <code>hostname</code> as we requested --ntasks-per-node=1. The output for the above script is,</p> <pre><code>c0187\nc0188\n</code></pre> <p>Here is another example of running different independent programs simultaneously on different resources within a batch job. Multiple <code>srun</code> can execute simultaneously as long as they do not exceed the resources reserved for that job i.e., step 1 executes in node 1 with --ntasks=4, and step 2 executes in node 2 with --ntasks=4 simultaneously. Note that <code>--nodes=1 -r1</code> in step 2 defines the number of nodes and their relative node position within the resources assigned to the job.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks=8\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=1\n#SBATCH --partition=amd-hdr100\n#SBATCH --time=05:00\n#SBATCH --mem-per-cpu=1G\n\n#Partioning of resources for two different tasks\n#STEP 1\nsrun --nodes=1 --ntasks=4 hostname\n#STEP 2\nsrun --nodes=1 -r1 --ntasks=4 uname -a\n</code></pre> <p>Here is the output for running multiple <code>srun</code> in a single job, i.e., executing the <code>hostname</code> and <code>uname -a</code> tasks simultaneously but on different nodes.</p> <pre><code>c0203\nc0203\nc0203\nc0203\nLinux c0204 3.10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\nLinux c0204 3.10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\nLinux c0204 3.10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\nLinux c0204 3.10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre> <p>Alternatively, <code>srun</code> can also run MPI, OpenMP, hybrid MPI/OpenMP, and many more parallel jobs. For more details on using <code>srun</code>, please see the official documentation.</p> <p>Important</p> <p><code>srun</code> has been disabled for use with MPI. We have removed this functionality due to an open vulnerability: https://nvd.nist.gov/vuln/detail/CVE-2023-41915. The vulnerability could allow an attacker to escalate privileges to root and/or access data they do not have permissions for.</p> <p>Instead of <code>srun</code>, please load one of the <code>OpenMPI</code> modules with an appropriate version. Please contact Support with any questions or concerns.</p>"},{"location":"cheaha/slurm/submitting_jobs/#environment-setup-and-module-usage-in-job-submission","title":"Environment Setup and Module Usage in Job Submission","text":"<p>Before submitting a job using <code>sbatch</code>, it's crucial to establish a tailored environment, including software installations and loading necessary modules containing the required software packages. We highly recommend the practice of putting <code>module reset</code> before any <code>module load</code> calls in job scripts. The module system modifies the environment whenever the module list changes, and Slurm jobs inherit the environment from whatever called <code>sbatch</code> or <code>srun</code>. The module reset command normalizes the initial environment for the script, improving repeatability and minimizing the risk of hard-to-diagnose module conflicts. For examples and further information, please see best practice for loading modules.</p>"},{"location":"cheaha/slurm/submitting_jobs/#graphical-interactive-jobs","title":"Graphical Interactive Jobs","text":"<p>It is highly recommended to use the Open OnDemand web portal for interactive apps. Interactive sessions for certain software such as MATLAB and RStudio can be created directly from the browser while an HPC Desktop is available to access all of the other software on Cheaha. A terminal is also available through Open OnDemand.</p> <p>It is possible to use other remote desktop software, such as VNC, to start and interact with jobs. These methods are not officially supported and we do not have the capacity to help with remote desktop connections. Instead, please consider switching your workflow to use the Open OnDemand HPC Desktop. If you are unable to use this method, please contact Support.</p>"},{"location":"cheaha/slurm/submitting_jobs/#estimating-compute-resources","title":"Estimating Compute Resources","text":"<p>Being able to estimate how many resources a job will need is critical. Requesting many more resources than necessary bottlenecks the cluster by reserving unused resources for an inefficient job preventing other jobs from using them. However, requesting too few resources will slow down the job or cause it to error.</p> <p>Questions to ask yourself when requesting job resources:</p> <ol> <li>Can my scripts take advantage of multiple CPUs?<ol> <li>For instance, RStudio generally works on a single thread. Requesting more than 1 CPU here would not improve performance.</li> </ol> </li> <li>How large is the data I'm working with?</li> <li>Do my pipelines keep large amounts of data in memory?</li> <li>How long should my job take?<ol> <li>For example, do not request 50 hours time for a 15 hour process. Have a reasonable buffer included to account for unexpected processing delays, but do not request the maximum time on a partition if that's unnecessary.</li> </ol> </li> </ol> <p>Note</p> <p>Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs.</p> <p>To get the most out of your Cheaha experience and ensure your jobs get through the queue as fast as possible, please read about Job Efficiency.</p>"},{"location":"cheaha/slurm/submitting_jobs/#faster-queuing-with-job-efficiency","title":"Faster Queuing with Job Efficiency","text":"<p>Please see our page on Job Efficiency for more information on making the best use of cluster resources to minimize your queue wait times.</p>"},{"location":"cheaha/software/modules/","title":"Pre-installed Modules","text":"<p>Most software available on Cheaha is installed as modules, managed by the Lmod system. This document will provide a basic rundown of using Lmod commands to customize a software environment. <code>module</code> is the main command used to interface with module files in Lmod.</p> <p>As of the most recent update of this page there are 4,445 active modules installed on Cheaha. The most commonly used general research software modules are listed in the table below. Read on to learn more about searching for and loading modules. If you can't find what you need in our modules, learn more about getting software installed. If you need further assistance, please contact Support.</p> Name Description Anaconda3 Software that can install the Python language, Python packages, and other research software. Learn more about using Anaconda at our Anaconda on Cheaha page. You may be interested in our OpenOnDemand Jupyter Notebook interactive app. CUDA, cuDNN Libraries for developing and using deep learning and AI models with NVidia GPUs. Commonly used with TensorFlow and PyTorch. See our GPU page for more information. Mathematica Mathematical CAS and numerical computing software. Try our Open OnDemand HPC Desktop interactive app. Matlab Matlab language and development environment. We recommend using our Open OnDemand Matlab interactive app. R, Rstudio R language and RStudio IDE. We recommend using our Open OnDemand RStudio interactive app. SAS Statistical analysis software. Try our Open OnDemand HPC Desktop interactive app. Singularity Software container engine. See our Containers page for more information. Stata Statistical analysis software. Try our Open OnDemand HPC Desktop interactive app."},{"location":"cheaha/software/modules/#listing-and-searching-modules","title":"Listing and Searching Modules","text":"<p>To begin, all module commands are run from the terminal. To know what software is installed on Cheaha, use the <code>avail</code> command.</p> <pre><code>module avail\n</code></pre> <p>If you need to know what software is already loaded in your environment, run:</p> <pre><code>module list\n</code></pre> <p>If there is specific software you want to search for, you can use the <code>spider</code> subcommand, and provide a string or regular expression to match against. All modules containing the string (case-insensitive) or matching the regular expression will be returned along with their installed versions.</p> <pre><code># list modules containing string\nmodule spider &lt;string&gt;\n\n# list modules matching a regular expression\nmodule -r spider &lt;regex&gt;\n</code></pre>"},{"location":"cheaha/software/modules/#loading-modules","title":"Loading Modules","text":"<p>To load modules, run:</p> <pre><code>module load module1 module2 ...\n</code></pre> <p>Note</p> <p>If you only specify a module name without an accompanying version tag, the most recently installed version will be loaded into the workspace. If your scripts depend on specific versions of software being used, explicitly load the module version you need.</p> <p>To unload packages, run:</p> <pre><code>module unload package1 package2 ...\n</code></pre> <p>If you want to revert to the default modules, you can use:</p> <pre><code>module reset\n</code></pre>"},{"location":"cheaha/software/modules/#saving-modules-using-collections","title":"Saving Modules using Collections","text":"<p>To save time in typing in long list of modules everytime you work on a project, you can save the desired list of modules using module collection. To acheive this, load the desired modules and save them to a collection using a module collection name, as shown below.</p> <pre><code>module load module_1 module_2 ...\n\nmodule save collection_name\n</code></pre> <p>Here, the <code>collection_name</code> can be something relevant to your project and easy to remember.</p> <p>To load the desired modules using the saved collection use,</p> <pre><code>module restore collection_name\n</code></pre> <p>To delete a collection use the below command,</p> <pre><code>module disable collection_name\n</code></pre> <p>To list the save list of module collection use,</p> <pre><code>module savelist\n</code></pre> <p>Warning</p> <p>Using <code>module save</code> command without a collection name saves the desired modules in the name <code>default</code> to the location $HOME/.lmod.d/default, and causes issue in launching Open On Demand (OOD) HPC desktop job. The user gets a VNC error such as, <code>Unable to contact settings server</code> and/or <code>Unable to load a failsafe session</code>.  To address this issue, it is recommended to follow the instructions outlined in the FAQ entry.</p>"},{"location":"cheaha/software/modules/#best-practice-for-loading-modules","title":"Best Practice for Loading Modules","text":"<p>When using modules in Cheaha, we recommend users to follow these best practices to avoid any potential module conflicts, reduce unexpected behavior and/or to get rid of Lmod errors:</p> <ol> <li> <p>Avoid using <code>module load</code> in <code>$HOME/.bashrc</code>. Instead, create a bash script with the module load commands and source it each time to load the modules needed in a shell/sbatch script. Here is an example of loading module in a bash script named <code>module_test.sh</code> and compilation,</p> <pre><code>#!/bin/bash\nmodule reset\nmodule load Bowtie/1.1.2-foss-2016a\nmodule load SAMtools/1.3.1-foss-2016a\nmodule load TopHat/2.1.1-foss-2016a\nmodule -t list\n</code></pre> <pre><code>$ chmod +x module_test.sh\n$ source ./module_test.sh\n\nResetting modules to system default\n\n# Currently Loaded Modules\nshared\nslurm/18.08.9\nrc-base\nDefaultModules\nGCCcore/4.9.3\nbinutils/2.25-GCCcore-4.9.3\nGCC/4.9.3-2.25\nnumactl/2.0.11-GCC-4.9.3-2.25\nhwloc/1.11.2-GCC-4.9.3-2.25\nOpenMPI/1.10.2-GCC-4.9.3-2.25\nOpenBLAS/0.2.15-GCC-4.9.3-2.25-LAPACK-3.6.0\ngompi/2016a\nFFTW/3.3.4-gompi-2016a\nScaLAPACK/2.0.2-gompi-2016a-OpenBLAS-0.2.15-LAPACK-3.6.0\nfoss/2016a\nBowtie/1.1.2-foss-2016a\nncurses/6.0-foss-2016a\nzlib/1.2.8-foss-2016a\nSAMtools/1.3.1-foss-2016a\nbzip2/1.0.6-foss-2016a\nBoost/1.61.0-foss-2016a\nTopHat/2.1.1-foss-2016a\n</code></pre> </li> <li> <p>Be selective and only load a specific module version that you need for your current workflow. Loading unnecessary modules can lead to conflicts and inefficiencies.</p> </li> <li>Before loading modules in a shell/bash/sbatch script, use a clean shell by using <code>module reset</code> at the beginning.<ul> <li>What it does:<ul> <li>Clearing loaded modules.</li> <li>Loading default modules specified by the system administrator.</li> </ul> </li> <li>What it prevents from happening:<ul> <li>Module conflicts.</li> </ul> </li> <li>Why it is a best-practice:<ul> <li>Ensures reproducibility by starting with a clean environment.</li> <li>Manages software dependencies effectively.</li> </ul> </li> </ul> </li> </ol> <p>Using <code>module reset</code> before loading modules separates what software is loaded in the working shell from the software loaded in the script shell. Be aware that forked processes (like scripts) and Slurm commands inherit the environment variables of the working shell, including loaded modules. Here is an example that shows module conflict between cuda11.8 and cuda11.4 versions that may lead to unexpected behavior or an erroneous output.</p> <pre><code># Working shell where you may try testing module load and your run script\n$ module load cuda11.4/toolkit\n\n$ module -t list\n\n#Currently Loaded Modules\nshared\nslurm/18.08.9\nrc-base\nDefaultModules\ncuda11.4/toolkit/11.4.2\n</code></pre> <pre><code># bash script you are passing in a sbatch script\n#!/bin/bash\nmodule load cuda11.8/toolkit\nmodule -t list\n</code></pre> <pre><code># Not using `module reset` at the beginning of the bash script could cause CUDA conflict issues.\n$ source ./module_test2.sh\n\n#Currently Loaded Modules\nshared\nslurm/18.08.9\nrc-base\nDefaultModules\ncuda11.4/toolkit/11.4.2\ncuda11.8/toolkit/11.8.0\n</code></pre> <p>Note</p> <p>The best practice would be to avoid using <code>module reset</code> in the <code>Environment Setup</code> of Open OnDemand jobs as the OOD session, by default, resets the module at the beginning of every session. It is observed to cause unexpected behavior if <code>module reset</code> is used in the Rstudio server OOD sessions.</p>"},{"location":"cheaha/software/modules/#licensed-and-commercial-software-restrictions","title":"Licensed and Commercial Software Restrictions","text":"<p>The following software have license restrictions that may preclude some researchers or collaborators depending on their departmental or group affiliations. In the table, \"affiliated\" means employed by, or a student of, unless otherwise noted. External collaborators are not considered affiliated with UAB for the purposes of software licensing and access, unless otherwise noted. These software packages may be commercial paid software. If you believe you should have access to software that you do not have access to, please contact Support.</p> Software Restricted to... License Holder Amber Lab Specific Lab PI Ansys School of Engineering affiliated people School of Engineering Gurobi Named individual Individuals LS-Dyna School of Engineering affiliated people School of Engineering Matlab UAB affiliated people UAB Campus Mathematica UAB affiliated people UAB Campus SAS UAB affiliated people UAB Campus Stata UAB affiliated people UAB Campus <p>Use of these software packages without authorization may be a violation of the UAB IT Acceptable Use Policy.</p>"},{"location":"cheaha/software/modules/#security-issues","title":"Security Issues","text":""},{"location":"cheaha/software/modules/#igv","title":"IGV","text":"<p>Danger</p> <p>Versions of IGV prior to <code>2.11.9</code> use a compromised version of log4j. Those versions are affected by a serious remote code execution issue. Please transition your software to use versions of IGV &gt;= <code>2.11.9</code>.</p>"},{"location":"cheaha/software/modules/#gsea","title":"GSEA","text":"<p>Danger</p> <p>Versions of GSEA prior to <code>4.2.3</code> use a compromised version of log4j. Those versions are affected by a serious remote code execution issue. Please transition your software to use versions of GSEA &gt;= <code>4.2.3</code>.</p>"},{"location":"cheaha/software/modules/#known-issues","title":"Known Issues","text":""},{"location":"cheaha/software/modules/#matlab-issues","title":"Matlab Issues","text":"<p>There is a critical, hard-to-diagnose MATLAB parpool bug in versions before R2022a.</p> <p>The issue arises when using a <code>parpool</code> for multiple jobs simultaneously, as with an <code>sbatch --array</code> job. MATLAB <code>parpool</code> can be started manually, or at the first <code>parfor</code> loop encountered, among other functionality. See the MATLAB Documentation for more information and a complete list.</p> <p>Before R2022a, MATLAB assumed that only one parpool will be used at a time for each user, and put necessary communication files in a common directory. When multiple parpools are run simultaneously by the same user, they may attempt to write to those files at the same time, corrupting the files, resulting in a range of obscure Parallel Computing Toolbox (PCT) errors. The collisions are effectively random, which can make the issue hard to reproduce and hard to diagnose. The more parpools open simultaneously, the more likely there will be at least one error. In the worst case, we have seen unrecoverable corruption of the parpool common directory, which can be fixed by deleting the directory.</p> <p>Symptoms of the bug include:</p> <ul> <li>Excessive load and context switching on affected nodes</li> <li>Inconsistent and varied PCT errors</li> <li>Inability to start Matlab parpool</li> </ul> <p>To avoid the bug, please use the latest available version of MATLAB and no earlier than R2022a. Upgrading MATLAB versions may require some effort and testing of your code, because MATLAB is not always backwards compatible. Be sure to test that your code works as expected on the new version before using it for research.</p> <p>If you aren't able to use R2022a or newer, there is a workaround available. Please navigate to this GitHub repository and follow the instructions in <code>README.md</code>. Some light MATLAB programming is required to effectively use the workaround. Please contact Support if you would like assistance.</p>"},{"location":"cheaha/software/software/","title":"Software Installation","text":""},{"location":"cheaha/software/software/#anaconda-on-cheaha","title":"Anaconda on Cheaha","text":"<p>For additional general information on using Anaconda please see Anaconda Environments.</p> <p>If you are using Jupyter Notebook, please see our section on Packages for Jupyter.</p>"},{"location":"cheaha/software/software/#loading-anaconda","title":"Loading Anaconda","text":"<p>Anaconda is installed on Cheaha as a family of modules, and does not need to be installed by Researchers. Instead, the most recent version of Anaconda installed on Cheaha may be loaded using the command <code>module load Anaconda3</code>. Other versions may be discovered using the command <code>module avail Anaconda</code>. We recommend always using the latest version.</p> <p>Note</p> <p>If you are using Open OnDemand Jupyter Notebook you do not need to use the <code>module load</code> command as part of creating the job.</p>"},{"location":"cheaha/software/software/#using-anaconda","title":"Using Anaconda","text":"<p>Anaconda on Cheaha works like it does on any other system, once the module has been loaded, with a couple of important differences in the callouts below.</p> <p>Note</p> <p>The <code>base</code> environment is installed in a shared location and cannot be modified by researchers. Other environments are installed in your home directory by default.</p> <p>Important</p> <p>Only create environments on compute nodes. Anaconda environment creation consumes substantial resources and should not be run on the login node.</p> <p>Warning</p> <p>The Cheaha operating system has a version of Python installed. This version is used by <code>python</code> calls when Anaconda has not been loaded. This can cause unexpected errors. Be sure you've loaded the Anaconda environment you need before using Python.</p> <p>Danger</p> <p>Do not use <code>conda init</code> on Cheaha! Anaconda is managed as a module, including script setup. Using <code>conda init</code> at any point can cause hard-to-diagnose issues with Open OnDemand Interactive Jobs. Please see this ask.ci FAQ for how to undo what <code>conda init</code> does.</p> <p>If the Anaconda software instructs you to use <code>conda init</code> while on Cheaha, please ignore it to avoid future issues with Open OnDemand.</p> <p>For more information on usage with examples, see Anaconda Environments. Need some hands-on experience, you can find instructions on how to install PyTorch and TensorFlow using Anaconda in this tutorial.</p>"},{"location":"cheaha/software/software/#singularity-containers","title":"Singularity Containers","text":"<p>Containers are a very useful resource for installing software without needing administrator permission. Please read the full documentation about singularity and containers on our main Singularity page.</p>"},{"location":"cheaha/tutorial/","title":"Getting Started with Using Anaconda on Cheaha","text":"<p>Python is a high level programming language that is widely used in many branches of science. As a result, many scientific packages have been developed in Python, leading to the development of a package manager called Anaconda. Anaconda is a widely used Python package manager for scientific research. Consequently Anaconda is used on Cheaha for managing environments and packages.</p> <p>Have you encountered problems while using Anaconda on Cheaha? We have provided this page to curate a number of walkthroughs on how you can address majority of the needs you may have or challenges you may experience using Anaconda on Cheaha.</p> <p>Below is a list of Tutorials we currently have Using Anaconda on Cheaha;</p> <ol> <li>Using PyTorch and TensorFlow with Anaconda on Cheaha, click here.</li> </ol>"},{"location":"cheaha/tutorial/pytorch_tensorflow/","title":"Anaconda Environment Tutorial for PyTorch and TensorFlow","text":"<p>The below tutorial would show you steps on how to create an Anaconda environment, activate, and install libraries/packages for machine and deep learning (PyTorch and Tensorflow) using an Anaconda environment on Cheaha. There are also steps on how to access the terminal, as well as using Jupyter Notebook's Graphical User Interface (GUI) to work with these Anaconda environments. There are detailed steps here to guide your creation of a Jupyter Notebook job.</p>"},{"location":"cheaha/tutorial/pytorch_tensorflow/#installing-anaconda-environments-using-the-terminal","title":"Installing Anaconda Environments Using the Terminal","text":"<p>To access the terminal (shell), please do the following.</p> <ol> <li> <p>Login to rc.uab.edu</p> </li> <li> <p>Create a job on Cheaha using the Interactive Apps dropdown option.</p> </li> <li> <p>Select Jupyter Notebook, and fill out the options, as per your project needs, then click Launch.  For more information on compute needs, and a guide for selecting the right options click here. </p> </li> <li> <p>Click the Connect to Jupyter button </p> <p>You will see the below interface. </p> </li> <li> <p>When the job has been created, on the My Interactive Sessions page, click the button in front of Host (usually colored blue) in the format &gt;_c0000.</p> <p></p> <p>This should open into a terminal as shown below.</p> <p></p> </li> <li> <p>In this interface, you can create, and activate environments, as well as install packages, modules and libraries into your activated environment.</p> </li> </ol>"},{"location":"cheaha/tutorial/pytorch_tensorflow/#how-do-we-create-a-custom-environment-for-pytorch-and-tensorflow","title":"How do we create a custom environment for PyTorch and TensorFlow","text":"<p>The instructions below, provide a recommended step by step guide to creating and activating an environment that has PyTorch and/or TensorFlow installed and ready to use for deep learning projects.</p>"},{"location":"cheaha/tutorial/pytorch_tensorflow/#installing-pytorch-using-the-terminal","title":"Installing PyTorch Using the Terminal","text":"<p>There are two instances of PyTorch that can be installed, one requiring GPUs, and another utilising only CPUs. GPUs generally improve project compute speeds and are preferred. For both instances of pytorch, please follow these steps;</p> <ol> <li> <p>Create and activate an environment as stated in these links.</p> </li> <li> <p>Access the terminal following the steps here.</p> </li> </ol> <p>Note</p> <p>When installing packages, modules and libraries into environments, remember to also install <code>ipykernel</code> using <code>conda install ipykernel</code>. This way your activated environment would appear in the list of kernels in your Jupyter Notebook.</p> <p>For a correct installation of pytorch, we have to ensure some conditions are met. See partition docs for a guide. One of such conditions, is to load CUDA toolkit using the below command in your environment setup form (see image below).</p> <pre><code>module load CUDA/11.8.0\n</code></pre> <p></p> <p>Note</p> <p>The cudatoolkit version may vary, as at the time of this tutorial, 11.8 is the version used. Running <code>nvidia-smi</code>, as in the image below, will show you the status, version and other information on GPUs in your created job session. The CUDA version is highlighted. The GPU CUDA Version available on Cheaha at the time of this tutorial is 12.3. Because the toolkit version used is lower than the Cheaha GPU version, it works.</p> <p></p> <p>When your job has been created and your environment created and activated from the terminal (see above instructions), run the below command.</p> <pre><code>conda install pytorch torchvision torchaudio cudatoolkit=11.8 -c pytorch -c nvidia\n</code></pre> <p>This commands will install a GPU compatible PyTorch version into your environment. To verify PyTorch is installed, and to see what version you have installed in your environment, use the below command.</p> <pre><code>conda list | grep \"torch\"\n</code></pre> <p>You should get an output like the below image.</p> <p></p> <p>The same process can be followed for installing another Deep Learning library Tensorflow (see instructions below) with some minute differences. You may decide to install the TensorFlow library into the same environment or create a new one. As a best practice, you may want to install these libraries in different environments.</p>"},{"location":"cheaha/tutorial/pytorch_tensorflow/#using-pytorch-on-jupyter-notebook","title":"Using PyTorch on Jupyter Notebook","text":"<p>As an example we will be using a sample Jupyter Notebook with just a simple torch function to test if a GPU will be utilized with PyTorch functions. Run the command in a cell, and if your output is <code>True</code>, then you have your GPU setup to support PyTorch functions.</p> <pre><code>import torch\n\nprint(torch.cuda.is_available())\nx = torch.cuda.current_device()\nprint(torch.cuda.get_device_name(x))\n</code></pre> <p></p>"},{"location":"cheaha/tutorial/pytorch_tensorflow/#install-tensorflow-gpu-using-the-terminal","title":"Install TensorFlow GPU Using the Terminal","text":"<ol> <li> <p>Create a new environment that is compatible with supported tensorflow versions, use the below command to do this. For this tutorial we will use Python 3.11.</p> <pre><code>conda create -n tensorflow python=3.11\n</code></pre> </li> <li> <p>The TensorFlow CPU and GPU versions requires pip to be up-to-date, to install and upgrade pip to the latest version use the below command.</p> <pre><code>pip install --upgrade pip\n</code></pre> </li> <li> <p>Install TensorFlow with pip</p> <pre><code>pip install tensorflow[and-cuda]\n</code></pre> </li> </ol> <p>The image below shows an output that the TensorFlow library will utilize the available GPU.</p> <p></p> <p>Note</p> <p>The information (I) and warning (W) outputs notifies you of the installed Tensorflow binary and how it would function. The I output informs you that the installed Tensorflow library will utilize your CPU for additional speed when GPUs are not the most efficient way to do processing for these operations. The W output tells you TensorRT is not available, please note TensorRT is not currently supported on our systems.</p> <p>Now that you have completed the tutorial, you can find more Anaconda information here, Using Anaconda page.</p>"},{"location":"contributing/contributor_guide/","title":"Contributor Guide","text":"<p>We appreciate any and all opportunities to improve our documentation, and your contributions are welcome! To ensure a high-quality documentation experience, we have some guidelines for contributors who wish to create.</p>"},{"location":"contributing/contributor_guide/#faq","title":"FAQ","text":""},{"location":"contributing/contributor_guide/#how-do-i-report-content-inaccuracies-or-errors","title":"How do I report content inaccuracies or errors?","text":"<ul> <li>Prepare to write write down what you've found:<ul> <li>Note the URL of the page using your browser's address bar.</li> <li>Note the section where the inaccuracy is located.</li> <li>Have in mind a way to share what you've found and why you believe it is inaccurate.</li> </ul> </li> <li>Create a new issue<ul> <li>Click \"Get Started\" next to \"Inaccuracy Report\".</li> <li>Write a brief title that closely describes what you found.</li> <li>Under \"What is Inaccurate?\" please write the specifics of what content is not accurate or contains errors, and any relevant explanations.</li> <li>Under \"Where is the Inaccuracy?\" please paste the URL of the page from your browser's address bar, along with the section where you found inaccuracy.</li> </ul> </li> </ul>"},{"location":"contributing/contributor_guide/#creating-content","title":"Creating Content","text":"<p>We welcome content from our community! Most style choices and formatting are automated by our VSCode development environment, and most validation steps are automated with pre-commit hooks and CI/CD.</p> <p>If you wish to use VSCode, read Our Preferred Development Environment, and most of the work should be taken care of for you. VSCode will provide syntax highlighting and formatting automatically if you install the recommended extensions. Some linter warnings cannot be fixed automatically, so please pay close attention to what they say, and reference our Style Guide for guidance.</p> <p>If you choose not to use VSCode, be mindful of our Style Guide section below for guidance on how to craft content that meet our standards. Our Automation</p> <p>If you need assistance, please feel free to contact us.</p>"},{"location":"contributing/contributor_guide/#our-preferred-development-environment","title":"Our Preferred Development Environment","text":"<p>We understand that everyone has differing preferences when it comes to development environments, so please feel free to use the development environment of your choice. Please be aware that our content has been developed using VSCode and a collection of extensions, so the greatest level of support can be provided by us to you if you choose to use our tooling.</p> <p>We are using Visual Studio Code (VSCode) for development with several extensions installed, listed below. The extensions are also in <code>.vscode/extensions.json</code> and should pop up as recommendations when you open this repository. We use VSCode for the productivity benefits related to local Anaconda environment management, git integration, and dynamic formatters and linting. Linting is provided by pre-commit hooks and in our Continuous Integration definitions.</p> <p>VSCode may be obtained from Visual Studio Code and documentation is available at VSCode: Docs. The extensions should automatically show up as recommendations when opening the repo, or they can be downloaded using the VSCode Extensions menu (Ctrl+Shift+X on Windows or Cmd+Shift+X on Mac).</p> <p>We assume you have a <code>conda</code> distribution on your local machine. If you are affiliated with UAB, please install Miniforge. For detailed installation instructions, see here: https://github.com/conda-forge/miniforge/?tab=readme-ov-file#install. For more information on using <code>conda</code>, see our Anaconda page.</p>"},{"location":"contributing/contributor_guide/#style-guide","title":"Style Guide","text":""},{"location":"contributing/contributor_guide/#documentation-markdown-files","title":"Documentation Markdown Files","text":"<p>Documentation file style is automated and enforced by <code>markdownlint</code> through Continuous Integration when a Pull Request is made and when changes are pushed to the main branch. Linting is controlled by the <code>.markdownlint.json</code> file in the repository root. Linting codes are documented here: https://github.com/DavidAnson/markdownlint/tree/main/doc. Our choices are outlined below. We are using markdownlint-cli2 for markdown linting.</p> <p>To manually lint, see here.</p> <p>Admonitions are allowed, see here for how to use them and pass markdownlint limitations.</p> <p>Sometimes the Table of Contents, rendered on the right-hand side of pages, can get long or deeply nested, becoming hard to scan. If this happens on a page, use the <code>front matter</code> snippet. Move the caret to the top of the document, access the snippets menu from the command palette Ctrl+Shift+P, select \"Snippets: Insert Snippet\", then select <code>front matter</code>. The default level is 3, but this can be adjusted as needed.</p>"},{"location":"contributing/contributor_guide/#headings","title":"Headings","text":"<ul> <li>MD003: Open ATX headers denoted by one or more leading hash <code>#</code> characters.</li> <li>MD022 - Headings: Headings must have exactly one blank line before and one after.</li> <li>MD025 - Headings: Only one top-level heading (single hash <code>#</code> character) allowed per file.</li> <li>MD041 - Headings: The first line in each file must be a top-level heading (single has <code>#</code> character).</li> <li>MD043 - Headings: Heading structure is not prescribed.</li> </ul>"},{"location":"contributing/contributor_guide/#whitespace","title":"Whitespace","text":"<ul> <li>MD009 - Whitespace: Trailing spaces not allowed.</li> <li>MD010 - Whitespace: Hard tab characters <code>\\t</code> not allowed.</li> <li>MD012 - Whitespace: Multiple consecutive blank lines not allowed.</li> <li>MD030 - Whitespace: Exactly one space character between list item markers and list item contents, e.g., <code>1. item</code>.</li> </ul>"},{"location":"contributing/contributor_guide/#lists","title":"Lists","text":"<ul> <li>MD004: Unordered list items markers are the dash <code>-</code> character.</li> <li>MD007: Unordered list indentation of 4 characters.</li> <li>MD029: All ordered list item markers are <code>1.</code> to provide consistent width and spacing for improved readability.</li> </ul>"},{"location":"contributing/contributor_guide/#code-blocks","title":"Code Blocks","text":"<ul> <li>MD031: Fenced code blocks must have exactly one blank line before and one after.</li> <li>MD040: Fenced code block language may contain additional content, allowing the use of, e.g., line numbering with <code>mkdocs-material</code>.</li> <li>MD046: Code blocks must be fenced style, e.g., surrounded by <code>```</code>.</li> <li>MD048: Fenced code blocks must use backtick <code>`</code> characters.</li> </ul>"},{"location":"contributing/contributor_guide/#misc","title":"Misc","text":"<ul> <li>MD013: Line length not enforced. Use word wrapping in your editor instead.</li> <li>MD033: No HTML elements allowed, e.g. <code>&lt;br&gt;</code>.</li> <li>MD035: Horizontal rules must be exactly <code>---</code>.</li> <li>MD044: Proper names should have correct capitalization. Code blocks are excluded, HTML elements are included. See the <code>.markdownlint.json</code> file, in the repository root, for a complete list.</li> <li>MD049: Emphasized text (this text is emphasized) must use single underscores, e.g., <code>_this text is emphasized_</code>.</li> <li>MD050: Strong text (this text is strong) must use double asterisks, e.g., <code>**this text is strong**</code>.</li> <li>MD055: Table rows must have leading and trailing pipe <code>|</code> characters.</li> </ul>"},{"location":"contributing/contributor_guide/#csv-files-for-markdown-tables","title":"CSV Files for Markdown Tables","text":"<p>Prefer to use commas <code>,</code> as separators, quoting entries with double quotes <code>\"</code> where necessary. These are required by the default table reader plugin settings. Formatting and style are not automated at this time.</p> <p>Most simple markdown formatting used within CSV Files will be rendered as expected. We encourage cross-linking from within tables to other parts of the documentation, or linking to external resources, as appropriate. Most VSCode markdown extension features do not function within CSV tables, and these are not linted at this time.</p>"},{"location":"contributing/contributor_guide/#mkdocsyml","title":"<code>mkdocs.yml</code>","text":"<p>Style is not automated at this time as the cost is greater than the benefit. Entries in the following keys should be sorted alphabetically. Write lists and maps in the dashed style, rather than the bracketed <code>[]</code> style.</p> <ul> <li><code>markdown_extensions:</code></li> <li><code>plugins</code></li> <li><code>plugins: redirects: redirect-maps:</code></li> </ul>"},{"location":"contributing/contributor_guide/#build_envyml","title":"<code>build_env.yml</code>","text":"<p>Style is not automated at this time as the cost is greater than the benefit. Entires in the following keys should be sorted alphabetically.</p>"},{"location":"contributing/contributor_guide/#development","title":"Development","text":"<p>The workflow below assumes you are using VSCode and all of the prerequisites listed above. Some familiarity with git and GitHub are assumed.</p>"},{"location":"contributing/contributor_guide/#obtaining-a-working-copy-of-the-repository","title":"Obtaining a Working Copy of the Repository","text":"<p>Before you can get started working on contributions, you'll need a copy of the repository. The first step, done only once, is to fork the repository in GitHub to your personal account. The repository is located at https://github.com/uabrc/uabrc.github.io. More in-depth documentation on forking can be found at GitHub: Fork a Repo.</p> <p>Once the fork has been created, you can clone your fork using the Command Palette (Ctrl+Shift+P) and <code>Git: Clone...</code> in VSCode, or at the command line. More information on cloning can be found at GitHub: Cloning a Repository. More information on using git can be found at our git page.</p>"},{"location":"contributing/contributor_guide/#local-machine-setup-laptopdesktop","title":"Local Machine Setup (Laptop/Desktop)","text":"<ol> <li>Install <code>conda</code> on your machine using Miniforge.</li> <li>Configure <code>conda</code> to be visible to VSCode. The Miniforge default install choices will help do this correctly for your operating system. Important: On Windows, do not add <code>conda</code> to your <code>PATH</code> variable as it can disrupt proper operating system functioning.</li> <li> <p>Install the conda environment from <code>build_env.yml</code> using the following command</p> <pre><code>conda env create -f build_env.yml\n</code></pre> </li> <li> <p>Register the environment with this repository in VSCode:</p> <ol> <li>Open the Command Palette (Ctrl+Shift+P).</li> <li>Search for \"Python: Select Interpreter\" and select it.</li> <li>Search for the installed environment and select it.</li> </ol> </li> </ol> <p>Now pre-commit hooks should function as expected and manual validation is available on your local machine. You will also be able to build and serve the documentation locally to help you check your contributions before making a pull request.</p> <p>To build the documentation locally, press F5 in VSCode to start a local server and open the docs in your browser. This is provided by the <code>.vscode/launch.json</code> file. Alternatively, use <code>mkdocs serve --open</code> in a terminal to get the same effect. Be sure you've got your <code>conda</code> environment activated! We recommend trying this now to test your setup.</p>"},{"location":"contributing/contributor_guide/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/contributor_guide/#create-a-working-branch","title":"Create a working branch","text":"<p>You'll need to create a new branch on your local machine (the working branch). The purpose of a branch is to encapsulate a coherent set of changes to add or deprecate a feature, or fix a bug. Typically each bug is associated with one issue, or a set of very closely-related issues. See our Issue Tracker for available issues.</p> <ul> <li>A branch should be created to resolve an issue, and lives only until the issue is resolved.</li> <li>Give the branch a short but meaningful name for your intended changes.</li> <li>For bug fixes use names like <code>fix-broken-link</code> or <code>fix-page-formatting</code>.</li> <li>For new features or pages, use names like <code>feat-lts-page</code> or <code>feat-accessibility-improvement</code>.</li> <li>Command to create new branch in terminal: <code>git checkout -b &lt;name&gt;</code>.</li> </ul>"},{"location":"contributing/contributor_guide/#implement-your-changes","title":"Implement your changes","text":"<p>You'll need to add, remove or otherwise modify files as appropriate to implement the changes you intend.</p> <ul> <li>Stage and commit changes in small units as you go.</li> <li>Be sure you are on the correct branch, i.e. your working branch!</li> <li> <p>VSCode facilitates staging and committing files.</p> <p></p> </li> </ul>"},{"location":"contributing/contributor_guide/#verify-your-changes","title":"Verify your changes","text":"<ol> <li>Activate your conda environment.<ol> <li>Open the file <code>test.py</code> in the repository to start the Python extension.</li> <li>Select the interpreter using https://code.visualstudio.com/docs/python/environments#_select-and-activate-an-environment</li> </ol> </li> <li>Open a VSCode terminal using Ctrl+Shift+`.</li> <li> <p>Execute the command <code>mkdocs serve</code></p> <p></p> </li> <li> <p>If a new browser tab does not open automatically, use your browser to navigate to <code>http://localhost:8000</code>.</p> </li> <li> <p>Ensure your changes look and function as expected.</p> <p></p> </li> </ol>"},{"location":"contributing/contributor_guide/#make-a-pull-request","title":"Make a pull request","text":"<ol> <li>Push your local working branch to your GitHub remote repository.</li> <li>Navigate to the upstream repository at https://github.com/uabrc/uabrc.github.io.</li> <li> <p>Click the \"Pull requests\" tab and click the \"New pull request\" button.</p> <p></p> </li> <li> <p>Click the link \"compare across forks\".</p> <p></p> </li> <li> <p>There are four drop-down menus.</p> <ol> <li>The left two drop-down menus are for the base repository and should say <code>uabrc/uabrc.github.io</code> and <code>main</code> by default. Be sure that they do.</li> <li>In the third drop-down menu, select your fork.</li> <li>In the fourth drop-down menu, select your working branch.</li> </ol> <p></p> </li> <li> <p>Click the \"Create pull request\" button to open the pull request creation form.</p> <ol> <li>Give your pull request a concise and informative name. The name should describe what the pull request changes at a high level.</li> <li>In the description box, give details about what was changed at a conceptual level. The actual details of the changes can be viewed in the \"Commits\" and \"Files changed\" tabs.</li> <li>If you want reviewers to be able to make changes to your pull request (recommended) then leave the \"Allow edits\" checkbox checked.</li> </ol> <p></p> </li> </ol>"},{"location":"contributing/contributor_guide/#wait-for-review","title":"Wait for review","text":"<p>From here your pull request will go through a review process. The following criteria are checked.</p> <ol> <li>No linting errors</li> <li>Correct formatting</li> <li>Image alternate text (alt text)</li> <li>Images must use the gallery functionality, formatted as <code>![!alttext](path/to/file)</code>. Note the leading <code>!</code> in the alttext.</li> <li>Valid internal and external links.</li> <li>Quality, organization and accuracy of contribution.</li> </ol> <p>We will do our best to check information for accuracy, as well as proofread the text. Bear in mind Research Computing staff time is limited and we are not infallible, so please double-check your pull requests! Your audience is your research colleagues at UAB and beyond, and possibly even you at a future date!</p>"},{"location":"contributing/contributor_guide/#automation","title":"Automation","text":""},{"location":"contributing/contributor_guide/#cicd","title":"CI/CD","text":"<p>CI/CD is used to ensure consistency and formatting of markdown files via linting. Internal links are also checked for validity.</p> <ul> <li>Linting: <code>markdownlint</code> runs via markdownlint-cli2-action.</li> <li>Internal links are validated by the <code>mkdocs build</code> validation configuration.</li> </ul>"},{"location":"contributing/contributor_guide/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>We use pre-commit hooks to ensure contributions match our standards for consistency, formatting, and URL validity prior to pull requests.</p> <ul> <li>Pre-commit runs markdownlint-cli2 on markdown files.</li> <li>Pre-commit runs <code>mkdocs build</code> to validate links.<ul> <li>Internal links are validated by the <code>mkdocs build</code> validation configuration.</li> </ul> </li> </ul> <p>The <code>mkdocs-htmlproofer-plugin</code> can take substantial time to run. To disable it modify the file <code>.htmlproofer.env</code> to read <code>ENABLED_HTMLPROOFER=False</code>. Be sure not to commit this change! It is recommended to discard this change and re-run validaitions before submitting a pull request.</p> <p>To use the pre-commit hooks, you must have <code>conda</code> on your system <code>PATH</code> and have installed the bundled <code>build_env.yml</code> environment.</p>"},{"location":"contributing/contributor_guide/#manual-validation","title":"Manual Validation","text":"<p>Use the command <code>pre-commit run --all-files &gt; pre-commit-out.log 2&gt;&amp;1</code> to run all pre-commit hooks and store the results in the file <code>out.log</code> for simpler review within VSCode.</p>"},{"location":"contributing/contributor_guide/#manual-markdown-linting","title":"Manual Markdown Linting","text":"<p>We are using markdownlint-cli2 for markdown linting.</p> <ol> <li>Install <code>build-env.yml</code> and activate</li> <li>Run <code>markdownlint-cli2 \"**/*.md\" \"#node_modules\" 2&gt; markdownlint-cli2-out.log</code></li> <li>Review <code>markdownlint-cli2-out.log</code></li> </ol> <p>We use <code>.markdownlint.json</code> to handle markdown formatting rules, rather than placing it in <code>.markdownlint.json</code>.</p>"},{"location":"contributing/contributor_guide/#manual-documentation-build","title":"Manual Documentation Build","text":"<ol> <li>Install <code>build-env.yml</code> and activate</li> <li>Run <code>mkdocs build --strict &gt; mkdocs-build-out.log 2&gt;&amp;1</code></li> <li>Review <code>mkdocs-build-out.log</code></li> </ol>"},{"location":"contributing/contributor_guide/#external-url-validation","title":"External URL Validation","text":"<p>We are using linkchecker to validate external repository URLs.</p> <ol> <li>Install <code>build-env.yml</code> and activate</li> <li>Run <code>python scripts/linkchecker.py</code></li> <li>Review <code>out/linkchecker-out.csv</code> (feel free to ignore <code>out/linkchecker.log</code> unless you want verbose details)</li> </ol> <p>The <code>urlname</code> column contains the URL as it is written in the documentation. The <code>url</code> column contains the resulting URL after all forwarding is complete.</p>"},{"location":"contributing/contributor_guide/#linting-known-issues","title":"Linting Known Issues","text":"<p>There are known issues with the markdown linter and some of our non-standard plugins, especially admonitions (specifically a conflict involving fenced vs indented code blocks). To fix these cases please use one of the following methods. The <code>$MD_LINTER_CODE</code> can be found by hovering over the yellow squiggles in VSCode to bring up the warning lens.</p> <p>Please do not use these to silence all linter warnings, only for fixing known issues. Please read the warning lenses given by VSCode to identify the cause of the warning.</p>"},{"location":"contributing/contributor_guide/#silence-linter-warning-for-a-block","title":"Silence Linter Warning for a Block","text":"<pre><code>&lt;!-- markdownlint-disable $MD_LINTER_CODE --&gt;\n`linter error here`\n\n`maybe multiple lines`\n&lt;!-- markdownlint-enable $MD_LINTER_CODE --&gt;\n</code></pre>"},{"location":"contributing/contributor_guide/#silence-linter-warning-for-a-single-line","title":"Silence Linter Warning for a Single Line","text":"<p>We encourage denoting the warning being silenced here by filling out the <code>$MD_LINTER_CODE</code>, though it isn't required for the single line case.</p> <pre><code>&lt;!-- markdownlint-disable-next-line $MD_LINTER_CODE --&gt;\n`linter error here just for this line`\n</code></pre>"},{"location":"contributing/contributor_guide/#false-positive-lint-warnings-from-admonitions","title":"False Positive Lint Warnings from Admonitions","text":"<p>We allow and encourage the use of admonitions in our documentation, where appropriate. Because these are created using a plugin and are \"non-standard\" <code>markdown</code>, the VSCode <code>markdownlint</code> extension does not recognize admonitions and may produce a false positive warning about inconsistent code block styles.</p> <p>Two styles of code block are allowed in <code>markdown</code>: <code>fenced</code> and <code>indented</code>. To work around the false positive warning about admonitions, we require all code blocks to be <code>fenced</code>. All admonitions are assigned the warning <code>MD046</code>, which can be disabled by placing all admonitions in between the following comment block fences. The comment lines must be indented to the same level as the start of the admonition.</p> <pre><code>&lt;!-- markdownlint-disable MD046 --&gt;\n\n&lt;!-- markdownlint-enable MD046 --&gt;\n</code></pre> <p>The process can be simplified in VSCode using the <code>md046 disable</code> snippet, included in this repository at <code>.vscode/markdown.code-snippets</code>. When used, the snippet will automatically surround selected text with the appropriate fencing to disable markdownlint MD046. To use the snippet, select all of the lines belonging to the admonition, open the Command Palette (Ctrl+Shift+P), select \"Snippets: Insert Snippet\", then search for <code>md046 disable</code> and select it.</p> <p>This workaround is needed because <code>markdownlint</code> has no plans to add support for admonitions. There is no <code>markdownlint</code> plugin for that support either, and we don't have the ability to develop such a plugin.</p>"},{"location":"contributing/contributor_guide/#file-organization","title":"File Organization","text":"<ul> <li>Main headings are based on UAB Research Computing services</li> <li>Favor placing new pages and information into an existing section over creating</li> <li>Approach documentation from a problem solving angle rather than a technology. Examples:<ul> <li>Section title \"Installing Software Yourself with Anaconda\" vs \"Anaconda\"</li> <li>Section title \"Running Analysis Jobs\" vs \"Slurm\"</li> </ul> </li> <li>Put redirects for any page moves in case someone has bookmarked a page (see Redirect section below)</li> </ul>"},{"location":"contributing/contributor_guide/#redirects","title":"Redirects","text":"<p>If a page name must change, or the location of a page must change, it is necessary to create a redirect for that page so we don't break bookmarks and incoming links targeting our documentation.</p> <p>Redirecting pages is possible using the plugin at https://github.com/datarobot/mkdocs-redirects. To redirect a page, add a line in <code>mkdocs.yml</code> under the following keys. The line takes the form <code>original page location: new page location</code>, and each side of <code>:</code> must be a full path under <code>docs/</code>. An example is below.</p> <pre><code>plugins:\n  - redirects:\n      redirect_maps:\n        account_management/uab_researcher.md: account_management/cheaha_account.md\n</code></pre> <p>Maintain the list in alphabetic order.</p>"},{"location":"contributing/contributor_guide/#section-index-pages","title":"Section Index Pages","text":"<p>To create a section index page:</p> <ol> <li>Create a file called <code>index.md</code> in a reasonable directory location for that section.</li> <li> <p>Add something like the following to that section entry under <code>nav:</code> in <code>mkdocs.yml</code>.</p> <pre><code>nav:\n  ...\n  - Cheaha Guide:\n    - cheaha/index.md  # add the index.md here.\n    - Hardware: ...\n    - ...\n  ...\n</code></pre> </li> <li> <p>When a site visitor clicks <code>Cheaha Guide</code> in the nav pane, the page <code>cheaha/index.md</code> will be loaded.</p> </li> </ol>"},{"location":"contributing/contributor_guide/#accessibility-tools","title":"Accessibility Tools","text":"<ul> <li>Color vision deficiency checker: https://www.toptal.com/designers/colorfilter/</li> <li>Contrast checker: https://webaim.org/resources/contrastchecker/</li> </ul>"},{"location":"contributing/contributor_guide/#uab-branding-guidance","title":"UAB Branding Guidance","text":"<ul> <li>Brand main page: https://www.uab.edu/toolkit/branding</li> <li>Brand colors: https://www.uab.edu/toolkit/brand-basics/colors</li> <li>Copyright guidance: https://www.uab.edu/toolkit/trademarks-licensing/uab-trademarks</li> </ul>"},{"location":"contributing/contributor_guide/#reviewer-guidance","title":"Reviewer Guidance","text":"<p>Note</p> <p>Currently only RC Data Science staff have permissions to review pull requests.</p> <p>Reviewing a pull request means obtaining a copy of the pull request branch and Verifying the Changes on your local machine or on your fork. GitHub provides a facility for obtaining pull request branches directly from the upstream repository.</p>"},{"location":"contributing/contributor_guide/#add-upstream-remote","title":"Add upstream remote","text":"<p>Add the Upstream Remote using <code>git remote add upstream https://github.com/uabrc/uabrc.github.io.git</code>.</p>"},{"location":"contributing/contributor_guide/#pull-the-pull-request","title":"Pull the pull request","text":"<ol> <li> <p>Fetch the pull request with <code>git fetch upstream pull/&lt;id&gt;/head:&lt;branch-name&gt;</code>.</p> <p></p> <ol> <li>Replace <code>&lt;id&gt;</code> with the pull request id number.</li> <li>Replace <code>&lt;branch-name&gt;</code> with the branch name from the pull request source.</li> </ol> </li> <li> <p>Checkout the branch using <code>git checkout &lt;branch-name&gt;</code>.</p> </li> <li>Follow the instructions for Verifying Changes</li> <li>(Optional) make modifications to the pull request.<ol> <li>Before starting, make sure that the pull request author has allowed edits to their branch.</li> <li>Add the Author's Fork as a Remote.</li> <li>Push changes to the Author's Fork. Be sure to push to the correct remote!</li> </ol> </li> </ol>"},{"location":"contributing/contributor_guide/#internal-developer-notes","title":"Internal Developer Notes","text":""},{"location":"contributing/contributor_guide/#slurm-hardware-partitions-qos-tables","title":"Slurm Hardware, Partitions, QoS Tables","text":"<p>Building hardware tables is a semi-automated script based on a manually curated table. The repository is located here: https://gitlab.rc.uab.edu/rc-data-science/metrics/rc-hardware. The repository is only accessible to developers at this time.</p> <p>Building Partition and QoS tables is automated based on <code>scontrol</code> output. The repository is located here: https://github.com/wwarriner/slurm_status_tools. To use, install the conda environment at the linked repo, activate it, and run the following commands.</p> <pre><code>python -u sstatus.py -c partitions &gt; partitions.csv\npython -u sstatus.py -c qos &gt; qos.csv\n</code></pre>"},{"location":"contributing/contributor_guide/#terminology","title":"Terminology","text":"<ul> <li><code>Research Computing (RC)</code> for the IT group supporting campus HPC resources.</li> <li><code>UAB Campus Network</code> for the hard-wired network physically located on UAB campus, and the UAB WiFi Network (\"UABSecure\").</li> <li><code>UAB Campus VPN</code> for the VPN to tunnel remote connections through the UAB Campus Network.</li> </ul>"},{"location":"contributing/reporting_errors/","title":"Reporting Documentation Errors","text":"<p>Important</p> <p>This page is only for reporting errors with the documentation. For issues related to our computing services, please contact Support.</p> <p>Our documentation is hosted on GitHub at https://github.com/uabrc/uabrc.github.io. To report errors, make requests, and contribute to the documentation, you'll need to Create a GitHub Account if you do not have one already.</p>"},{"location":"contributing/reporting_errors/#how-do-i-report-inaccurate-information","title":"How Do I Report Inaccurate Information?","text":"<p>To report inaccurate information please create an Inaccuracy Report.</p>"},{"location":"contributing/reporting_errors/#how-do-i-report-a-bug","title":"How Do I Report a Bug?","text":"<p>Bug reports are intended to be issues with page appearance and feature functionality. They are not intended for inaccurate information. See [Report Inaccurate Information] to report inaccuracies.</p> <p>To report a bug please create a Bug Report.</p>"},{"location":"contributing/reporting_errors/#how-do-i-request-an-article-or-section","title":"How Do I Request an Article or Section?","text":"<p>To request an article or section, please create an Article Request.</p>"},{"location":"contributing/reporting_errors/#how-can-i-fix-it-myself","title":"How Can I Fix It Myself?","text":"<p>To get started creating content, please see our Contributor Guide. We assume you are already somewhat familiar with Git, GitHub, Markdown, and writing technical information for an educated audience with diverse backgrounds and expertise.</p>"},{"location":"data_management/alternate_storage/","title":"Alternative Storage Options","text":"<p>UAB Offers alternative storage options to faculty, staff, and students. You can find all of those details here.</p> <p>Please note, these services are maintained by UAB IT, and Ask IT manages usage and access to these services.</p>"},{"location":"data_management/alternate_storage/#box-storage","title":"Box Storage","text":"<p>To find information on Box, and how to log in to your Box account, please find those details here..</p>"},{"location":"data_management/alternate_storage/#finding-your-box-storage-total","title":"Finding your Box Storage Total","text":"<p>The following steps will guide you on how to access your Box Storage total, how much Box storage you have used, and how much storage you have left.</p> <ol> <li> <p>When you are logged into Box, click on the Profile/Account Icon. This is usually in the top right corner of the Box landing page, and should be your intials in the format \"FL\" (F = First Name; L = Last Name).</p> </li> <li> <p>On the drop down menu select \"Account Settings\". A tab named \"Account\" should now be visible.</p> <p></p> </li> <li> <p>Scroll down the tab Account, to a section named \"Account Details\". Your Box storage information would now be visible.</p> <p></p> </li> </ol>"},{"location":"data_management/code_storage/","title":"Code Storage","text":"<p>Unlike with traditional raw data storage, code evolves over time, often rapidly. Rapid changes are inevitable and commonplace in academia. Scientific research demands reproducibility and accountability, especially with methodology. Code you write is part of your methodology, so it should be carefully tracked and documented as it evolves, the same way you keep lab notebooks and document how you performed experiments. Git is the foundational tool on which to build code reproducibility and accountability. GitHub and GitLab provide central, internet-based locations in which to store your code.</p>"},{"location":"data_management/code_storage/#uab-gitlab-vs-the-third-party-github-which-should-i-choose","title":"UAB GitLab vs the third-party GitHub: which should I choose?","text":"<p>If your work is private, or internal to UAB operations, it is probably best to store it in our GitLab instance, hosted on-premises as UAB. In contrast, if you must collaborate with external researchers, it might make more sense to store code in GitHub, instead. GitLab has more feature-rich project management tooling in support of code collaboration, but GitHub is more widely known, recognized and can more readily travel with you as you progress in your career.</p>"},{"location":"data_management/code_storage/#gitlab","title":"GitLab","text":"<p>UAB Research Computing maintains a GitLab instance. To gain access, please visit https://gitlab.rc.uab.edu and create an Account.</p>"},{"location":"data_management/code_storage/#github","title":"GitHub","text":"<p>Please visit https://github.com.</p> <p>For UAB Research Computing's GitHub repositories, please see our Social Media page.</p>"},{"location":"data_management/code_storage/#how-do-i-effectively-use-git-github-and-gitlab-for-collaboration","title":"How do I effectively use Git, GitHub, and GitLab for collaboration?","text":"<p>Please see our page on Collaborating with Git.</p>"},{"location":"data_management/storage/","title":"Storage","text":"<p>Research Computing offers several data storage options to meet individual or shared needs of UAB researchers, depending on their requirement and use-cases. The types of storage available, procedures for requesting access, responsibilities, and usage guidelines are detailed in the following sections.</p>"},{"location":"data_management/storage/#what-type-of-storage-do-i-need","title":"What Type of Storage Do I Need?","text":"<p>There are multiple locations for data storage both on and off Cheaha each with a specific purpose. You can look at the table below to help determine the storage platform we provide that best matches your needed use-case. If you need additional assistance, please contact Support.</p> Platform Long-term Storage User Data and Home Directories Project Directories User Scratch Local Scratch Data Use-Case Data that rarely or never changes. Static data hosting. Acquired data pick-up repository. General-purpose data storage. Data to be shared among collaborator group and ready for analysis. Temporary files created during analysis. Rolling file deletion over time. Small files created during jobs. Must be deleted at the end of a job. Default Quota Individual: 5 TBGroup: 75 TB 5 TB 25 TB 100 TB Small Owner Individual: per researcherGroup: Lab PI or Core Director Individual Researchers Lab PI or Core Director Individual Researchers Individual Researchers Accessible From - Cheaha- Globus - Cheaha- Globus - Cheaha- Globus Cheaha Cheaha Cheaha Path No path; see LTS Interfaces. <code>/home/&lt;BlazerID&gt;</code> (<code>$HOME</code>) and <code>/data/user/&lt;BlazerID&gt;</code> (<code>$USER_DATA</code>) <code>/data/project/&lt;name&gt;</code> <code>/scratch/&lt;BlazerID&gt;</code> (<code>$USER_SCRATCH</code>) <code>/local/$SLURM_JOB_ID</code> Read/Write (IO) Speed Slower Fast Fast Fast Fastest Responsibilities &amp; Procedures PI/Director responsible for data and access control. PI/Director responsible for data and access control. Data deleted after 30 days. Data deleted as needed. Access Control Bucket policies Self only <code>chmod</code> and ACLs Self only Self only How to Request Upon request. Comes with Cheaha account. Upon request. Comes with Cheaha account. Comes with Cheaha account."},{"location":"data_management/storage/#what-individual-storage-solutions-are-available","title":"What Individual Storage Solutions are Available?","text":"<p>Every Cheaha user has personal directories found at <code>/home/$USER</code> (or <code>$HOME</code>) and <code>/data/user/$USER</code> (or <code>$USER_DATA</code>), which are created automatically during account registration. In addition, individual allocations on Long-Term Storage (LTS) are also available upon request. Please read more about Long-Term Storage and User Data and Home Directories.</p>"},{"location":"data_management/storage/#how-do-i-request-individual-long-term-storage","title":"How Do I Request Individual Long-Term Storage?","text":"<p>To request individual Long-Term Storage, please first read and understand how Long-Term Storage differs from traditional file systems, like GPFS on Cheaha. Decide if it is suitable for your needs. Then please feel free to contact Support.</p>"},{"location":"data_management/storage/#what-shared-storage-solutions-are-available","title":"What Shared Storage Solutions are Available?","text":"<p>Shared Storage is available via two services. We have Project Storage (located in <code>/data/project</code> or Cheaha) and Long-Term Storage (LTS). The two offerings are suited to different sets of use-cases and are available upon request, so please read on to determine which may be most suitable.</p> <p>Project Storage is best-suited for changing or dynamic data. Specifically::</p> <ul> <li>Data needing/undergoing analysis</li> <li>Exploratory data</li> <li>Temporary data needed longer than 30 days</li> </ul> <p>In contrast, Long-Term Storage is best-suited for unchanging or static data. Specifically:</p> <ul> <li>Instrument-acquired data</li> <li>Completed analyses</li> <li>Hosting data for others to copy</li> <li>Hosting data for the public internet</li> <li>\"Pick-up\" and \"drop-off\" locations for data as part of a workflow</li> </ul> <p>Shared Storage is available for labs headed by a PI and for Core facilities headed by a director.</p> <p>Shared Storage is allocated on a per-organization basis, not on a per-person basis. If an individual researcher manages both a lab and a Core, they may request independent storage allocations for each organization. Each organization may request both Project Storage and Long-Term Storage.</p>"},{"location":"data_management/storage/#how-do-i-request-shared-storage","title":"How Do I Request Shared Storage?","text":"<p>To request shared Project Storage or Long-Term Storage, please contact Support. To ensure prompt allocation of Shared Storage, please follow the guidelines below:</p> <ul> <li>Requests must be made to support@listserv.uab.edu or via the AskIT HelpDesk.</li> <li>Requests must come from one of the proposed owners (a Lab PI, a Research Core director, or both).</li> <li>The role of Lab PI entitles a person to a project space for that lab.</li> <li>The role of Research Core director entitles a person to a project space for that core. If one person has both roles, they may have two shared Storage spaces, one for each role.</li> <li>All proposed owners must have created their Research Computing accounts at the time of the request.</li> </ul> <p>Please provide the following information. Missing information can delay allocation of Shared Storage as we either look up the information, or ask followup questions.</p> <ul> <li>Responsible Party/Owner: The BlazerID of the person claiming responsibility for what happens and what is stored in the space. Typically this would be a Principal Investigator (PI) or a Core Director.<ul> <li>Multiple responsible parties are allowed.</li> <li>We need one person declared as \"primary\" owner. This person will be the literal owner (in the Linux sense) for Project Storage.</li> </ul> </li> <li>Members: A list of BlazerIDs of people to give access to the space. (Note: this only applies to Project Storage. LTS access controls are managed differently.)</li> <li>Type of Organization: Is the Shared Storage request for a lab, core, campus administrative group, or something else?</li> <li>Name of Organization: The specific name of the organization the Shared Storage request is for.</li> <li>Parent Organization: The name of the parent organization for your organization. Please be as detailed as possible.</li> <li>Purpose of Shared Storage: The research purpose for the storage, how do you intend to use it? Please feel free to be as detailed as you like, but please limit to a few sentences at most.</li> <li>Internal UAB Collaborator Organizations: The name(s) of any other UAB organizations participating in the Shared Storage.</li> <li>External Collaborator Organizations: The name(s) of any external organizations participating in the Shared Storage.</li> <li>Regulatory Requirements: List any regulatory requirements or agencies affecting data to be stored in the space. Possibilities include, but are not limited to: IRB, EHR, HIPAA, PHI, FERPA.</li> <li> <p>Name of Shared Storage: Please give us a generic name specific to your project/Lab.</p> <ul> <li>For Labs, we recommend using the format <code>&lt;PI_BlazerID&gt;_lab</code>, where <code>&lt;PI_BlazerID&gt;</code> is the BlazerID or name of the Principal Investigator (PI). For example: <code>PI_BlazerID_lab</code>, <code>PI_name_lab</code>.</li> <li>For Cores, we recommend using a shortened version of the Core name. For example: <code>core_facility_space</code></li> <li>For Project Storage, the name you choose will be used in the path <code>/data/project/&lt;PI_BlazerID&gt;_lab</code> on Cheaha. Also, this name,<code>&lt;PI_BlazerID&gt;_lab</code>, will be given to your shared LTS account.</li> </ul> <p>Tip</p> <ul> <li>Keep the name short, memorable, and relevant.</li> <li>Use <code>underscores (_)</code> or <code>hyphen (-)</code> to separate words.</li> <li>To serve future projects, consider names that are generic.</li> </ul> </li> </ul> <p>If some members have not created their accounts at the time of the request, we will proceed with allocating the Shared Storage. Additional members may be added at a later time in a new service request.</p>"},{"location":"data_management/storage/#how-do-i-make-changes-to-shared-storage-membership","title":"How Do I Make Changes to Shared Storage Membership?","text":"<p>To request changes in Shared Storage membership, please contact Support. Please take note of the following guidelines to ensure changes can be made promptly.</p> <ul> <li>We must have written approval from an owner to make membership changes.</li> <li>The exact name of the Shared Storage. If it is Project Storage, the path to the storage location, i.e., <code>/data/project/...</code>.</li> <li>Please give BlazerIDs of members to add or remove.</li> </ul>"},{"location":"data_management/storage/#how-can-i-get-a-larger-dataproject-gpfs-allocation","title":"How Can I Get A Larger <code>/data/project/</code> (GPFS) Allocation?","text":"<p>At this time, due to constraints on total GPFS storage, we are not able to increase <code>/data/project/</code> allocations. Please consider batching your analyses by leveraging a combination of LTS to store raw and/or input data, and User Scratch for temporary storage of up to 100 TB of data for use during analysis.</p> <p>If you wish to have further discussion of options for expanding your GPFS allocation and other workarounds tailored to your workflow, please Contact Support. Please also note that project storage is not just for a single project only, it is meant as a storage for multiple projects.</p>"},{"location":"data_management/storage/#how-can-i-get-a-larger-lts-lab-allocation","title":"How Can I Get A Larger LTS Lab Allocation?","text":"<p>At this time, due to constraints on total LTS storage, increasing an LTS allocation requires purchasing additional hardware. Below are some facts about purchasing additional storage nodes.</p> <ul> <li>Allocation increases occur by purchasing whole storage nodes.</li> <li>Each node has 133 TB of usable storage.</li> <li>Nodes are purchased with researcher funds at vendor cost.</li> <li>No markups are added to the cost of nodes.</li> <li>Purchased nodes are racked with existing hardware in our data centers.</li> <li>Purchased nodes are maintained by Research Computing with the same level of service as other hardware.</li> <li>Purchased nodes are supported for 5 years from date of purchase, the industry standard for commercial datacenter hardware.</li> <li>Once an order is placed with the vendor, we can provide additional storage immediately if free storage is available, regardless of lead-time.</li> </ul> <p>If you have additional questions or wish to discuss further, please Contact Support.</p>"},{"location":"data_management/storage/#if-i-cant-get-a-larger-allocation-what-alternatives-are-there","title":"If I Can't Get a Larger Allocation, What Alternatives Are There?","text":"<p>One alternative we recommend is breaking your dataset into batches. A generic, template workflow might be something like below.</p> <ul> <li>Copy a batch of data from LTS, or an internet source, to User Scratch.</li> <li>Perform analyses on copied data in User Scratch.</li> <li>Store intermediate or final results in <code>/data/project/</code> or LTS.</li> <li>Delete copied data from User Scratch.</li> <li>Start again with the next batch.</li> </ul> <p>When all batches have been processed, begin processing or aggregating the resulting data.</p> <p>If you wish to discuss other alternatives tailored to your workflow, please Contact Support.</p>"},{"location":"data_management/storage/#user-data-and-home-directories","title":"User Data and Home Directories","text":"<p>Every user of Cheaha are given a storage space to store general data and data that can be used during active analysis. While there are no data retention policies in place, these spaces are not intended for long-term storage of data that changes infrequently. Traditionally, <code>$HOME</code> is intended to store scripts, supporting files, software configuration files, and toolboxes such as Anaconda virtual environments or R packages. In contrast, <code>$USER_DATA</code> is intended to store datasets and results for individual research projects, with access granted only to the user of that directory. Since the quotas for these directories are limited to 5TB, you may consider using scratch space and/or project directories for storing, moving, and analyzing data.</p>"},{"location":"data_management/storage/#project-directory","title":"Project Directory","text":"<p>The Project Directories are larger than home directories and serves as a storage solution accessible to Labs led by a PI and Core facilities led by a director. It is intended for sharing data and code within a group of researchers or among lab members and collaborators, located under <code>/data/project/&lt;project&gt;</code>.</p> <p>The PI is the owner of the project directory, and when a directory <code>/data/project/&lt;project&gt;</code> is created, researchers permitted to collaborate on the project are added as members of this group, granting them access to the project directory. New members can be added or removed from the group upon PI approval. Currently, a project directory space is 25 TB, and this space is not designated for a single project only; it serves as storage for multiple projects.</p>"},{"location":"data_management/storage/#project-directory-permissions","title":"Project Directory Permissions","text":"<p>Every project directory has a group that is unique system-wide, and not used anywhere else on the filesystem. The unique project group will be referred to as <code>&lt;grp&gt;</code> and generally has the same name as the top level project directory.</p> <p>Note</p> <p>Some early group names may not match their project directory, but should be reasonably close.</p> <p>Members of the project directory group have permissions to access that project directory. Adding and removing members from the project directory group is how Research Computing controls access to, and ownership of, project directories. We do not use access control lists (ACLs) to manage permissions ourselves, but use of ACLs is allowed and encouraged for PIs and project administrators who want more fine-grained control. Please see our section on ACLs for more information.</p> <p>Be default, project space permissions are set up in the following way:</p> Top level directory Newly created files Newly created directories Numeric Permissions <code>2770</code> <code>0664</code> <code>2775</code> Symbolic Permissions <code>drwxrws---</code> <code>-rw-rw-r--</code> <code>drwxrwsr-x</code> <code>setgid</code> enabled Yes Yes User owner PI/Admin Creator Creator Group owner <code>&lt;grp&gt;</code> <code>&lt;grp&gt;</code> <code>&lt;grp&gt;</code> <p>Having <code>setgid</code> enabled on directories means new files and directories created within will inherit group ownership and the <code>setgid</code> bit. The <code>setgid</code> bit is reflected by the <code>2</code> in the numeric permissions and the <code>s</code> in the symbolic permissions. The <code>setgid</code> bit and per-directory project groups is how Research Computing controls access to each project directory.</p> <p>There are some known issues surrounding project directory permissions when files are put into the project directory. Different commands have different behaviors. The following list describes the behaviors of various commands used to move and copy data, as well as good practices.</p> <ul> <li><code>mv</code> maintains all permissions and ownerships of the source file or directory.<ul> <li>For files and directories created outside the project directory, avoid using <code>mv</code>, prefer <code>cp</code> or similar instead. See below for alternatives.</li> <li>For files and directories created within the project directory, <code>mv</code> may work, but be sure the file has correct permissions and group ownership.</li> </ul> </li> <li><code>cp</code>, <code>tar -x</code>, <code>rsync</code>, <code>rclone</code>, <code>sftp</code> and Globus all behave as though creating a new file at the target location, by default. Prefer these commands when it is sensible to do so.<ul> <li>Avoid using the <code>-p</code> flag with <code>cp</code>, <code>tar</code>, <code>rsync</code> and <code>sftp</code>.</li> <li>When using the <code>-p</code> flag, files and directories will retain their source permissions.</li> <li>Retaining source permissions in project directories is undesirable behavior and can create headaches for you, your colleagues, and your project directory administrators and PIs.</li> </ul> </li> </ul> <p>For PIs and project administrators:</p> <ul> <li>Please educate your staff and collaborators about the above permission setups, and any additional ACLs you may have in place, to minimize future challenges.</li> <li>If you have issues with permissions, please contact Support. We can guide you through Managing Permissions and Managing Group Ownership.</li> </ul>"},{"location":"data_management/storage/#scratch","title":"Scratch","text":"<p>Two types of scratch space are provided for analyses currently being ran, network-mounted and local. These are spaces shared across users (though one user still cannot access another user's files without permission) and as such, data should be moved out of scratch when the analysis is finished.</p> <p>Important</p> <p>Starting January 2023, scratch data will have limited retention. See Scratch Retention Policy for more information.</p>"},{"location":"data_management/storage/#user-scratch","title":"User Scratch","text":"<p>All users have access to a large, temporary, work-in-progress directory for storing data, called a scratch directory in <code>/scratch/$USER</code> or <code>$USER_SCRATCH</code>. Use this directory to store very large datasets or temporary pipeline intermediates for a short period of time while running your jobs. The maximum amount of data a single user can store in network scratch is 100 TB at once.</p> <p>Network scratch is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 1 PB of storage. If using scratch, this should be your jobs' primary working directory, unless the job would benefit from local scratch (see below).</p> <p>Warning</p> <p>Research Computing expects each user to keep their scratch areas clean. The cluster scratch areas are not to be used for archiving data. In order to keep scratch clear and usable for everyone, files older than 30 days will be automatically deleted.</p>"},{"location":"data_management/storage/#local-scratch","title":"Local Scratch","text":"<p>Each compute node has a local scratch directory that is accessible via <code>/local/$SLURM_JOB_ID</code>. At this time, you will need to create the directory manually using <code>mkdir -p /local/$SLURM_JOB_ID</code>. If your job performs a lot of file I/O, the job should use <code>/local/$SLURM_JOB_ID</code> rather than <code>$USER_SCRATCH</code> to prevent bogging down the network scratch file system. It's important to recognize that most jobs run on the cluster do not fall under this category.</p> <p>If you are using <code>amperenodes</code> and the A100 GPUs, then it is highly recommended to move your input files to <code>/local/$SLURM_JOB_ID</code> prior to running your workflow, to ensure adequate GPU performance. Using <code>$USER_SCRATCH</code>, or other network file locations, will starve the GPU of data, resulting in poor performance. For more information please see Ensuring IO Performance With A100 GPUs.</p> <p>Be sure to clean up <code>/local/$SLURM_JOB_ID</code> after your job is complete! An example script to automate this process is shown below.</p> <pre><code>#!/bin/bash\n#SBATCH ...\n\n# LOAD MODULES\n# module load ...\n\n# CREATE TEMPORARY DIRECTORY\n# WARNING! $TMPDIR will be deleted at the end of the script!\n# Changing the following line can cause permanent, unintended deletion of important data.\nTMPDIR=\"/local/$USER/$SLURM_JOB_ID\"\nmkdir -p \"$TMPDIR\"\n\n# COPY RESEARCH DATA TO LOCAL TEMPORARY DIRECTORY\n# Replace $MY_DATA_DIR with the path to your data folder\ncp -r \"$MY_DATA_DIR\" \"$TMPDIR\"\n\n# YOUR ORIGINAL WORKFLOW GOES HERE\n# be sure to load files from \"$TMPDIR\"!\n\n# CLEAN UP TEMPORARY DIRECTORY\n# WARNING!\n# Changing the following line can cause permanent, unintended deletion of important data.\nrm -rf \"$TMPDIR\"\n</code></pre> <p>Important</p> <p>Using <code>/local/$SLURM_JOB_ID</code> with MPI jobs takes additional consideration. If you do not need MPI, please use the <code>#SBATCH --nodes=1</code> slurm directive to specify that all requested cores are on the same node. If you need the performance of <code>/local/$SLURM_JOB_ID</code> in an MPI job, please contact Support and read about the Slurm commands <code>sbcast</code> and <code>sgather</code>.</p> <p>Note</p> <p>By default the variable <code>$TMPDIR</code> points to <code>/scratch/local/</code> which in turn is a symbolic link to <code>/local/</code>. The variable <code>$LOCAL_SCRATCH</code> is identical to <code>$TMPDIR</code>.</p> <p>We recommend overwriting <code>$TMPDIR</code>, as above, because it will ensure that jobs always write to a safe location on the same node the work is being done.</p>"},{"location":"data_management/storage/#temporary-files-tmp-directory","title":"Temporary Files (<code>/tmp/</code> directory)","text":"<p>Please do not use the directory <code>/tmp/</code> as storage for temporary files. The <code>/tmp/</code> directory is local to each node, and a full <code>/tmp/</code> directory harms compute performance on that node for all users. Instead, please use <code>/local/$SLURM_JOB_ID</code> for fast access and <code>$USER_SCRATCH</code> for larger space.</p> <p>Some software defaults to using <code>/tmp/</code> without any warning or documentation, especially software designed for personal computers. We may reach out to inform you if your software fills <code>/tmp/</code>, as it can harm performance on that compute node. If that happens we will work with you to identify ways of redirecting temporary storage to one of the scratch spaces.</p>"},{"location":"data_management/storage/#software-known-to-use-tmp","title":"Software Known to Use <code>/tmp/</code>","text":"<p>The following software are known to use <code>/tmp/</code> by default, and can be worked around by using the listed flags. See Local Scratch for more information about creating a local temporary directory.</p> <ul> <li>Java: <code>java * -Djava.io.tmpdir=/local/$SLURM_JOB_ID</code></li> <li>UMI Tools: <code>umi_tools * --temp-dir=/local/$SLURM_JOB_ID</code></li> <li>Samtools Sort: <code>samtools sort * -T /local/$SLURM_JOB_ID</code></li> <li>GATK Tool: <code>gatk --java-options * --tmp-dir /local/$SLURM_JOB_ID</code></li> <li>Parabricks: <code>pbrun * --tmp-dir=/local/$SLURM_JOB_ID</code></li> <li>FastQC: <code>fastqc * -d /local/$SLURM_JOB_ID</code></li> <li>MACS2: <code>macs2 callpeak * --tempdir /local/$SLURM_JOB_ID</code></li> </ul> <p>Software known to use <code>/tmp/</code> by default with no know workaround.</p> <ul> <li>Keras has <code>/tmp/.keras</code> hardcoded as a fallback cache directory if <code>~/.keras</code> is inaccessible. See here for a discussion of the issue.</li> </ul>"},{"location":"data_management/storage/#how-much-space-do-i-have-left","title":"How much space do I have left?","text":"<ul> <li>Individual Storage: use the command <code>quota-report</code> to see usage in <code>/data/user/$USER</code> and <code>/scratch/$USER</code>.</li> <li>Project Storage: use the command <code>proj-quota-report &lt;project&gt;</code>. Replace <code>&lt;project&gt;</code> with the appropriate project directory name, i.e., <code>/data/project/&lt;project&gt;</code>. Be sure to not use a trailing slash. Use <code>proj-quota-report mylab</code> not <code>proj-quota-report mylab/</code>.</li> <li>Long-Term Storage: please contact Support.</li> </ul> <p>Quota reports are updated nightly, so they may be out of date if you move data around before running these commands.</p> <p>Tip</p> <p>Running out of space? Can't afford to remove any data? Please consider using our Long Term Storage (LTS) system.</p>"},{"location":"data_management/storage/#data-responsibilities-and-procedures","title":"Data Responsibilities and Procedures","text":""},{"location":"data_management/storage/#archival","title":"Archival","text":"<p>Important</p> <p>Archival of data is the responsibility of researchers using Cheaha.</p> <p>At this time, Research Computing does not offer a method of archival. If you have need for archival, please feel free to contact Support to start a conversation.</p> <p>A possible external resource for archival is available through University of Oklahoma (OU) Supercomputing Center for Education and Research (OSCER). Please see the following link for details: https://www.ou.edu/oscer/resources/ourrstore--ou---regional-research-store.</p>"},{"location":"data_management/storage/#backups","title":"Backups","text":"<p>Important</p> <p>Backups of data are the responsibility of researchers using Cheaha.</p> <p>A good practice for backing up data is to use the 3-2-1 rule, as recommended by US-CERT:</p> <ul> <li>3: Keep 3 copies of important data. 1 primary copy for use, 2 backup copies.</li> <li>2: Store backup copies on 2 different media types to protect from media-specific hazards.</li> <li>1: Store 1 backup copy offsite, located geographically distant from the primary copy.</li> </ul> <p>What hazards can cause data loss?</p> <ul> <li>Accidental file deletion.<ul> <li>Example: mistakenly deleting the wrong files when using the shell command <code>rm</code>.</li> <li>Files deleted with <code>rm</code> or any similar command can not be recovered by us under any circumstances.</li> <li>Please restore from a backup.</li> </ul> </li> <li>Natural disasters.<ul> <li>Examples: tornado; hurricane.</li> <li>All of our data sits in one geographical location at the UAB Technology Innovation Center (TIC).</li> <li>Plans to add geographical data redundancy are being considered.</li> <li>Please restore from an offsite backup.</li> </ul> </li> <li>Unusable backups.<ul> <li>Examples: backup software bug; media destroyed; natural disaster at offsite location.</li> <li>Regularly test data restoration from all backups.</li> </ul> </li> </ul> <p>How can I ensure data integrity?</p> <ul> <li>Regularly back up your (and your lab's) data in an offsite location.</li> <li>S3 based long-term storage (LTS) can be used for short-term onsite backup.</li> <li>Crashplan licenses are available for automatic offsite backups, please contact Support for more information.</li> </ul>"},{"location":"data_management/storage/#hipaa-compliance","title":"HIPAA Compliance","text":"<p>Cheaha is HIPAA compliant and can accept Protected Health Information (PHI) data. Currently, long-term storage is NOT HIPAA compliant but will be in the future.</p> <p>For UAB policies surrounding PHI data, please see the following URLs.</p> <ul> <li>Data Classification</li> <li>Data Protection and Security Policy</li> <li>Data Access Policy</li> <li>HIPAA Data Policy</li> </ul> <p>Important</p> <p>It is the responsibility of researchers to make sure PHI is accessible only to people on the relevant IRB, with a demonstrated need to know. If PHI is stored in a project directory where some researchers are not on the IRB, their access to those files should be restricted using Access Control Lists (ACLs). Access control should be planned in advance of moving PHI data to Cheaha. If you need assistance setting up ACLs properly, please contact Support.</p> <p>Managing PHI data can be challenging. There are experts on Campus who can provide assistance. Please contact Support if you intend to use Research Computing services in combination with PHI and PHI-derived data.</p>"},{"location":"data_management/storage/#scratch-retention-policy","title":"Scratch Retention Policy","text":"<p>Data stored in <code>/scratch</code> is subject to two limited retention policies.</p> <ul> <li>Each user will have a quota of 50 TB of scratch storage.</li> <li>Files will be retained for a maximum of 30 days.</li> </ul>"},{"location":"data_management/lts/","title":"Long-Term Storage","text":"<p>UAB Long-term storage (LTS) is an S3 object-storage platform hosted at UAB. This storage is designed to hold data that is not currently being used in analysis but should be kept for data sharing, recapitulation purposes, or reused for further analysis in the future. This documentation covers multiple methods for accessing LTS in Windows, Mac, and Linux environments.</p> <p>Tip</p> <p>Globus may be used to transfer data with LTS.</p>"},{"location":"data_management/lts/#terminology","title":"Terminology","text":"<p>When talking about S3 storage, some terms are different compared to a normal filesystem. This section is here to briefly explain some differences in case you go to other documentation and see these terms instead.</p> <ul> <li><code>object</code>: any unit (i.e. file) stored in LTS. Typical folders and files do not exist in S3.</li> <li><code>bucket</code>: The root which objects are stored in. Each account can create a certain number of buckets and each bucket can be shared individually with other users. Bucket names are unique across the entire LTS platform (see the section on duplicate names)</li> <li><code>prefix</code>: Used in place of a file path to an object, and so can be used to represent an object's place in a typical filesystem. Stored as metadata in each object, prefixes are used for searches in a bucket.</li> <li><code>policy</code>: sets permissions for whole buckets and individual objects. Policies allow or deny access to buckets for individual accounts. These are controlled by the owner of the bucket.</li> <li><code>access key</code>: a unique identifier given to each user for access to LTS, similar to a username. A user's access key is preset and given to them after account setup.</li> <li><code>secret key</code>: a credential string similar to a password given to each user for access to LTS. The secret key is preset and given to the user after account setup</li> </ul> <p>Danger</p> <p>Never give access and secret keys for personal or lab accounts to anyone! Bad actors who are given keys to accounts which own important buckets can change access permissions and delete any and all data!</p> <p>If you need to give elevated permissions to other users to view, upload, download, delete, etc. any data from a bucket, those permissions can be changed via bucket policies without giving out keys. Please contact Research Computing for help setting up and applying policies if you need it</p> <p>Note</p> <p>If you lose your access and secret keys, please submit a support ticket to support@listserv.uab.edu to request your keys. Keys will only be given to an account owner as verified by RC staff.</p> <p>This documentation will use the standard file and path terms since those are more easily understood by most users. Just be aware that documentation such as AWS CLI will use terms prefix, object, and others that are not standard in a typical filesystem.</p>"},{"location":"data_management/lts/#requesting-an-account","title":"Requesting an Account","text":"<p>UAB researchers do not have automatic access to LTS, and currently, single sign on is not enabled. To request access to LTS, please send an email to support@listserv.uab.edu. You will be then be given an Access Key and a Secret Access Key, both of which will be used later on. Keep track of both of these keys and do not share them with anyone else, these are your login credentials for LTS. Please visit Shared Storage to review guidelines to request a LTS account.</p>"},{"location":"data_management/lts/#avoiding-duplicate-names-for-buckets","title":"Avoiding Duplicate Names for Buckets","text":"<p>Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage.</p> <p>Good practice when naming buckets is to use a short, descriptive and memorable name, then append a universally unique identifier (UUID) to the end. Websites like https://www.uuidgenerator.net/ may be used to generate and copy UUIDs. There are \\(5.3\\times 10^{36}\\) possible UUIDs, which means the chance of duplicating one is virtually zero. Please see Wikipedia for math supporting the low rate of duplication.</p>"},{"location":"data_management/lts/interfaces/","title":"Connecting to LTS","text":"<p>LTS is not available as a mounted filesystem on local computers or Cheaha. You must use an interface to transfer data between LTS and whichever machine you are using. There are a variety of interfaces with the following recommendations.</p>"},{"location":"data_management/lts/interfaces/#globus","title":"Globus","text":"<p>Globus is a general file transfer system that operates through a web browser and is recommended for most file transfer needs. UAB has an S3 connector for Globus that can transfer data to and from LTS as long as the user has access to the desired buckets.</p> <p>To connect to the LTS endpoint in Globus, search <code>UAB Research Computing LTS</code> in the search bar and enter your access and secret keys given to you by Research Computing staff. You will be able to see the buckets owned by the account associated with the keys you entered.</p> <p>Important</p> <p>If your LTS account was given permission to access a bucket owned by another account, it will not automatically appear in the Globus file browser. You can access buckets you have <code>s3:ListBucket</code> permissions on by typing <code>/&lt;bucket-name&gt;/</code> in the Path field under the LTS endpoint.</p> <p></p> <p>Globus is very useful for single transfers of data either to or from LTS and is available on any computer with an internet connection. However, it is currently not capable of managing buckets. This must be done through a command line interface.</p>"},{"location":"data_management/lts/interfaces/#managing-lts-credentials-on-globus","title":"Managing LTS Credentials on Globus","text":"<p>See our Globus - Adding LTS Allocation Credentials section for more information.</p>"},{"location":"data_management/lts/interfaces/#command-line","title":"Command Line","text":"<p>While globus is the recommended tool for most data transfers, command line tools are necessary for planned, regular transfers as well as managing permissions on buckets. We recommend the following two tools for different purposes:</p> <ol> <li>s3cmd is a Python tool that we suggest using for managing bucket permissions as well as small transfers.</li> <li>s5cmd is a Go package that transfers data much more quickly than s3cmd, especially as the file size and/or quanitity increases. It does not have full bucket management capabilities.</li> </ol>"},{"location":"data_management/lts/interfaces/#installation-of-s3cmd-and-s5cmd-on-cheaha","title":"Installation of <code>s3cmd</code> and <code>s5cmd</code> on Cheaha","text":"<p>To install the tools on Cheaha, you can request a compute node through Cheaha's Open OnDemand web portal.Once your job is launched, open a terminal to execute the commands listed below. You do not need to install both tools if they aren't necessary. Both are available to install into Anaconda environments. It's suggested to create a single environment named <code>s3</code> and install both s3cmd and s5cmd into it for easy access to both tools. Specific install and usage commands for each are given in their respective sections. You can create the general environment using the following commands:</p> <pre><code>module load Anaconda3\nconda create -n s3 -c conda-forge pip s5cmd\nconda activate s3\npip install s3cmd\n</code></pre> <p>Please note that the instructions mentioned above are specific to the Cheaha system. To transfer data between your personal computer and LTS, you will need to install <code>s3cmd</code> or <code>s5cmd</code> on your machine. Please refer to this section for installation instructions specific to your operating system.</p> <p>Note</p> <p>We manually install pip into the conda environment so that <code>pip</code> will install <code>s3cmd</code> into the conda environment as opposed to <code>$HOME/.local</code>. This way, you do not need to add the <code>.local</code> folder to your path whenever you want to use <code>s3cmd</code>.</p>"},{"location":"data_management/lts/interfaces/#s3cmd","title":"s3cmd","text":"<p>s3cmd is a tool used for managing buckets and objects in Amazon S3 (Simple Storage Service). s3cmd is our suggested tool for operations such as listing buckets, managing bucket permissions, synchronizing directories with s3 buckets, and for small periodic file transfers. If high-speed transfer of a large files is required, we recommend using s5cmd. See the preceding section for instructions on how to install both it and s5cmd into an Anaconda environment.</p>"},{"location":"data_management/lts/interfaces/#configuring-s3cmd","title":"Configuring s3cmd","text":"<p>Configuring s3cmd is necessary to establish a secure connection for accessing your LTS bucket in Amazon S3. Once you have s3cmd installed and the environment active, you can start the configuration process like so:</p> <pre><code>s3cmd --configure [-c $HOME/profile_name]\n</code></pre> <p>You can run the configuration either with or without the <code>[-c]</code> option. If you use it, a file named <code>profile_name</code> will be created in your home directory with your login credentials and other information. If you omit the <code>-c</code> option, a file called <code>$HOME/.s3cfg</code> will be created by default. This can be helpful if you have multiple S3 profiles you are using. If you use UAB LTS as your only S3 storage platform and are only managing a single account, it's suggested to omit the <code>-c</code> option. If you are a PI or data manager and are managing both a personal and lab/core LTS account, you will need to make a separate profile for each account.</p> <p>Note</p> <p>After configuration, the <code>s3cmd</code> command will default to using the <code>.s3cfg</code> file for credentials if it exists. If you create a separate named profile file, you will need to add that to the <code>s3cmd</code> call each time you run it.</p> <p>During configuration, you will be asked to enter some information. You can follow the example below, inputting your user-specific information where required. Lines requiring user input are highlighted.</p> <pre><code>Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: &lt;access key&gt;\nSecret Key: &lt;secret key&gt;\nDefault Region [US]: &lt;leave blank&gt;\n\nUse \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.\nS3 Endpoint [s3.amazonaws.com]: https://s3.lts.rc.uab.edu\n\nUse \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used if the target S3 system supports dns based buckets.\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).s3.lts.rc.uab.edu\n\nEncryption password is used to protect your files from reading by unauthorized persons while in transfer to S3\nEncryption password: &lt;leave blank or enter password&gt;\nPath to GPG program [/usr/bin/gpg]: &lt;leave blank&gt;\n\nWhen using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: &lt;leave blank&gt;\n\nOn some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly\nHTTP Proxy server name: &lt;leave blank&gt;\n\nNew settings:\n  Access Key: &lt;access key&gt;\n  Secret Key: &lt;secret key&gt;\n  Default Region: US\n  S3 Endpoint: https://s3.lts.rc.uab.edu\n  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket).s3.lts.rc.uab.edu\n  Encryption password:\n  Path to GPG program: $HOME/bin/gpg\n  Use HTTPS protocol: True\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n\nTest access with supplied credentials? [Y/n] n\n\nSave settings? [y/N] y\n</code></pre> <p>Important</p> <ol> <li>If you choose to test access using your credentials, the test may fail. Do not rely on the automatic test results, test access yourself by either creating a bucket or listing files from a existing bucket using the commands listed below.</li> <li>To locate the appropriate \"Path to GPG program\" for Ubuntu and Mac operating systems, please use the command <code>which gpg</code>. The location may vary depending on your operating system.</li> </ol>"},{"location":"data_management/lts/interfaces/#s3cmd-commands","title":"s3cmd Commands","text":"<pre><code># General command structure for s3cmd\ns3cmd [-c profile_file] &lt;command&gt; [options] [-n --dry-run]\n</code></pre> <p>The <code>[-c profile_file]</code> is only required if you are using credentials NOT saved in the <code>$HOME/.s3cfg</code> file. Otherwise, you can omit it.</p> <p>To see a list of all available commands, use <code>s3cmd --help</code>. Additionally, if you want to test an action without actually running it (i.e. it prints all actions that would be performed), you can add the <code>-n</code> or <code>--dry-run</code> option. A list of selected commands are provided below for reference</p> <pre><code># Create a bucket\ns3cmd mb s3://&lt;bucket&gt;\n\n# List a bucket/path within the bucket\ns3cmd ls [-r, --recursive] s3://&lt;bucket/path&gt;\n\n# Check bucket or folder size\ns3cmd du -H s3://&lt;bucket/path/&gt;\n\n# transfer a file or folder from local to a bucket\ns3cmd put &lt;source&gt; s3://&lt;bucket/path/destination/&gt;\n\n# transfer a file or folder from a bucket to a local drive\ns3cmd get s3://&lt;bucket/path/source/&gt; &lt;destination&gt;\n\n# transfer between two S3 locations\ns3cmd cp s3://&lt;bucket/path/&gt; s3://&lt;bucket/path/&gt;\n\n# sync an S3 location with a local source. The S3 destination will be made exactly the same as the source including file deletions.\n# The source is unaltered. The S3 bucket/folder can be either the source or the destination\ns3cmd sync &lt;source&gt; s3://&lt;bucket/path/destination&gt;\n\n# remove a single object or all objects within a given path\ns3cmd rm s3://&lt;bucket/path/file&gt; [--recursive]\n\n# remove an entire bucket\ns3cmd rb s3://&lt;bucket&gt;\n\n# get info about the bucket\ns3cmd info s3://&lt;bucket&gt;\n</code></pre> <p>Danger</p> <p>Be extremely cautious using <code>sync</code>. If there are files in the destination that are not in the source, it will delete those files in addition to adding files to the destination. If data is deleted from LTS, it is not recoverable.</p> <p>Note</p> <p>When using <code>ls</code> to list buckets, it will only show the buckets you own, not buckets you have been given permissions on. This is a limitation of the S3 system. You can still interact with any buckets you have been given relevant permissions on, but you will need to remember the names of the buckets you don't own.</p>"},{"location":"data_management/lts/interfaces/#s5cmd","title":"s5cmd","text":"<p>s5cmd is a parallel transfer tool suggested for period transfers of large and/or many files at a time. It has options for customizing how many processors are available for transferring data as well as how many chunks files can be broken into during transfer to minimize transfer time. See the preceding section for instructions on how to install both it and s3cmd into an Anaconda environment</p>"},{"location":"data_management/lts/interfaces/#configuring-s5cmd","title":"Configuring s5cmd","text":"<p>s5cmd does not use the same authentication file as s3cmd. Instead, it uses official AWS SDK to access S3 including LTS. The default credentials file for AWS CLI would found at <code>${HOME}/.aws/credentials</code>. This file is then populated with different profiles and their access and secret keys. You can create the necessary file with the following commands.</p> <pre><code>mkdir ${HOME}/.aws\ntouch ${HOME}/.aws/credentials\n</code></pre> <p>Open the credentials file with your favorite editor (i.e. <code>vim</code>, <code>nano</code>, <code>gedit</code>, etc.) and create a default profile by adding the following lines.</p> <pre><code>[default]\naws_access_key_id = &lt;access_key&gt;\naws_secret_access_key = &lt;secret_key&gt;\n</code></pre> <p>Note</p> <p>Do not include the <code>&lt;&gt;</code> symbols in the credentials file when saving your keys</p> <p>One of the benefits of this credential method is that multiple sets of credentials can be kept in the same file. For instance, if you have both a lab/core LTS account and a personal account, you could set your personal account as the default profile and then add your lab credentials under a named profile like so:</p> <pre><code>[default]\naws_access_key_id = &lt;personal_access_key&gt;\naws_secret_access_key = &lt;personal_secret_key&gt;\n\n[example-lab]\naws_access_key_id = &lt;lab_access_key&gt;\naws_secret_access_key = &lt;lab_secret_key&gt;\n</code></pre>"},{"location":"data_management/lts/interfaces/#s5cmd-commands","title":"s5cmd Commands","text":"<p>s5cmd has the following general form.</p> <pre><code>s5cmd --endpoint-url https://s3.lts.rc.uab.edu [global_options] command [command options] [arguments]\n</code></pre> <p>Here, global options must be kept separate from command specific options. For instance, the <code>--endpoint-url</code> option is a global option that specifies the URL for the S3 server. This must be included with every s5cmd command to communicate with UAB LTS, otherwise it will default to accessing AWS servers. Other global options include <code>--numworkers</code> and <code>--profile</code>, the number of available CPUs and which account to use in the <code>credentials</code> file, respectively. You can see a list of global options and the list of available commands by running <code>s5cmd --help</code>. A selection of commands are listed below.</p> <pre><code># copy all files from a local directory to a bucket using a single CPU\ns5cmd --endpoint-url https://s3.lts.rc.uab.edu cp /path/to/directory/* s3://bucket/\n\n# copy all files from a local directory to a bucket using 10 CPUs  and allowing the files to be broken into 5 parts during transfer\ns5cmd --endpoint-url https://s3.lts.rc.uab.edu --numworkers 10 cp --concurrency 5 /path/to/directory/* s3://bucket/\n\n# sync an S3 bucket (destination) to a local directory (source)\ns5cmd --endpoint-url https://s3.lts.rc.uab.edu sync /path/to/directory/ s3://bucket/\n\n# remove all objects with a given prefix from a bucket\ns5cmd --endpoint-url https://s3.lts.rc.uab.edu rm s3://bucket/prefix/*\n</code></pre> <p>As with s3cmd, be very careful using the <code>sync</code> and <code>rm</code> commands as these can/will delete files either locally or on LTS. There are many more commands s5cmd can use as well as a number of command options that can be used to customize how an operation is performed. Please see the help documentation for a full list.</p> <p>It's important to note that the main functionality of s5cmd over s3cmd is the parallelization options given by the <code>--numworkers</code> global option and the <code>--concurrency</code> local option for <code>cp</code> and <code>sync</code> commands. Choosing not to use these options will result in unoptimized performance.</p> <p>Important</p> <p>When setting the value for <code>--numworkers</code>, do not select a value beyond the number of CPUs you have requested for your job! This can cause high context switching (meaning individual CPUs are switching between multiple running processes) which can affect job performance for all jobs on a node.</p>"},{"location":"data_management/lts/interfaces/#installation-of-s3cmd-and-s5cmd-on-personal-systems-without-anaconda","title":"Installation of <code>s3cmd</code> and <code>s5cmd</code> on Personal Systems without Anaconda","text":"<p>The installation instructions and software dependencies may differ depending on the operating system being used. Following are the installation instructions tested for different operating systems. You may also use Anaconda to install either or both packages.</p>"},{"location":"data_management/lts/interfaces/#ubuntu","title":"Ubuntu","text":"<p>To install <code>s3cmd</code> please use the following commands.</p> <pre><code>sudo apt update\nsudp apt install s3cmd\n</code></pre> <p>To install <code>s5cmd</code>, you will have to first install <code>go</code> software version <code>&gt;=1.19</code>. Please refer to the Download page, and installation instruction for further details. After intalling <code>go</code>, you can build <code>s5cmd</code> in your <code>$HOME</code> directory using the below steps.</p> <pre><code>cd $HOME\ngo install github.com/peak/s5cmd/v2@master\n</code></pre> <p>The below steps are to add <code>go</code> bin directory to your system's <code>PATH</code> which allows you to run <code>s5cmd</code> from any location in your terminal.</p> <pre><code>echo 'export PATH=$PATH:$HOME/go/bin' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"data_management/lts/interfaces/#mac","title":"Mac","text":"<p>Use the following commands to install <code>s3cmd</code> and <code>s5cmd</code> on a Mac System.</p> <pre><code>brew install s3cmd\nbrew install peak/tap/s5cmd\n</code></pre> <p>You may need to install <code>gpg</code> on a Mac using the below command,</p> <pre><code>brew install gnupg\n</code></pre>"},{"location":"data_management/lts/interfaces/#windows","title":"Windows","text":"<p>To install <code>s3cmd</code> and <code>s5cmd</code> on a Windows system, you will first need to install Windows Subsystem for Linux (WSL). Once WSL is installed, you can use the command line instructions for Ubuntu to install <code>s3cmd</code> and <code>s5cmd</code>.</p> <p>For more information on <code>s3cmd</code> and <code>s5cmd</code>, please refer to the official s3tools Page, and s5cmd page.</p>"},{"location":"data_management/lts/interfaces/#alternatives","title":"Alternatives","text":"<p>There are other tools for interfacing with LTS such as rclone. Please see our rclone documentation for more details.</p>"},{"location":"data_management/lts/lts_cores/","title":"LTS Core Data Management Plans","text":"<p>UAB Core Facilities provide access to research instruments and services for scientific and clinical investigators. Cores can generate large amounts of data very quickly for many labs and so have some unique data management concerns. These concerns can be summarized as follows:</p> <ol> <li>Data transfer off local machines</li> <li>Data organization</li> <li>Data distribution</li> </ol> <p>UAB Cores can request a 75 TB allocation on LTS with possibilities for expansion to use for both storage and distribution of data. Together with Globus and CLI tools for transfer, LTS can be the basis for a full core data management plan. This documentation aims to provide details of general solutions for common problems with managing large amounts of data in a core.</p>"},{"location":"data_management/lts/lts_cores/#specifics-of-core-lts-accounts","title":"Specifics of Core LTS Accounts","text":"<p>Core LTS accounts behave the same as lab LTS accounts. The base allocation is 75 TB across 100 buckets with possibilities for expansion over time based on the needs of the core. The account will be a separate entity that owns its own buckets and data. Either the director data manager of the core will own the credentials for the core account and so will receive a set of access and secret keys separate from their personal and lab keys.</p> <p>Warning</p> <p>Do not share these keys with anyone. A person with core LTS keys have essentially admin access on all data stored in core buckets. This allows them to change who can access data as well as delete any and all data in the core LTS account.</p>"},{"location":"data_management/lts/lts_cores/#data-organization","title":"Data Organization","text":"<p>Effectively organizing an LTS space will dictate where data are transferred within it. The suggested storage structure can be summarized as follows:</p> <ol> <li>The core creates a bucket for each lab or group it services containing raw or analyzed data for that group. Permissions for each bucket are managed independently.<ol> <li>Due to all buckets in LTS needing to have unique names, buckets should be named more descriptively than just the name of the group to which the data belong. Instead, the format <code>[core_name]-[group_name]</code> will be both specific and unique enough to work in most circumstances. For example, data analyzed by the Research MRI Core (RMRIC) core for group <code>smithlab</code> would be put in a bucket named <code>rmric-smithlab</code>. Keep in mind the allowed characters when creating bucket names.</li> </ol> </li> <li>Within each bucket, data are generally organized by classification as raw or analyzed and then further organized by type of data or analysis. The exact form this takes is left up to the core itself. This style of organization lends itself to both structural clarity and automation of data transfer.</li> </ol> <p>A simplified diagram illustrating this organization can be seen below.</p> <p></p> <p>Due to data sensitivity, we advise against storing data from different groups in the same bucket. Managing permissions for a bucket where many groups can only access specific portions of a bucket is cumbersome and prone to error. Instead, separate policy files should be kept for each bucket specifying which researchers from each lab or group should be able to access the stored data. These policy files can be kept in a single location owned by the core for future modification. This also facilitates future transfer of bucket ownership from the core to the lab or group to whom the data belongs after data collection or analysis is complete.</p>"},{"location":"data_management/lts/lts_cores/#transferring-from-local-machines-to-lts","title":"Transferring From Local Machines to LTS","text":"<p>The details concerning data transfer from core instruments and analysis machines can differ drastically across cores. For cores where all data are transferred from a single machine or server to LTS, a single installation of Globus Connect Personal or Globus Connect Server would be satisfactory for uploading data to LTS.</p> <p>Warning</p> <p>Do not set up transfers from machines that core customers will have access to. This creates opportunities for researchers to access data they are not authorized for. Data collected by the core should only be accessible by core personnel until those data are distributed to their respective labs.</p> <p>For situations where data either needs to be transferred from multiple machines to LTS. please contact Research Computing for a consultation.</p>"},{"location":"data_management/lts/lts_cores/#distributing-to-data-owners","title":"Distributing to Data Owners","text":"<p>While uploading and/or managing data for other groups, data in any buckets the core owns will count against the quota for the core. Data will need to be distributed in some way to the groups who own the data to free up storage in the core's account once those data have been fully collected or analyzed. It is not currently possible to directly the change the owner of a bucket without submitting a ticket to research computing, however it is possible to set permissions on a bucket to allow the data owners to copy the data to a new bucket under their ownership. Once the data are copied, the original bucket can be moved onto a physical disc as an archive or deleted.</p> <p>Permission to copy data is granted via a policy file. Policy files can be customized extensively, but a general file can be seen below.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n    {\n        \"Sid\": \"lab-permissions\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/lab-group\"\n            ]\n        },\n        \"Action\": [\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::core-bucket\",\n            \"arn:aws:s3:::core-bucket/*\"\n        ]\n    }]\n}\n</code></pre> <p>This policy file allows the <code>lab-group</code> LTS account permission to list and copy all objects in the bucket. Data should typically only be copied from a core bucket to a lab bucket, not to a specific individual's bucket. The owner of <code>lab-group</code> will need to initiate the transfer. General commands for creating a bucket and transferring data to it can be found below for convenience. These commands use s5cmd for transfer and assume the credentials used are for the <code>lab-group</code>.</p> <pre><code># Create a new bucket to hold the data\ns5cmd --endpoint-url https://s3.lts.rc.uab.edu mb s3://destination-bucket\n\n# Copy files from core bucket to lab bucket\ns5cmd --endpoint-url https://s3.lts.rc.uab.edu --numworkers 10 cp --concurrency 5 s3://core-bucket/* s3://destination-bucket\n</code></pre> <p>Note</p> <p>The <code>cp</code> command above specifies 10 CPUs to use for transfer to increase throughput. This value should be changed based on the number of cores available in your job if you're copying using Cheaha.</p> <p>Globus is another option for transferring data using a GUI as opposed to a command line. When using Globus, you will need to set up the LTS endpoint using the credentials for the lab account. Access the LTS endpoint in both window panes and type the names of the buckets to transfer to and from. See the image below as an example</p> <p></p> <p>Once you've accessed those buckets, drag and drop which files need to be transferred.</p>"},{"location":"data_management/lts/lts_faq/","title":"LTS FAQ","text":""},{"location":"data_management/lts/lts_faq/#what-are-valid-bucket-names-in-lts","title":"What Are Valid Bucket Names In LTS?","text":"<p>Bucket names must be comprised only of lowercase letters, numbers, and hyphens. No capital letters or underscores are allowed. Trying to create a bucket with the disallowed characters will return an error.</p>"},{"location":"data_management/lts/lts_faq/#can-i-share-my-account-access-keys-with-other-people","title":"Can I Share My Account Access Keys With Other People?","text":"<p>You should never share access keys with anyone. These should be treated similarly to your BlazerID and password. Sharing keys creates a point of vulnerability and if they fall into a nefarious actor's hands, all buckets that account owns and the data in them can be deleted.</p> <p>In some cases, you may not be actively managing data in a bucket even though you own the account which owns a shared bucket. Instead of sharing keys with a data steward, instead that steward should be given admin-esque permissions on the required bucket via a policy file.</p>"},{"location":"data_management/lts/lts_faq/#how-should-i-organize-my-lts-shared-account","title":"How Should I Organize My LTS Shared Account?","text":"<p>This is ultimately up to the bucket owner, but there are a couple of single-bucket solutions depending on your specific use-case for LTS:</p> <ol> <li>Semi-synced copy of everything in the project space.<ul> <li>General permissions: Data stewards and the bucket owner would have permission to delete any files. All other users would be able to upload and download files only. All users would be able to see all files uploaded by all other users to that bucket./</li> <li>Purpose: This fulfills more of a pure backup role compared to option 2. While all users can upload files</li> <li>Benefits: The policy file for these permissions is much simpler to create and manage. Limits the number of people who can remove files that might be needed down the line.</li> <li>Drawbacks: Needing to ask a steward or the bucket owner to delete individual files introduces friction to data management.</li> <li>Example Policy File</li> </ul> </li> <li>Active and collaborative external storage. All users would have a specific prefix/folder they have complete control where they can add or remove data at will.<ul> <li>General permissions: Data stewards and the bucket owner would have permission to delete any files. Regular users would only be able to upload to and delete files from their owned prefix/folder. All users would be able to see and download any files from any other user.</li> <li>Purpose: This satisfies the need for expanded storage accessible from Cheaha (via the terminal or Globus). All users have their own space they can use as they see fit within the bucket for extra storage while still being able to access, but not alter, files from other users in cases they need to be shared. Part of the bucket, or a separate bucket entirely, can also be used as a backup for old or current datasets where users only have read permissions.</li> <li>Benefits: How the bucket can be used is much more malleable and up to the individual users. Empowers them to add and remove data from their own prefix/folder without oversight from stewards or the bucket owner.</li> <li>Drawbacks: The policy file is more difficult to craft and manage when researchers needed to be added or removed from the bucket. Allowing users to delete their uploaded data at their discretion may conflict with the owner's view of those data.</li> <li>Example Policy File</li> </ul> </li> </ol> <p>While these are two simple solutions, a combination of both can be implemented with some clever crafting of the policy file. As well, you could take advantage of both solutions with multiple buckets. Keep in mind that data in all buckets contribute towards the total storage allocation equally. Once an account's storage quota is reached, no files can be added to any bucket owned by that account until files are removed.</p>"},{"location":"data_management/lts/lts_faq/#are-automatic-backups-to-lts-available","title":"Are Automatic Backups to LTS Available?","text":"<p>Automatic backups are not available by default. If you would like to periodically sync your bucket to a directory on your local machine or Cheaha, you will need to set up a cron task to submit a Slurm job that will run a sync. IF you would like to implement this for your own bucket, please contact us.</p>"},{"location":"data_management/lts/lts_faq/#why-can-i-not-interact-with-a-file-in-my-bucket","title":"Why Can I Not Interact With A File In My Bucket?","text":"<p>While S3's object storage system does not have POSIX permissions seen in a Linux system entirely, we have found that users who upload files to a shared space have ownership permissions on those objects, and the bucket owner and stewards cannot interact with those objects by default. Instead, owners and stewards need to be given explicit permissions to move or delete all objects in a bucket. This can be dealt with by adding the following sections to the policy file:</p> <pre><code>{\n    \"Sid\": \"admin\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": [\n            \"arn:aws:iam:::user/example_core\",\n            \"arn:aws:iam:::user/account_owner@uab.edu\"\n        ]\n    },\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::bucket\",\n        \"arn:aws:s3:::bucket/*\"\n    ]\n},\n{\n    \"Sid\": \"data-steward\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": [\n            \"arn:aws:iam:::user/steward_1@uab.edu\"\n        ]\n    },\n    \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:GetBucketPolicy\",\n        \"s3:PutBucketPolicy\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::bucket\",\n        \"arn:aws:s3:::bucket/*\"\n    ]\n}\n</code></pre>"},{"location":"data_management/lts/lts_faq/#how-can-i-share-a-bucket-with-all-lts-users","title":"How Can I Share A Bucket With All LTS Users?","text":"<p>The following policy file will give read permission to all LTS users for all objects in a bucket:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"admin\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": [\n                    \"arn:aws:iam:::user/account_owner@uab.edu\"\n                ]\n            },\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n            \"arn:aws:s3:::bucket\",\n            \"arn:aws:s3:::bucket/*\"\n            ]\n        },\n        {\n            \"Sid\": \"read-only-all\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": [\n                    \"arn:aws:iam:::user/*\"\n                ]\n            },\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n            \"arn:aws:s3:::bucket\",\n            \"arn:aws:s3:::bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"data_management/lts/lts_faq/#can-i-change-permissions-on-a-bucket-via-globus","title":"Can I Change Permissions On A Bucket Via Globus?","text":"<p>As of now, there is no way to change permissions on a bucket via Globus. The only way to change permissions is via the command line.</p>"},{"location":"data_management/lts/policies/","title":"Sharing Buckets","text":"<p>A major use for LTS is storage of data that should be accessible to multiple users from a lab or research group. By default, buckets are only visible and accessible to the owner of the bucket, and no mechanism exists to search for buckets other users have created.</p> <p>Instead, sharing buckets must be done through the command line using bucket policies. A bucket policy is a JSON formatted file that assigns user read and write permissions to the bucket and to objects within the bucket. If you have not worked with JSON files before, a brief explanation can be found here. It's important to note that the bucket owner will always retain the ability to perform all actions on a bucket and its contents and so do not need to be explicitly granted permissions.</p> <p>Important</p> <p>Your username for LTS could potentially be <code>&lt;BlazerID&gt;</code> or <code>&lt;BlazerID&gt;@uab.edu</code> depending on when your account was created. It is very important when crafting these policies that the correct username is specified, and these two are not interchangeable. For users with XIAS accounts, your username should be the email address you signed up for the XIAS account with. The usernames are case-sensitive. If you do not remember what your username is, see the email you received with your access key and secret key information or submit a support ticket to support@listserv.uab.edu.</p>"},{"location":"data_management/lts/policies/#policy-structure","title":"Policy Structure","text":"<p>Policies files are essentially built as a series of statements expressly allowing or deny access to functions that interact with objects in S3. A skeleton policy file would look like this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Sid\": \"description\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\"AWS\": []},\n        \"Action\": [],\n        \"Resource\": []\n    }]\n}\n</code></pre> <p>Each statement is made up of a few fields:</p> <ol> <li>Sid: a short decription of what the statement is for (i.e. \"bucket-access\")</li> <li>Effect: \"Allow\" or \"Deny\" permissions based on how you want to alter permissions</li> <li>Principal: Essentially a list of users to change permissions for. Have to formatted like <code>arn:aws:iam:::user/&lt;lts_username&gt;</code>.</li> <li>Action: A list of commands to allow or deny permission for, depending on the Effect value.</li> <li>Resource: The name of the bucket or objects to apply permissions to. Must be formatted like <code>arn:aws:s3:::&lt;bucket[/path/objects]&gt;</code>.</li> </ol> <p>It is currently suggested to have at least two statements, one statement allowing access to the bucket itself, and another statement dictating permissions for objects in the bucket.</p> <p>For example, if you wanted to give users <code>bob</code> and <code>jane@uab.edu</code> the ability to list objects in your bucket <code>bucket1</code>, the statement would be:</p> <pre><code>{\n    \"Sid\": \"list-bucket\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": [\n            \"arn:aws:iam:::user/bob\",\n            \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n    \"Action\": [\n        \"s3:ListBucket\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::bucket1\"\n    ]\n}\n</code></pre> <p>Critically, this does not allow the given users the ability to read, download, edit, or delete any objects in the bucket. They will be able to list the objects, see the names, sizes, and directory structure but will not be able to interact with the objects. These permissions should be enumerated in a separate statement like:</p> <pre><code>{\n    \"Sid\": \"read-only\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": [\n            \"arn:aws:iam:::user/bob\",\n            \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n    \"Action\": [\n        \"s3:GetObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::bucket1/*\"\n    ]\n}\n</code></pre> <p>This permission set allows <code>bob</code> and <code>jane@uab.edu</code> to download files but not to move, overwrite, delete, or otherwise interact with them. Notice that the Resource value has changed from just <code>bucket1</code> to <code>bucket1/*</code>. We can set this Resource value to different paths and objects to limit the permissions granted. For example:</p> <ol> <li><code>bucket1/*</code>: Apply permissions to all objects in the entire bucket</li> <li><code>bucket1/test_folder/*</code>: Apply permissions to all objects in folder <code>test_folder</code></li> <li><code>bucket1/test_folder/*jpg</code>: Apply read permissions to only JPGs within <code>test_folder</code>.</li> </ol> <p>For the last two examples, <code>bob</code> and <code>jane@uab.edu</code> will not have permission to download any files outside of the <code>test_folder</code> folder. All permissions are implicitly denied unless explicitly given in the policy statements.</p>"},{"location":"data_management/lts/policies/#common-actions","title":"Common Actions","text":"<p>Being able to download a file is only one possible action you may want to give permission for. Uploading files as well as altering the policy of the bucket may also be useful to give permissions for. Here is a short list of common actions you may want to give permissions for:</p> <ol> <li><code>s3:ListBucket</code>: access to see but not interact with objects</li> <li><code>s3:GetObject</code>: download objects</li> <li><code>s3:PutObject</code>: upload objects</li> <li><code>s3:DeleteObject</code>: remove objects</li> <li><code>s3:GetBucketPolicy</code>: view the current bucket policy</li> <li><code>s3:PutBucketPolicy</code>: change the current bucket policy</li> </ol> <p>A full list of Actions for UAB LTS can be seen on the Ceph docs.</p>"},{"location":"data_management/lts/policies/#example-policies","title":"Example Policies","text":""},{"location":"data_management/lts/policies/#read-only-for-all-files","title":"Read-Only for All Files","text":"<p>This will give permissions to <code>bob</code> and <code>jane@uab.edu</code> for bucket <code>bucket1</code>. The permissions will include bucket access (the <code>list-bucket</code> statement) as well as read-only permissions for all objects in the bucket (the <code>read-only</code> statement). Specifically, they will be able to copy the files from the bucket to another bucket or a local file system.</p> <pre><code>    {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Sid\": \"list-bucket\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/bob\",\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::bucket1\"\n        ]\n    },\n    {\n        \"Sid\": \"read-only\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/bob\",\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:GetObject\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::bucket1/*\"\n        ]\n    }]\n    }\n</code></pre>"},{"location":"data_management/lts/policies/#read-write-permissions","title":"Read-Write Permissions","text":"<p>This will give read, write, and delete permissions to <code>bob</code> and <code>jane@uab.edu</code> so they are able to sync directories between a local source folder and the S3 destination. This can be dangerous because of the delete permissions so care should be given in handing out that permission. <code>s3:DeleteObject</code> can be set into another statement field and limited to very select users while giving upload permission to many users.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Sid\": \"list-bucket\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/bob\",\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::bucket1\"\n        ]\n    },\n    {\n        \"Sid\": \"read-write\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/bob\",\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::bucket1/*\"\n        ]\n    }]\n}\n</code></pre>"},{"location":"data_management/lts/policies/#tiered-permissions","title":"Tiered Permissions","text":"<p>In some instances, the bucket owner (i.e. ideally the PI for the lab if this is a shared lab space) will want to allow certain users to have permissions to alter the policies for new or departing lab members. This example will give standard read-write permissions to both our lab members, but only policy altering permissions to <code>jane@uab.edu</code>.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Sid\": \"list-bucket\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/bob\",\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::bucket1\"\n        ]\n    },\n    {\n        \"Sid\": \"change-policy\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:GetBucketPolicy\",\n            \"s3:PutBucketPolicy\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::bucket1\"\n        ]\n    },\n    {\n        \"Sid\": \"read-write\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": [\n                \"arn:aws:iam:::user/bob\",\n                \"arn:aws:iam:::user/jane@uab.edu\"\n            ]\n        },\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\"\n        ],\n        \"Resource\": [\n         \"arn:aws:s3:::bucket1/*\"\n        ]\n    }]\n}\n</code></pre>"},{"location":"data_management/lts/policies/#comments-in-s3-iam-policies","title":"Comments in S3 IAM Policies","text":"<p>IAM policies for <code>S3</code>, used by LTS for object-level access control within buckets, are written in <code>JSON</code> format. Since <code>JSON</code> does not support comments natively, <code>AWS</code> does not provide a dedicated comment field in their IAM policy schema.</p> <p>The optional <code>SID</code> field in IAM policies, though intended for uniquely identifying statements, can also be used as an ad-hoc comment. In the example below, the <code>SID</code> field provides a description of the statement, serving as a comment.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"This statement grants read access to all objects in bucket1\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\":[\n            \"arn:aws:iam:::user/bob\",\n            \"arn:aws:iam:::user/jane@uab.edu\"\n        ]\n      },\n      \"Action\": [\n        \"s3:GetObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::bucket1\",\n        \"arn:aws:s3:::bucket1/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"data_management/lts/policies/#specifying-all-actions-in-iam-policies","title":"Specifying \"all actions\" in IAM Policies","text":"<p>To allow or deny all actions on a specific resource, such as an <code>S3</code> bucket or object, use the following <code>Action</code> block as part of a <code>Statement</code> object to specify that all actions are affected by the statement:</p> <pre><code>    \"Action\": [\n    \"s3:*\"\n    ],\n</code></pre> <p>Here is an example IAM policy that grants all <code>S3</code> actions on bucket1 and all its objects:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"This statement grants access to all S3 actions in bucket1\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\":[\n             \"arn:aws:iam:::user/bob\",\n             \"arn:aws:iam:::user/jane@uab.edu\"\n        ]\n      },\n      \"Action\": [\n        \"s3:*\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::bucket1\",\n        \"arn:aws:s3:::bucket1/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"data_management/lts/policies/#applying-a-policy","title":"Applying a Policy","text":"<p>Policies can be applied to a bucket either by the owner or by a user who has been given the <code>s3:PutBucketPolicy</code> permission. Use s3cmd to apply policies.</p> <p>Note</p> <p>As of now, rclone has not implemented a way to alter policies, and AWS CLI does not apply them to our system correctly. If you have been using rclone or AWS CLI, you will need to configure s3cmd for policy management instead</p> <p>Applying a policy is fairly straightforward for both tools. First, you should save the policy as a JSON file. The, you can interact with the policy using these commands:</p> <pre><code># s3cmd set policy\ns3cmd setpolicy &lt;policy_file&gt; s3://&lt;bucket&gt;\n\n# s3cmd view policy\ns3cmd info s3://&lt;bucket&gt;\n\n# s3cmd remove policy\ns3cmd delpolicy s3://&lt;bucket&gt;\n</code></pre>"},{"location":"data_management/lts/policies/#policy-suggestions","title":"Policy Suggestions","text":"<p>Important</p> <p>Policies can be very complicated depending on how many people need access to the bucket and how you want to tier permissions (i.e. which people are read-only, read-write, admin-esq priveleges, etc.). If you need help structuring your policy files please visit us during office hours and we will be happy to help structure your policy file to your needs.</p>"},{"location":"data_management/lts/policies/#admin-esq-priveleges","title":"Admin-esq Priveleges","text":"<p>It is suggested to keep the number of people who have permission to delete data and alter policies to a minimum. Inexperience with policies can result in permissions being granted to incorrect users which can potentially lead to irrecoverable consequences. Syncing data without purposeful thought can result in the undesired loss of data.</p>"},{"location":"data_management/lts/policies/#bucket-ownership","title":"Bucket Ownership","text":"<p>For labs using LTS to store data from their Cheaha Project Storage directory, it is highly advised that the PI for the lab creates and owns the bucket and then gives policy changing permissions to another researcher for day-to-day maintenance if desired. For instance, if a lab manager creates the bucket and then leaves the university without giving policy permissions to other users, the lab will not be able to change the policies for those data.</p>"},{"location":"data_management/lts/policies/#sharing-multiple-datasets-with-different-groups","title":"Sharing Multiple Datasets with Different Groups","text":"<p>Some groups on campus may distribute datasets to other research groups using LTS. If you are distributing data to multiple groups, and those groups should not have access to each other's data, it is highly advised to store those datasets in separate buckets as opposed to separate directories in a single bucket.</p> <p>An idiosyncrasy of buckets involves the fact that all objects are stored in the top level of the bucket, and once permissions are given to someone to see the bucket, they will be able to see all objects within the bucket without restrictions even if they are not given download permissions for some objects. If any identifying or priveleged information is given in file names on LTS, it could constitute an IRB violation. Additionally, managing permissions for groups to access data only from specific folders makes the policy file much more complicated and prone to errors. When sharing multiple datasets with multiple different groups, it's advised to keep these data in separate buckets and have individual policy files for each bucket to make policy management simpler and less prone to error.</p>"},{"location":"data_management/lts/tutorial/","title":"LTS and S3 Basic Tutorial","text":"<p>Effective data management and sharing are essential for maintaining data integrity and facilitating collaboration in research. UAB\u2019s Long-Term Storage (LTS) offers a robust solution for securely archiving data that may not be actively used but remains crucial for future research and compliance. The <code>s3cmd</code> command-line tool complements this by enabling efficient management and transfer of files to and from LTS.</p> <p>Why use LTS and <code>s3cmd</code> for your research data? LTS provides a secure and long term storage solution for preserving your valuable data over time. To manage this data efficiently, <code>s3cmd</code>is the key. <code>s3cmd</code> simplifies interactions with LTS, making it easy to upload, download, and organize your files. In addition, <code>s3cmd</code> allows you to define various policies including for read and read/write access to your data, enhancing security and control over your research data. Together, LTS and <code>s3cmd</code> provide an efficient approach to data management in your research workflow.</p> <p>Below are the tutorials we currently have on using LTS and s3cmd:</p> <ul> <li>Basic workflow with Individual LTS and s3cmd: This tutorial provides step-by-step instructions for moving files to and from your individual LTS account using the Cheaha system. It covers the installation, configuration and use of the different <code>s3cmd</code> commands, as well as setting up access policies.</li> </ul>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/","title":"Basic Workflow with Individual LTS and s3cmd","text":"<p>In this tutorial, we will guide you through using <code>s3cmd</code> on the Cheaha system to effectively manage and interact with your individual LTS account. We will cover the installation and configuration of <code>s3cmd</code>, and demonstrate essential operations, including creating buckets, listing, copying, downloading, and deleting buckets and objects in your LTS account. In addition, we will show you how to set and manage read and write access for other accounts to your LTS buckets and objects.</p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#prerequisites","title":"Prerequisites","text":"<p>To get up to speed, you should have a basic understanding of how to use the shell/terminal. If you\u2019re not familiar with these concepts, we recommend checking out our learning resources on basic shell usage.</p> <p>You will also need an individual LTS account created by our team. If you believe you need an account but do not have one, please contact us.</p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#setting-up-your-environment","title":"Setting Up Your Environment","text":""},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#install-s3cmd-within-conda-environment-on-cheaha","title":"Install s3cmd within Conda Environment on Cheaha","text":"<p>To interact with LTS (Long-Term Storage) using S3 (Simple Storage Service), you need the <code>s3cmd</code> tool installed.<code>s3cmd</code> is a command-line tool for managing files in cloud storage systems like S3. It's recommended to install it using <code>pip</code>, the standard package installer for Python, which allows you to install packages from the Python Package Index (PyPI), within a Conda environment on Cheaha.</p> <p>Please avoid using <code>conda install s3cmd</code>, as that version will not work as expected. Instead, follow the steps below to install <code>s3cmd</code> using <code>pip</code> within your Conda environment.</p> <p>First, access our interactive Open OnDemand (OOD) portal at https://rc.uab.edu and create a job on Cheaha using one of our interactive applications. For guidance, refer to our tutorial on installing and setting Conda environment.</p> <p>Once your interactive apps session is launched, open the terminal as described in step 5 of the Anaconda tutorial page and run the below commands.</p> <pre><code>module load Anaconda3\nconda create -n s3\nconda activate s3\npip install s3cmd\n</code></pre> <p>Once these steps are completed, verify the installation by running <code>pip list | grep \"s3cmd\"</code> in the terminal. If <code>s3cmd</code> is listed, as shown in the screenshot below, the package has been successfully installed.</p> <p></p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#install-s3cmd-on-your-local-systems","title":"Install s3cmd on Your Local Systems","text":"<p>To install s3cmd on your local machine, please follow the instructions provided in our s3cmd documentation for local installation.</p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#configuring-s3cmd-for-lts-buckets","title":"Configuring s3cmd for LTS Buckets","text":"<p>Properly configuring <code>s3cmd</code> is important for working with LTS buckets and objects. The configuration process varies depending on whether you have a single LTS account or multiple accounts to manage. In this section, we will provide a step-by-step guide tailored specifically for the Cheaha system and a researcher with an individual LTS account.</p> <p>Open a terminal using one of the interactive apps on Cheaha. Activate your conda environment created in the Install s3cmd using within Conda Environment section, and then run the below command:</p> <pre><code>s3cmd --configure\n</code></pre> <p>This will prompt you to enter the access key and secret key associated with your individual LTS account. You will be asked for additional information, which will be displayed on the screen, as shown below. You can copy the necessary details from the example provided here.</p> <p></p> <p>Once the configuration is complete, <code>s3cmd</code> will generate a <code>.s3cfg</code> file in your home directory (<code>$HOME</code>), as shown below. To find your home directory in Cheaha and view the <code>.s3cfg</code> file, follow the instructions on our Navigating Open OnDemand page. Be sure to check the Show Dotfiles option in the top right corner to make hidden files visible.</p> <p></p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#creating-buckets","title":"Creating Buckets","text":"<p>Long Term Storage (LTS) services like Amazon S3 use a flat data organization model based on buckets and objects. Think of buckets as folders that contain individual pieces of data called objects. We have documentation about basic terminology on s3 storage system here.</p> <p>Once you have complete <code>s3cmd</code> configuration, you can create new buckets in your individual LTS storage. To create a bucket use a <code>mb</code> (make bucket) command:</p> <pre><code>s3cmd mb s3://your-bucket-name\n</code></pre> <p>Please replace <code>your-bucket-name</code> with your desired name. This command creates a new bucket that you named <code>your-bucket-name</code> in your LTS storage using the currently configured <code>s3cmd</code> profile. For example, the image below shows an LTS bucket named <code>first-test-bucket</code> that was created successfully after running the command <code>s3cmd mb s3://first-test-bucket</code>.</p> <p></p> <p>When creating a bucket, it is important to be aware of name uniqueness and naming conventions. For detailed information on bucket naming, please refer to our documentation on valid bucket names in LTS and avoiding duplicate names for buckets.</p> <p>If you try to create a bucket with <code>s3cmd mb</code> with the name that already exists within your namespace, the system will report success without making any changes. For example, if you run <code>s3cmd mb s3://existing-bucket-name</code> and that bucket name is already taken in your namespace, the command will complete successfully without creating a new bucket and showing an error. However, if you try to create a bucket with a name that is already used in someone else\u2019s namespace, you will receive a <code>409 (BucketAlreadyExists)</code> error. To create a unique bucket within your namespace, first use <code>s3cmd ls</code> to list existing buckets and choose a name that is not already in use. To avoid the <code>409 (BucketAlreadyExists)</code> error and ensure your bucket name is unique, follow the avoiding duplicate names for buckets guide. This will help you create your bucket successfully and maintain its uniqueness.</p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#managing-buckets","title":"Managing Buckets","text":"<p>To manage a bucket, various commands can be used. Below are some common <code>s3cmd</code> commands to interact with your LTS bucket and its objects:</p> <ul> <li>To list all buckets you own with your current <code>s3cmd</code> profile, use command: <code>s3cmd ls</code>.</li> <li>To list all objects in a bucket accessible with your current <code>s3cmd</code> profile, use command: <code>s3cmd ls s3://your-bucket-name</code>.</li> <li>To upload a file named <code>file.txt</code> to a bucket, use a command: <code>s3cmd put file.txt s3://your-bucket-name/</code>. For example, to upload the file <code>file.txt</code> from the <code>/data/user/$USER</code> directory on Cheaha to the bucket <code>your-bucket-name</code>, you can use the command: <code>s3cmd put /data/user/$USER/file.txt s3://your-bucket-name</code>.</li> <li>To download an object named <code>file.txt</code> from a bucket, use a command:<code>s3cmd get s3://your-bucket-name/file.txt</code>. For example, to download the object <code>file.txt</code> from the bucket <code>your-bucket-name</code> to the <code>/data/user/$USER</code> directory on Cheaha, you can use a command <code>s3cmd get s3://your-bucket-name/file.txt /data/user/$USER</code>.</li> <li>To delete/remove an object, use command: <code>s3cmd del s3://your-bucket-name/your-object-name</code>.</li> <li> <p>To delete/remove a bucket, use command: <code>s3cmd rb s3://your-bucket-name</code>.</p> <p>Note</p> <p>An S3 bucket cannot be deleted unless it is completely empty. If the bucket contains any objects, <code>s3cmd rb</code> will report an error, like <code>S3 error: 409 (BucketNotEmpty)</code> when you attempt to delete it. To remove all objects within the bucket, use: <code>s3cmd del -r s3://your-bucket-name --force</code>. You can then remove the bucket itself with: <code>s3cmd rb s3://your-bucket-name</code>.</p> <p>Danger</p> <p>Deleting objects and buckets cannot be undone. Once the delete command is entered, any data is lost permanently and cannot be restored.</p> </li> </ul> <p>You can find a variety of <code>s3cmd</code> commands in our documentation at here and on the S3tools website. For quick reference, you can also use the <code>s3cmd --help</code> command to view available options directly in your terminal.</p> <p>If you are continuing in the same session with your conda environment already activated, you can directly use the <code>s3cmd</code> commands. If you are starting a new session or returning at a later date, make sure to load the Anaconda module and activate your conda environment before using <code>s3cmd</code>.</p>"},{"location":"data_management/lts/tutorial/individual_lts_tutorial/#how-to-grant-access-to-other-accounts-for-your-buckets","title":"How to Grant Access to Other Accounts for your Buckets?","text":"<p>Managing access to your buckets is essential for both collaboration and security. By setting up specific bucket policies, you can control who can view or modify your bucket\u2019s contents. Follow these steps to grant access:</p> <ul> <li>Create a policy file: define a policy and save it as a <code>JSON</code> file. For guidance and details on creating and formatting policy files, refer to our create a policy structure guide. For example, you might create a policy file named <code>my_policy.json</code> with read permissions.</li> <li>Apply the policy: Use the command like <code>s3cmd setpolicy policy_file.json s3://your-bucket-name</code> to apply your defined read policy to your bucket. Replace <code>policy_file.json</code> with the name of your policy file and <code>your-bucket-name</code> with the name of your bucket.</li> <li>Verify the policy update: After applying the policy, you should see a <code>Policy updated</code> message if the operation was successful. You can also verify the applied policy by running: <code>s3cmd info s3://your-bucket-name</code>.</li> </ul> <p>Below is a screenshot showing how to apply a policy file named <code>my_policy.json</code> to a bucket named <code>first-test-bucket</code>, and the command to view information about this bucket, including the policy file that we defined and applied.</p> <p></p> <p>Please note that the permissions granted are determined by the settings defined in your policy file. The policy demonstrated in this example is a read-only policy. Below, you can find examples of the different policies, including the read and write policies, you can set and apply for your buckets and objects in your individual LTS account.</p> <ul> <li> <p>Read-only Access</p> <p>To allow another account to view and copy files from your bucket without making any changes, use the read only permission policy.</p> </li> <li> <p>Read/Write Access</p> <p>To grant another account the ability to both view and modify the contents of your bucket, use the read/write permissions policy.</p> </li> </ul> <p>For detailed information on LTS bucket policies and instructions on how to apply and remove bucket policies, please refer to our policy structure and apply bucket policy guides.</p>"},{"location":"data_management/transfer/filezilla/","title":"FileZilla","text":"<p>Filezilla is a free SFTP platform used to transfer data between a local machine and a remote server, in this case remote storage accessible to Cheaha and UAB Cloud or long-term S3 storage. Filezilla is useful for medium sized data transfers from a local machine as opposed to Globus which can easily connect two different local or remote servers and handle large quantities of data.</p>"},{"location":"data_management/transfer/filezilla/#installation","title":"Installation","text":"<p>To download the program, go to the FileZilla site and click Download FileZilla Client. Filezilla is also already installed on Cheaha and can be accessed through the Accessories menu.</p>"},{"location":"data_management/transfer/filezilla/#using-filezilla","title":"Using FileZilla","text":"<p>Once FileZilla is installed and you open it, you will see the following window</p> <p></p> <p>A file browser for the local machine FileZilla is running on is on the left while the file system for the remote site will be shown on the right once a remote site has been connected.</p>"},{"location":"data_management/transfer/filezilla/#creating-a-remote-connection","title":"Creating a Remote Connection","text":"<p>You can easily connect to a remote site using the QuickConnect bar near the top of the window. You will need to input the following options:</p> <ul> <li>Host: <code>sftp://cheaha.rc.uab.edu</code></li> <li>Username: your BlazerID or XIAS ID (do NOT include '@uab.edu')</li> <li>Password: your BlazerID or XIAS password</li> <li>Port: <code>22</code></li> </ul> <p>Click <code>Quickconnect</code> and you will be connected to the remote storage system. The window should now look like:</p> <p></p> <p>When connecting in the future, you will be able to select the connection from the dropdown arrow next to the Quickconnect button.</p> <p>In both the local and remote panes, you can navigate to the directories you are transferring from and to. You only have access to directories you can normally access on Cheaha, so your Individual Storage as well as any Project Storage directories you have been added to.</p>"},{"location":"data_management/transfer/filezilla/#transferring-data","title":"Transferring Data","text":"<p>From here you can drag and drop whichever files and directories between the local and remote site windows. This will automatically initiate a file transfer. The directory structure is maintained from the source to the destination and is recursive, so all subdirectories and folders within the main directory will be transferred as well.</p> <p>Transfer status will be logged at the bottom of the window. Once all your files have been transferred, you can close FileZilla which will close the SFTP connection.</p>"},{"location":"data_management/transfer/globus/","title":"Globus","text":"<p>Globus is a powerful tool for robustly and securely managing data transfers to and from collaborators and within UAB Research Computing. Globus is recommended for most single-use, day-to-day data transfer use-cases.</p> <p>UAB Research Computing uses High Assurance Endpoints, meaning there are additional security measures in place to reduce risk and move toward HIPAA compliance. Generally speaking, if you have used Globus in the past, the data transfer interface has not changed, but there are a few new restrictions/changes.</p> <ol> <li>You will be prompted to prove authorization each time you access a UAB Research Computing endpoint or attempt to download files to your local machine from such an endpoint. If you are already logged in with Single Sign-On (SSO) the process is simple. If not, you will need to authenticate with SSO.</li> <li>Bookmarks are not allowed in High Assurance endpoints.</li> <li>In the newer version, Globus \"Endpoints\" have been moved under \"Console\", which can be located in the left-hand navigation pane.        </li> </ol> <p>For more detailed information on High Assurance please see the Globus official pages below:</p> <ul> <li>High Assurance Security Overview</li> <li>High Assurance Collections</li> </ul>"},{"location":"data_management/transfer/globus/#setting-up-globus-connect-personal","title":"Setting up Globus Connect Personal","text":"<p>Globus Connect Personal is software meant to be installed on local machines such as laptops, desktops, workstations and self-owned, local-scale servers. Globus maintains excellent documentation for installation on MacOS, Linux and Windows.</p> <p>To verify your installation is complete, please visit https://app.globus.org and log in. Click \"Collections\" in the left-hand navigation pane and then click the \"Administered By You\" tab. Look in the table for the collection you just created.</p>"},{"location":"data_management/transfer/globus/#managing-identities","title":"Managing Identities","text":"<p>Globus Identities is a concept helping to map Globus Accounts (one per person) to institutions (one or more per person).</p> <p>Most UAB researchers will have a single identity, their UAB identity, tied to their BlazerID. Some researchers may have external collaborations or appointments that provide additional entities which need access to other endpoints on Globus.</p> <p>To manage your identities, navigate to https://app.globus.org/account/identities and sign in.</p> <p>Important</p> <p>To use UAB Research Computing endpoints, you will need to ensure you are using your UAB identity.</p>"},{"location":"data_management/transfer/globus/#moving-data-between-collections","title":"Moving Data Between Collections","text":"<ol> <li> <p>Log in to the Globus App online at https://app.globus.org using UAB Single Sign-On (SSO). Start typing \"University of Alabama at Birmingham\" into the \"Use your existing organizational login\" text box and selected it when it appears in the list.</p> <p></p> </li> <li> <p>Click File Manager in the left-hand navigation pane.</p> <p></p> </li> <li> <p>Ensure the center icon for the \"Panels\" selection is picked.</p> <p></p> </li> <li> <p>Click the \"Search\" icon in the \"Collection\" text box near the top-left or top-right of the page to locate an endpoint/collection. An endpoint is a server that provides access to data stored in a cluster. A collection can be a mapped collection or guest collection, which represents a user's own collection of files or directories mapped from storage endpoints to their Globus account for easy management, sharing and accessing. Please refer to the Globus endpoints and collections to know more. There are multiple ways to find an endpoint. For some endpoints you may be asked to log in, which is true of all UAB endpoints. Some UAB endpoints may also require that you be on the UAB Campus VPN.</p> <p></p> <ol> <li> <p>Begin typing in the box to search for an endpoint. To find UAB-related endpoints, search for \"UAB\". There are two Cheaha endpoints</p> <ol> <li>Cheaha cluster on-campus (UAB Science DMZ) for machines that are either on the UAB Campus Network, or connected to the UAB Campus VPN.</li> <li>Cheaha cluster off-campus (UAB Science DMZ) for machines that are not on the UAB Campus Network and not on the UAB Campus VPN.</li> </ol> </li> <li> <p>The \"Recent\" tab shows endpoints/collections that have most recently been used.</p> <p></p> </li> <li> <p>The \"Bookmarks\" tab shows a list of collection bookmarks. Bookmarks may not reference folders within UAB Research Computing or other High Assurance endpoints.</p> <p></p> </li> <li> <p>The \"Your Collections\" tab shows all collections owned by you. For most researchers this will be one or more Globus Connect Personal collections.</p> <p></p> </li> <li> <p>The \"Shared With You\" tab shows any private collections that have been shared with you by other users, possibly collaborators.</p> </li> </ol> </li> <li> <p>When an endpoint/collection has been selected you will see a list of folders and files on the default path for that endpoint/collection in the bottom box. You can use the \"Path\" box to type a path to find the files you are looking for.</p> <p></p> </li> <li> <p>Repeat the process of selecting an endpoint/collection for the other \"Collection\" text box.</p> <p></p> </li> <li> <p>When both endpoints have been selected and you have chosen the correct paths for each collection, select files and/or folders on the side you wish to transfer FROM. We will call this side the source collection, and the other side the target collection. Selections may be made by clicking the checkboxes that appear when you hover over each file or folder.</p> <p></p> </li> <li> <p>When all files and folders have been selected from the source collection, click the \"Start\" button on the source collection side. This will start a transfer process from source to target. The files will be placed in the currently open path on the target collection.</p> <p></p> </li> <li> <p>A green pop-up notification will appear indicating the transfer has started. Click \"View details &gt;\" to be taken to the status of the transfer. You can also check on the status of any transfers by clicking the \"Activity\" button in the left-hand navigation pane.</p> </li> </ol> <p>Note</p> <p>File permissions from the source will not be copied to the destination. Please read more at this ask.ci FAQ.</p>"},{"location":"data_management/transfer/globus/#transfer-and-sync-options","title":"Transfer and Sync Options","text":"<p>Between the two \"Start\" buttons on the \"File Manager\" page is a \"Transfer &amp; Timer Options\" drop down menu. Click that button to change the options. More information on each option. A brief summary of the options under \"Transfer Settings\" are...</p> <p></p> <ol> <li>sync - Sync files only, rather than create new files.</li> <li>delete files - Delete any files on the target that are not on the source. Useful for forcing identical filesystems when syncing.</li> <li>preserve source - Copies file \"modified time\" metadata.</li> <li>verify integrity - Verifies that checksums are identical on source and target after transfer completes. The default option is to verify integrity. Its highly recommended to leave this unchecked.</li> <li>encrypt transfer - Encrypts data before leaving source and decrypts after arriving at destination. Recommended for all transfers, required and enforced for all UAB endpoints. It is checked by default.</li> <li>skip files - Skips source files that cause errors during the transfer. Otherwise the entire transfer will stop when an error is encountered.</li> <li>quota fail - Fails instead of retries when the target storage quota is exceeded.</li> <li>Apply filter - Filter rules can be used to customize and fine-tune the transfer process, ensuring that only the desired files/directories are transferred while excluding others based on the rules defined.</li> </ol> <p>Below the \"Transfer Settings\", there are additional options available that you can enable to customize notification of success/failure, and schedule the transfer to occur at specific time.</p>"},{"location":"data_management/transfer/globus/#common-errors","title":"Common Errors","text":"<ol> <li>File Not Found - This may mean that a file was not readable by Globus. Check that the file hasn't moved or changed names during the transfer. It is recommended to not modify files while they are being transferred by Globus.</li> <li>Permission Denied - Globus is not able to access the files because permissions do not allow it. For Globus Connect Personal, be sure the containing folder is on the \"Accessible Folders\" list. Be sure that your Cheaha account has access to read the file.</li> </ol>"},{"location":"data_management/transfer/globus/#project-space-permissions","title":"Project Space Permissions","text":"<p>Globus does not preserve permissions nor ownership when data is transferred, instead using whatever permissions are default at the target location, and making the owner the authenticated user who initiated the transfer. Typically this is not an issue, but may cause problems for Project Storage directories. Please see our Project Directory Permissions Section for more information.</p>"},{"location":"data_management/transfer/globus/#more-information","title":"More Information","text":"<p>A Globus FAQ is available for additional information on endpoints and transfers.</p>"},{"location":"data_management/transfer/globus/#connectors","title":"Connectors","text":"<p>UAB Researcher Computing has subscriptions to connectors for cloud services and other types of filesystems.</p>"},{"location":"data_management/transfer/globus/#uab-box-connector","title":"UAB Box Connector","text":"<p>To use the UAB Box Connector, search for a collection/endpoint like usual and enter \"UAB Box\" into the search box. Select the endpoint labeled \"UAB Box\". You should see a list of files and folders that are available to you at https://uab.app.box.com. File transfers work as they would with any other endpoint or collection.</p>"},{"location":"data_management/transfer/globus/#long-term-storage-s3-lts-connector","title":"Long-term Storage S3 (LTS) Connector","text":"<p>Important</p> <p>LTS behaves differently from other file systems and comes with a few possible pitfalls. Keep in mind the following three rules: (1) all data must be in buckets, (2) buckets are only allowed in the root folder, and (3) buckets must have unique names.</p> <p>To use the UAB LTS Connector, search for a collection/endpoint like usual and enter \"UAB LTS\" into the search box. Select the endpoint labeled \"UAB Research Computing LTS (Long Term Storage aka S3)\". If you have stored data within LTS already you should see a list of folders, otherwise you will see an empty space where folders may be placed. Each folder corresponds to a bucket in LTS. To create a bucket, click \"New Folder\" in the \"File Manager\" window in Globus. Note that buckets must have globally unique names. Read on for more information about possible pitfalls.</p> <p></p>"},{"location":"data_management/transfer/globus/#adding-lts-allocation-credentials","title":"Adding LTS Allocation Credentials","text":"<p>Before you can manage an LTS allocation using Globus, you'll need to add the LTS S3 Access and Secret Keys to the LTS endpoint. To do so, please follow the instructions given below.</p> <ol> <li>In your browser, navigate to https://app.globus.org and login using UAB SSO.</li> <li>Click \"Collections\" in the left-hand navigation pane.</li> <li>In the search field at the top of the \"Collection\" page, enter \"UAB LTS\" and then click the search button. Be sure to uncheck \"Recently Used\" if it is checked. You should see a collection titled \"UAB Research Computing LTS (Long Term Storage aka S3)\" appear in the search results.</li> <li> <p>Click the right arrow indicator at the right-hand side of the \"UAB Research Computing LTS\" result. You should be taken to the Overview page for the \"UAB Research Computing LTS\" endpoint.</p> <p></p> </li> <li> <p>Near the top of the Overview page, click the \"Credentials\" tab\". After some time, you should see a form requesting your \"AWS IAM Access Key ID\" and \"AWS IAM Secret Key\".</p> </li> <li> <p>Enter the two requested keys in the appropriate boxes, then press \"Continue\". For data security reasons, you may be asked to authenticate again with your UAB SSO credentials. If you have more than one set of keys, you will need to choose which to enter. At this time, there doesn't appear to be a way to enter or use more than one set of credentials at a time with Globus.</p> <p></p> </li> <li> <p>You should be taken back to the UAB Research Computing LTS endpoint Overview page with the Credentials tab selected. There should now be one entry on this page showing your AWS IAM Access Key ID and your Globus Identity (i.e., your <code>BlazerID@uab.edu</code> email address). If you have multiple Access Keys and ever wish to change which one is being used with Globus, then click the \"Trash Can\" icon next to the entry on this page to delete it, and start the key entry process over from the previous step.</p> <p></p> </li> </ol> <p>At this point you are able to access the LTS allocation associated with the Access Key you entered, and any buckets which have granted access to that Access Key. If you have more than one Access Key, e.g. for each of your personal and project allocations, you will need to choose which key to enter above. If you ever wish to change credentials, then click the \"Trash Can\" icon next to the entry shown in step 6, above, and start the key entry process over.</p>"},{"location":"data_management/transfer/globus/#data-must-be-in-buckets","title":"Data Must be in Buckets","text":"<p>All data transferred to LTS must be placed in a bucket, and may not be placed directly into the root directory. Attempting to move data to the root directory will result in an unhelpful error message in the \"Activity\" window.</p> <p></p> <p>Clicking on the \"view event log\" link shows the following.</p> <p></p> <pre><code>Error (transfer)\nEndpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3)\nServer: m-a201b5.9ad93.a567.data.globus.org:443\nCommand: STOR /test.log\nMessage: The connection to the server was broken ---\nDetails: an end-of-file was reached\\nglobus_xio: An end of file occurred\\n\n</code></pre>"},{"location":"data_management/transfer/globus/#buckets-must-have-globally-unique-names","title":"Buckets Must Have Globally Unique Names","text":"<p>When creating new buckets, the name must be unique across all buckets on the system. If a duplicate bucket name, for example <code>first-test-bucket</code>, is entered, a long error message will appear in a small space next to the new bucket name. For readability, the expanded message is shown below.</p> <p></p> <pre><code>Remote Endpoint Failure: Path already exists, Error (mkdir)\nEndpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3)\nServer: 138.26.220.68:443\nMessage: Path '/first-test-bucket/' already exists\n</code></pre> <p>At first glance, requiring unique names across all buckets on the system may sound very restrictive, but it is necessary for LTS to be as fast as it is. Fortunately, there is an easy way to deal with the limitation. See our LTS section on good naming practice for how to avoid duplicate names. For example, if you want to name a bucket <code>ai-lab</code> for storing data related to the entire AI lab or a specific dataset, you can append a universally unique identifier (UUID) to the name. To generate a UUID, visit https://www.uuidgenerator.net/, and a 16-byte UUID will be automatically generated. You can then copy it and append it to the name <code>ai-lab</code>, as shown below.</p> <p></p> <p>Similarly, if an invalid bucket name, such as <code>first_test_bucket</code>, is entered (due to the use of an underscore, which doesn\u2019t follow LTS bucket naming rules), an error will also be displayed as shown below. To avoid this, please refer to the guidelines for valid bucket name in LTS.</p> <p></p> <pre><code>Bad Gateway: Endpoint Error, Error (mkdir)\nEndpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3)\nServer: 138.26.220.68:443\nCommand: MKD /first_test_bucket/\nMessage: Fatal FTP Response ---\nDetails: 500\nglobus_gridftp_server_s3_base: S3\nError accessing \"\": ErrorInvalidBucketName: ErrorInvalidBucketName: \\r\\n\n</code></pre> <p>To cancel or dismiss these errors, click the <code>refresh list</code> button on the Globus collection panel, or scroll all the way to the right in the red area of the error message and click the <code>x</code> symbol as shown below.</p> <p></p> <p>Uploading a top-level folder that does not follow the bucket naming rules will cause an error similar to the one encountered when creating a bucket with an invalid name. When uploading a folder with a name similar to a bucket in your LTS, Globus will sync and save all sub-folders and files into that bucket. Subfolders also need to follow naming rules, but they inherit the uniqueness from the parent name. This is part of why we recommend random UUIDs. If you attempt to upload a folder with a name that matches an existing bucket in someone else's space, you will encounter a <code>permission denied error</code>, as shown below.</p> <p></p> <p>Globus can create buckets. By default, buckets are created without a policy, meaning only you can access them until a policy is added. However, Globus cannot be used to modify or add policies. In addition, files transferred to a bucket will become objects with the same name, as long as the name is valid and not duplicated. Globus does not recognize or handle metadata, so you cannot use it to view or modify metadata. For guidance on defining policies for your bucket, please refer to our documentation on policy structure and applying a policy.</p>"},{"location":"data_management/transfer/globus/#using-bookmarks","title":"Using Bookmarks","text":"<p>To save a bookmark, use the File Manager interface to select an collection and navigate to a path on that collection. Then click the \"Create Bookmarks\" button as shown below.</p> <p></p> <p>To manage bookmarks, click on the Collection search bar, then select the Bookmarks tab. To edit a bookmark, click the 'Pencil' icon. To delete a bookmark, click the 'Trash Bin' icon.</p> <p></p> <p>Note</p> <p>It is not possible to create bookmarks within High Assurance Endpoints.</p>"},{"location":"data_management/transfer/globus/#managing-shared-collections-from-a-globus-connect-personal-endpoint","title":"Managing Shared Collections from a Globus Connect Personal Endpoint","text":"<p>It is NOT RECOMMENDED to make Globus Connect Personal collections public as this is insecure. It is more difficult to manage access controls for the entire Globus Connect Personal collection than for a shared collection. Shared collections make it simpler to share different data with distinct collaborators, and to manage who has access to what data. Be secure, use shared collections!</p>"},{"location":"data_management/transfer/globus/#creating-a-shared-collection","title":"Creating a Shared Collection","text":"<ol> <li> <p>Click \"Collections\" in the left-hand navigation pane.</p> </li> <li> <p>Click the \"Administered By You\" tab (or) you can search for the collections in search.</p> <p></p> </li> <li> <p>In the table, find the collections you wish to share data from and click its name. You will be taken to the page for that collection.</p> </li> <li> <p>Click the \"Collections\" tab.</p> <p></p> </li> <li> <p>Click the \"Add a Guest Collection\" button.</p> </li> <li> <p>Fill out the form.</p> <p></p> <ol> <li>Manually enter a path or click the Browse button to select a folder.</li> <li>Give a short but memorable name for your shared collection. This information will be useful for your collaborators.</li> <li>Optionally fill in a more detailed description of the shared collection for your records.</li> <li>Optionally fill in searchable keywords.</li> <li>Other additional options include, information link, contact email, organization/department.</li> <li>Default directory, if left empty, is equivalent to the first field \"Directory\".</li> </ol> </li> <li> <p>Click \"Create Collection\" to move to the next step. You will be taken to the page for the newly created collection, which is now a full-fledged shared collection. Any further references to \"an endpoint\" will be about the newly created, shared collection.</p> </li> <li> <p>Make sure you are on the \"Permissions\" tab. You should see a permissions table with your name in it.</p> <p></p> </li> <li> <p>Click \"Add Permissions -- Share With\" to share your guest collection with other users.</p> </li> <li> <p>Fill out the form.</p> <p></p> <ol> <li>Optionally enter a path within the shared endpoint or use the Browse button. If you leave the path as just a slash, the entire shared endpoint will be shared with the permitted users.</li> <li> <p>Select who to share with.</p> <ol> <li>User - One or more users.</li> <li>Group - All members of a group.</li> <li>All Users - All globus users.</li> <li> <p>Public - Makes data accessible to everyone.</p> <p>Danger</p> <p>It is important to note that options (iii) and (iv) poses a high risk in terms of security. Therefore, we strongly advise against this practice.This will expose information to everyone on Globus!</p> </li> </ol> </li> <li> <p>Search for users to add, or a group, depending on your choice above. You should be able to find any globus user using the search box.</p> <p>Warning</p> <p>Be certain of which user you are selecting! Check the email address domain.</p> </li> <li> <p>If adding users, optionally enter a message so they know why they are being added.</p> </li> <li>Select permissions. Read is automatically selected and cannot be changed. Write permissions are optional.</li> </ol> </li> <li> <p>Click \"Add Permission\" to add permissions for these users or groups.     You will be returned to the page for the shared collection and should     be on the \"Permissions\" tab and should see the user or group in the table.</p> </li> </ol>"},{"location":"data_management/transfer/globus/#deleting-a-shared-collection","title":"Deleting a Shared Collection","text":"<ol> <li> <p>Click \"Collections\" in the left-hand navigation pane, then</p> </li> <li> <p>Click the \"Administered By You\" tab.</p> <p></p> </li> <li> <p>Click the right caret \"&gt;\" icon at the right side of the row with the collection you wish to delete. You will be taken to the information page for that collection.</p> <p></p> </li> <li> <p>Click \"X Delete Collection\" and a confirmation dialog will open at the top of the page. Respond to the dialog to delete the collection, or to cancel.</p> <p></p> </li> </ol>"},{"location":"data_management/transfer/rclone/","title":"RClone","text":"<p>RClone is a powerful command line tool for transferring and synchronizing files over the internet between various machines, servers and cloud storage services. It is highly recommended for small to moderate amounts of data. For very large amounts of data consider using Globus for increased robustness against failure. Where Globus is not available, <code>rclone</code> is still suitable.</p> <p>RClone requires a modest amount of setup time on local machines, but once setup can be used fairly easily. RClone uses the concepts of \"remotes\", which is an abstract term for any storage service or device that is not physically part of the local machine. Many remotes are offered, including SFTP and various UAB Cloud Storage Solutions. SFTP may be used to access Cheaha, cloud.rc and other laptop and desktop computers.</p> <p>To use RClone effectively, you'll need to setup remotes before using the various commands. Most file manipulation commands on Linux can be found in the RClone commands, but may have slightly different names, e.g. <code>cp</code> is <code>rclone copy</code>.</p> <p>RClone is very powerful and, as such, has a wide variety of configuration options and flags to fine tune behavior. We will only cover the basics needed to install the software, setup remotes relevant to work at UAB, and some basic usage commands.</p>"},{"location":"data_management/transfer/rclone/#quick-tutorial-for-cheaha","title":"Quick Tutorial for Cheaha","text":"<p>To use RClone for simple data transfer to and from Cheaha, follow these steps:</p> <ol> <li>Load the module using <code>module load rclone</code>.</li> <li>Set up a remote storage provider</li> <li> <p>Copy files using the <code>rclone cp</code> command.</p> <ul> <li>To transfer from Cheaha, use <code>rclone cp :path/on/cheaha remote:path/on/dest</code></li> <li>To transfer to Cheaha, use <code>rclone cp remote:/path/on/remote :path/on/cheaha</code>.</li> </ul> <p>Be sure to replace <code>remote</code> with the name of the remote you set up in step 2.</p> </li> </ol>"},{"location":"data_management/transfer/rclone/#installing","title":"Installing","text":""},{"location":"data_management/transfer/rclone/#installing-on-cheaha","title":"Installing on Cheaha","text":"<p>On Cheaha, RClone is already installed as a Module. Use <code>module load rclone</code> to load it.</p>"},{"location":"data_management/transfer/rclone/#installing-on-linux-and-cloudrc","title":"Installing on Linux and cloud.rc","text":"<p>See Installing Software for general good practice on installing software, then use the following command.</p> <pre><code>curl https://rclone.org/install.sh | sudo bash\n</code></pre> <p>Open a new terminal and enter <code>rclone</code> to verify installation.</p>"},{"location":"data_management/transfer/rclone/#installing-on-windows","title":"Installing on Windows","text":"<p>It is highly recommended to install <code>rclone</code> in Windows Subsystem for Linux (WSL).</p> <p>To instead install natively on Windows, you will need to use the following instructions.</p> <ol> <li>Download the appropriate version from the downloads page.</li> <li>Extract <code>rclone.exe</code> into a memorable folder on your system. Do not put it into <code>Program Files</code>.</li> <li>In the Start Menu type <code>env</code> and look for the application \"Edit the system Environment Variables\" to open the System Properties dialog.</li> <li>Click the \"Environment Variables...\" button.</li> <li>Under \"User variables for $USER\" find the variable \"Path\".</li> <li>Click \"Path\" to select it.</li> <li>Click the \"Edit...\" button to open a new dialog.</li> <li>Click the \"New\" button.</li> <li>Type in the folder path to <code>rclone.exe</code> as <code>C:/path/to/rclone_folder</code>, modified appropriately.</li> <li>Click the \"OK\" button to confirm and close the dialog box.</li> <li>Click the \"OK\" button to confirm and close the Environment Variables dialog box.</li> <li>Click the \"OK\" button to confirm and close the System Properties dialog box.</li> <li>Verify the installation by typing \"cmd\" in the Start Menu and opening the Command Prompt application.</li> <li>Type <code>rclone</code> and you should see the <code>rclone</code> help text.</li> </ol>"},{"location":"data_management/transfer/rclone/#macos","title":"MacOS","text":"<p>Follow the online instructions for installing with <code>brew</code>.</p>"},{"location":"data_management/transfer/rclone/#setting-up-remotes","title":"Setting up Remotes","text":"<p>RClone is capable of interfacing with many remote cloud services, as well as using <code>sftp</code> for connecting two personal computers or servers. We will only cover those cloud services relevant to UAB use. We will not cover how to connect to any other cloud services using RClone. More detailed information is available at the RClone documentation</p> <p>Important</p> <p>Cloud access tokens are always supplied with an expiration date for security reasons. You will need to repeat the setup process periodically to regain access via RClone.</p> <p>Note</p> <p>RClone has an unusual user interface, using alternating red and green blocks to differentiate list items. The colors do not convey any particular meaning beyond differentiation.</p>"},{"location":"data_management/transfer/rclone/#setting-up-an-sftp-remote","title":"Setting up an SFTP Remote","text":"<p>RClone connects two personal computers or servers using SFTP which is built on SSH, so a lot of these instructions mirror what would be done with an SSH configuration.</p> <ol> <li>Generate a Key Pair for use with the remote machine.</li> <li>At the terminal enter <code>rclone config</code>.</li> <li>Follow the prompts to choose <code>sftp</code>.</li> <li>Enter the following values as they come up, using defaults for other values.<ul> <li><code>name&gt;</code> Name of the remote for future reference and configuration</li> <li><code>host&gt;</code> Remote IP address or <code>cheaha.rc.uab.edu</code> for Cheaha</li> <li><code>user&gt;</code> The user you will log into on the remote machine</li> <li><code>key_file&gt;</code> The absolute path to the private key file on the local machine, something like <code>~/.ssh/private_key_ed25519</code></li> <li><code>key_file_pass&gt;</code> The passphrase used to secure the private key file (optional, but highly recommended)</li> </ul> </li> <li>Verify by using <code>rclone lsd &lt;name&gt;</code>.</li> </ol> <p>The official docuemntation for <code>rclone sftp</code> is here.</p>"},{"location":"data_management/transfer/rclone/#setting-up-uab-cloud-remotes","title":"Setting up UAB Cloud Remotes","text":"<p>The setup process for UAB cloud remotes is generally the same, except for the specifics of authentication. The instruction template is outlined below and will point you to the authentication section specific to each remote when it becomes relevant.</p> <p>As you step through the process, you will ultimately open two terminal windows and a browser window, and will need to copy text between the terminal windows. The first terminal window will be used to setup the RClone cloud remote. The second terminal will be used to authenticate to that cloud service and gain a token that will be passed back to the first terminal. Authentication will happen by logging into the service in a browser window. This setup method is necessary for any machine where a browser is not readily available, such as a cloud.rc virtual machine. To facilitate setup on these machines, the second terminal will be opened on a machine with RClone and a browser. An example of what this setup might look like is given below.</p> <p></p> <p>Important</p> <p>If you are using RClone in Windows Subsystem for Linux (WSL), you won't be able to open a browser using WSL. Instead, you will need to Install RClone on Windows and use the Windows Command Prompt terminal to use <code>rclone authorize</code>.</p> <ol> <li>Open a terminal on the device you wish to authorize to access the chosen cloud service provider using RClone. This terminal will be referred to as terminal-1.</li> <li>At terminal-1 enter <code>rclone config</code>.</li> <li>Follow the prompts to choose one of the following. The selection here will be used later and will be referred to as <code>&lt;remote&gt;</code>.</li> <li>UAB Box: select <code>Box</code>. <code>&lt;remote&gt;</code> will be replaced by  <code>box</code>.</li> <li>UAB SharePoint Site: select <code>Microsoft OneDrive</code>. <code>&lt;remote&gt;</code> will be replaced by <code>onedrive</code>.</li> <li>UAB OneDrive: select <code>Microsoft OneDrive</code>. <code>&lt;remote&gt;</code> will be replaced by <code>onedrive</code>.</li> <li>Enter a short, memorable name for future reference when prompted with <code>name&gt;</code>. Keep this <code>&lt;name&gt;</code> in mind as it will be how you access the remote when Using Commands.</li> <li>Press enter to leave all additional prompts blank until \"Use auto config?\". Type \"n\", for no, and press enter.</li> <li>The prompt should now read <code>config_token&gt;</code>.</li> <li>On a machine with a browser, such as your personal computer, open a new terminal and enter <code>rclone authorize \"&lt;remote&gt;\"</code>. Replacing <code>&lt;remote&gt;</code> with the value from step (3). This terminal will be referred to as terminal-2.</li> <li>When the browser window opens, use it to authenticate to your selected service.<ul> <li>Authenticate to UAB Box.</li> <li>Authenticate to Microsoft OneDrive.</li> <li>Other services not officially supported by UAB IT are possible, but are not documented here. You will need to provide your own credentials for these services.</li> </ul> </li> <li> <p>Terminal-2 will print a secret token, which will appear like in the following image. You will need to copy the portion highlighted in the image, between the lines with <code>---&gt;</code> and <code>&lt;---</code>.</p> <p></p> </li> <li> <p>Copy and paste the token from the terminal-2 to terminal-1.</p> </li> <li>Follow the remaining prompts.</li> <li>Verify success by using <code>rclone lsd &lt;name&gt;:</code> in terminal-1.</li> </ol>"},{"location":"data_management/transfer/rclone/#authenticating-to-cloud-remotes","title":"Authenticating to Cloud Remotes","text":""},{"location":"data_management/transfer/rclone/#authenticating-to-uab-box","title":"Authenticating to UAB Box","text":"<ol> <li> <p>Click \"Use Single Sign On (SSO)\".</p> <p></p> </li> <li> <p>Type in your UAB email address (not your @uabmc.edu email!).</p> </li> <li> <p>Click \"Authorize\".</p> <p></p> </li> <li> <p>You will be redirected to the UAB SSO page.</p> </li> <li>Authenticate with your BlazerID credentials.</li> <li> <p>You will be asked to grant permission to the RClone software. Click \"Grant access to Box\" if you want the software to work with Box. If you do not grant permission, you will not be able to use RClone with Box.</p> <p></p> </li> <li> <p>You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token.</p> <p></p> </li> <li> <p>Return to Setting up UAB Cloud Remotes.</p> </li> </ol> <p>Warning</p> <p>Tokens are set to expire after some time of disuse to decrease risk of a data breach. If your token expires, you can Reconnect to an Existing Remote rather than recreate the remote configuration completely from scratch.</p>"},{"location":"data_management/transfer/rclone/#authenticating-to-microsoft-onedrive","title":"Authenticating to Microsoft OneDrive","text":"<ol> <li>Type in your UAB email address (not your @uabmc.edu email!).</li> <li> <p>Click \"Next\".</p> <p></p> </li> <li> <p>If prompted, click \"Work or school account\".</p> <p></p> </li> <li> <p>You will be asked to grant permission to the RClone software. Click \"Accept\" if you want the software to work with OneDrive. If you do not grant permission, you will not be able to use RClone with OneDrive.</p> <p></p> </li> <li> <p>You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token.</p> <p></p> </li> <li> <p>Next you will return to the general instructions. Before you do, note that you'll be asked to choose which type of OneDrive service to access. The prompt will look like the image below. For UAB, the two relevant selections will be (1) to access your personal OneDrive space and (3) for a SharePoint Site, e.g. for a lab or department.</p> <p></p> </li> <li> <p>With your selection in mind, return to Setting up UAB Cloud Remotes.</p> </li> </ol>"},{"location":"data_management/transfer/rclone/#setting-up-an-s3-lts-remote","title":"Setting Up an S3 LTS Remote","text":"<p>The full S3 configuration process can be done from a single command line terminal. Open a terminal and enter <code>rclone config</code> to begin the configuration process.</p> <p>Note</p> <p>The locations where you will need to input either a command or select an option are preceded with a <code>$</code> for easier navigation.</p> <pre><code>$ rclone config\n\n2022/02/22 13:02:15 NOTICE: Config file \"/home/mdefende/.config/rclone/rclone.conf\" not found - using defaults\nNo remotes found - make a new one\nn) New remote\ns) Set configuration password\nq) Quit config\n\n# select 'n' to create a new remote\n$ n/s/q&gt; n\n\n# name the new remote\n$ name&gt; uablts\n</code></pre> <p>At this point, you've created a new remote configuration called uablts. This will be the remote name used in further commands. You can name the remote whatever you would like, but will need to replace uablts in the instructions with whichever name you chose, if you chose a different name.</p> <pre><code>...\n4 / Amazon Drive\n  \\ (amazon cloud drive)\n5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS\n  \\ (s3)\n6 / Backblaze B2\n  \\ (b2)\n...\n\n$ Storage&gt; 5\n\n...\n2 / Alibaba Cloud Object Storage System (OSS) formerly Aliyun\n  \\ (Alibaba)\n3 / Ceph Object Storage\n  \\ (Ceph)\n4 / Digital Ocean Spaces\n  \\ (DigitalOcean)\n...\n\n$ provider&gt; 3\n\nOption env_auth.\nGet AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).\nOnly applies if access_key_id and secret_access_key is blank.\nChoose a number from below, or type in your own boolean value (true or false).\nPress Enter for the default (false).\n 1 / Enter AWS credentials in the next step.\n   \\ (false)\n 2 / Get AWS credentials from the environment (env vars or IAM).\n   \\ (true)\n\n$ env_auth&gt; 1 (or leave blank)\n\nOption access_key_id.\nAWS Access Key ID.\nLeave blank for anonymous access or runtime credentials.\nEnter a value. Press Enter to leave empty.\n\n$ access_key_id&gt; (Enter your access key given to you by research computing)\n\nOption secret_access_key.\nAWS Secret Access Key (password).\nLeave blank for anonymous access or runtime credentials.\nEnter a value. Press Enter to leave empty.\n\n$ secret_access_key&gt; (Enter your secret access key given to you by research computing here)\n\nOption region.\nRegion to connect to.\nLeave blank if you are using an S3 clone and you don't have a region.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n   / Use this if unsure.\n 1 | Will use v4 signatures and an empty region.\n   \\ ()\n   / Use this only if v4 signatures don't work.\n 2 | E.g. pre Jewel/v10 CEPH.\n   \\ (other-v2-signature)\n\n$ region&gt; (Leave empty)\n\nOption endpoint.\nEndpoint for S3 API.\nRequired when using an S3 clone.\nEnter a value. Press Enter to leave empty.\n\n$ endpoint&gt; s3.lts.rc.uab.edu\n</code></pre> <p>From here, press Enter to accept default options until it gives you a summary of your connection</p> <pre><code>[uablts]\ntype = s3\nprovider = Ceph\naccess_key_id = ****************** # these will be filled in on your screen\nsecret_access_key = ********************************\nendpoint = s3.lts.rc.uab.edu\n--------------------\ny) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt;\n</code></pre> <p>Make sure everything looks correct here, then press Enter. At this point, it will bring you back to the main configuration menu. You can choose the <code>Quit Config</code> option and exit back to a basic terminal.</p>"},{"location":"data_management/transfer/rclone/#reconnecting-to-an-existing-remote","title":"Reconnecting to an Existing Remote","text":"<p>When your tokens expire, rather than recreate the remote from scratch, simply use the following command with your existing remote <code>&lt;name&gt;</code>.</p> <pre><code>rclone config reconnect &lt;name&gt;:\n</code></pre> <ul> <li>If you already have a token, you will be asked if you want to refresh it. Choose yes if so, then continue.</li> <li>You will be prompted with <code>Use auto config?</code>. If you are on a machine with no access to a browser, respond <code>n</code>, as in the original setup.</li> <li>Follow the steps in the appropriate section under Authenticating to Cloud Remotes, as in the original setup.</li> </ul>"},{"location":"data_management/transfer/rclone/#usage","title":"Usage","text":"<p>RClone is a powerful tool with many commands available. We will only cover a small subset of the available commands, as most are beyond typical usage, so please see the RClone documentation for more information.</p> <p>All commands have access to the global flags. An important global flag is <code>--dry-run</code> to show what will happen without actually executing the command, which can be helpful to prevent costly mistakes. Other Helpful Global Flags are also available.</p> <p>The various remotes each have their own individual page with their own specific flags, and are linked in the relevant Setting up Remotes section above.</p> <p>Important</p> <p>Remote paths are always prefixed by the name of the remote like <code>cheaha:/path/to/files</code>. The colon character <code>:</code> is required for all remote paths. Local paths have no prefix like <code>/path/to/local/files</code>. RClone can thus be used between any two machines that are configured where <code>rclone</code> is being used, including from the local machine to itself. In the following instructions, replace <code>&lt;remote:&gt;</code> by the appropriate remote name from configuration. To access local files, leave <code>&lt;remote:&gt;</code> off entirely.</p> <p>Important</p> <p>Remember to use quotes <code>\"</code> around paths with spaces like <code>\"path\\to\\a folder with spaces\"</code></p>"},{"location":"data_management/transfer/rclone/#usage-concept","title":"Usage Concept","text":"<p>All <code>rclone</code> commands follow the same general patterns outlined below. <code>source</code> is the name of the source remote and <code>destination</code> is the name of the <code>destination</code> remote. Remotes must be set up before using commands. To work with local files use the format <code>:path/to/data</code> instead of <code>remote:path/to/data</code>. The colon is necessary to access local files.</p> <ul> <li> <p>Single-source commands like <code>ls</code>:</p> <pre><code>rclone ls &lt;flags...&gt; source:path/to/data\n</code></pre> </li> <li> <p>Transfer commands like <code>cp</code>:</p> <pre><code>rclone cp &lt;flags...&gt; source:path/to/data destination:path/to/data\n</code></pre> <p>Source always comes before destination. To change the direction of file transfer, swap the order of <code>source</code> and <code>destination</code>.</p> </li> </ul>"},{"location":"data_management/transfer/rclone/#creating-a-directory","title":"Creating a Directory","text":"<p>To create a directory use <code>rclone mkdir &lt;remote:&gt;&lt;path&gt;</code>.</p> <p>Example: <code>rclone mkdir box:manuscript</code>.</p>"},{"location":"data_management/transfer/rclone/#listing-files-and-directories","title":"Listing Files and Directories","text":"<p>To list files on a machine use <code>rclone ls &lt;remote:&gt;&lt;path&gt;</code>.</p> <p>Example <code>rclone ls box:</code>.</p> <p>To list directories on a machine use <code>rclone lsd &lt;remote:&gt;&lt;path&gt;</code>.</p> <p>Example: <code>rclone lsd box:</code> should show <code>manuscript</code>.</p>"},{"location":"data_management/transfer/rclone/#copying-files","title":"Copying Files","text":"<p>To copy files without changing their name, or to recursively copy directory content, use <code>rclone copy &lt;source:&gt;&lt;path&gt; &lt;destination:&gt;&lt;path&gt;</code>. Note that the directory contents are copied, so when copying a directory, be sure that directory exists on the remote and that you are copying into it.</p> <p>Example: <code>rclone copy \"C:\\users\\Name\\My Documents\" box:manuscript</code></p> <p>To copy a single file and change its name, use <code>rclone copyto &lt;source:&gt;&lt;path/oldname&gt; &lt;destination:&gt;&lt;path/newname&gt;</code>.</p> <p>Example <code>rclone copyto \"C:\\users\\Name\\My Documents\\manuscript.docx\" box:manuscript\\newest.docx</code></p>"},{"location":"data_management/transfer/rclone/#syncing-between-two-devices","title":"Syncing Between Two Devices","text":"<p>To make a destination directory's contents identical to a source directory, use <code>rclone sync &lt;source:&gt;&lt;path&gt; &lt;destination:&gt;&lt;path&gt;</code></p> <p>Example: <code>rclone sync cheaha:\"C:\\users\\Name\\My Documents\" box:manuscript</code>.</p> <p>Danger</p> <p><code>rclone sync</code> is a destructive operation and cannot be undone! If files exist on the destination that do not exist on the source, then they will be deleted permanently from the destination. To avoid accidental destruction of files use the <code>--immutable</code> flag.</p>"},{"location":"data_management/transfer/rclone/#other-helpful-global-flags","title":"Other Helpful Global Flags","text":"<p>Flag used with any RClone command are called global flags. Below are some useful global flags.</p> <ul> <li><code>-C</code> or <code>--checksum</code>: Skip syncing files based on checksum instead of last modified time.</li> <li><code>--dry-run</code>: Show what will happen if the command were executed. No changes are made.</li> <li><code>--immutable</code>: Do not allow any files to be modified. Helpful to avoid unintended deletions and overwrites.</li> <li><code>--max-depth &lt;integer&gt;</code>: Only recurse to <code>&lt;integer&gt;</code> depth within directory tree. Using <code>rclone ls --max-depth 1</code> means only show top-level files in the current directory.</li> <li><code>-P</code> or <code>--progress</code>: Show progress of command as it runs.</li> <li><code>--quiet</code>: Print as little as possible. Useful in scripts.</li> <li><code>-u</code> or <code>--update</code>: Skips files that are newer on the remote.</li> </ul>"},{"location":"data_management/transfer/tutorial/","title":"Data Transfer Tutorials","text":""},{"location":"data_management/transfer/tutorial/#globus","title":"Globus","text":"<p>To learn how to use Globus, everyone should start with the Globus for Individuals Tutorials to become familiar with how to get on Globus and how it works.</p> <p>If you manage a lab or a Research Core on campus, you may also be interested in our Globus for Groups Tutorials. Research Core managers and directors should please Contact Support to start a conversation around Data Management good practices.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/","title":"Globus Tutorials for Research Groups","text":"<p>This tutorial is designed to guide UAB research managers, such as Lab PIs, Core Directors, and their management staff, to help them set up and use Globus Connect Personal (GCP) for secure data sharing on their local/personal computers. GCP allows you to share data with collaborators efficiently meeting security and compliance standards for file sharing.</p> <p>If you are new to Globus, we recommend starting with our Globus Tutorials for Individual Researchers first to familiarize yourself with how Globus is used. When those are complete, we then recommend following the tutorials on this page in order from start to finish, as later tutorials assume the previous tutorials have been completed.</p> <p>Topics covered:</p> <ol> <li>Why Globus?</li> <li>How Does Globus Work?</li> <li>Tutorial Prerequisites</li> <li>How Do I Get onto the Globus Web App?</li> <li>How Do I Install Globus Connect Personal?<ul> <li>Windows</li> <li>MacOS</li> </ul> </li> <li>How Do I Choose Specific Folders to Share Using Globus Connect Personal?<ul> <li>Windows</li> <li>MacOS</li> </ul> </li> <li>How Do I Find Collections I Created or Own?</li> <li>How Do I Enable Sharing for My Globus Account?</li> <li>How Do I Create a Collection?</li> <li>How Do I Share a Collection with Others?</li> <li>How Do I Share Data with a Research Core Customer?</li> </ol>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#why-globus","title":"Why Globus?","text":"<p>Globus is a data transfer ecosystem that simplifies the process of transferring, sharing, and managing large datasets. It is used by research institutions around the world to move research data between different storage devices, computer systems, and institutions. Globus has many research data oriented features, making it ideal for many research data transfer scenarios. Below is a list of features.</p> <ul> <li>Straight-forward, browser-based interface.</li> <li>Compatible with Long-Term Storage.</li> <li>Can be used to share data with Research Core customers.</li> <li>Can be used to transfer data between lab workstations, servers, and Cheaha.</li> <li>Transfers are automatically retried in the event of network or computer system outages.</li> <li>Transfers are encrypted end-to-end. Globus never sees your data.</li> <li>Suitable for transferring PHI and HIPAA data. Note: a UAB Enterprise IT risk assessment is required.</li> </ul>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-does-globus-work","title":"How Does Globus Work?","text":"<p>Globus is an ecosystem of software intended to make research data transfer simpler. The Globus Web Application (Web App) at https://app.globus.org allows you to initiate transfers between any two Collections you have authorization to access. The Globus Connect Personal (GCP) and Globus Connect Server (GCS) software let you turn any computer into a Globus Collection. At no point do Globus servers touch your research data. Instead, when you initiate a transfer between two Collections, the Globus application tells the two Collections that they need to talk to each other and data is sent directly between them. The Collections update the application with information you may need to know, such as how much data has transferred so far, how fast the transfer is proceeding, and any errors that occur. If the connection between Collections is interrupted for any reason, the Globus application will attempt to restart the transfer from where it left off.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#tutorial-prerequisites","title":"Tutorial Prerequisites","text":"<p>For these tutorials, you will need your BlazerID or XIAS ID and password to authenticate using UAB Single Sign-On (SSO).</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-get-onto-the-globus-web-app","title":"How Do I Get onto the Globus Web App?","text":"<p>To learn how to get onto the Globus Web App Globus Tutorials for Individual Researchers Page. Please visit that link and then return here when you have finished.</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-install-globus-connect-personal","title":"How Do I Install Globus Connect Personal?","text":"<p>Globus Connect Personal (GCP) is available to install on the following Operating Systems. Linux is also available, but we do not cover the installation process here. If you need to install GCP on Linux, please refer to the official documentation.</p> <p>Managing data for a Research Core? Please stop here. You should know that Globus Connect Personal is not suitable for production use with customers. Please Contact Support to discuss setting up Globus Connect Server for your Research Core.</p> <p>Representing a lab? Globus Connect Personal may be suitable for your use case. If you transfer data infrequently, or transfer small amounts of data, then GCP is probably sufficient. If you frequently transfer large amounts of data, then Globus Connect Server can enable higher data transfer rates through parallelization. Please Contact Support if you want to discuss installation of Globus Connect Server.</p> <p>GCP installation instructions for:</p> <ul> <li>Windows</li> <li>MacOS</li> </ul> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#installing-gcp-on-windows","title":"Installing GCP On Windows","text":"<p>The following steps will guide you to install Globus Connect Personal (GCP) on your computer running Windows.</p> <ol> <li> <p>Navigate to the Globus Connect Personal official page and scroll down to find the GCP Windows version. Click on the \"INSTALL NOW\" button in the red box, as shown in the image below, to be taken to the official installation instructions and download link for GCP for Windows. The \"INSTALL NOW\" button is located below the text \"Globus Connect Personal for Windows\".</p> <p></p> </li> <li> <p>Click the \"Download Globus Connect Personal\" link on the instructions page, as shown below. This will redirect you to the GCP for Windows installer.</p> <p></p> </li> <li> <p>As shown in the image below, click the \"Download Globus Connect Personal for Windows\" button to download the installer to download the installer to your computer.</p> <p></p> </li> <li> <p>Find the installer on your computer and open it. Select where you would prefer to have your GCP installed and click the \"Install\" button.</p> <p></p> <p>Please note you must have administrator permissions, to do this. If you are unable to do so, you will need to contact the IT group managing your computer.</p> </li> <li> <p>When the installation is complete, click the \"Finish\" button to complete the GCP installation.</p> <p></p> </li> <li> <p>Following installation, GCP will launch in a new window. If it does not, look for it in your Start Menu.</p> <p>When GCP has started, click the \"Log In\" button to authenticate with Globus to begin the Collection setup process. This is a one-time setup to configure GCP to allow your machine to act as a Collection, enabling research data transfer with your computer.</p> <p></p> <p>Note that if you uninstall and reinstall GCP, you will need to complete this process again. You should not need to repeat this process otherwise.</p> </li> <li> <p>Grant the required consents. This is required to set up your computer as a Collection. Also provide a name for your consents. We recommend choosing a name that is short, memorable, and related to the purpose for the Collection.</p> <p></p> </li> <li> <p>Enter the details for your GCP Collection, and click save to continue. The following list describes the fields in the form shown below.</p> <ul> <li>Owner Identity: is the person responsible for this Collection. This field should already be filled with UAB Campus or XIAS email address. If not, please select that email address here.</li> <li>Collection Name: is the name for the Collection. This should be filled with the name of the Collection from the previous step.</li> <li>Description: Feel free to enter descriptive information about the Collection here. This information will be displayed in the Globus Web App when the Collection is viewed by others.</li> <li>High Assurance: Only check this box if the Collection has or will have PHI, HIPAA, or other protected data. If this is the case, please ensure that you have already completed a risk assessment with UAB Enterprise IT.</li> </ul> <p></p> </li> <li> <p>GCP Setup is now complete on your computer. Your computer is now serving your new Globus Collection and may be used to transfer data. Click \"Exit Setup\" to close the window.</p> </li> <li> <p>After installation, locate your Windows System Tray. In it you should see a white lowercase letter \"g\" in a filled blue circle. If you do not, try finding the Globus Connect Personal application in your start menu and starting the application.</p> <p></p> </li> </ol> <p>By default your Documents folder (typically <code>C:/Users/%username%/Documents</code>) is listed in your new Globus Collection. To change it continue on with How Do I Choose Specific Folders Using Globus Connect Personal?</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#installing-gcp-on-macos","title":"Installing GCP On MacOS","text":"<p>The following steps will guide you to install Globus Connect Personal (GCP) on your computer running MacOS.</p> <ol> <li> <p>Navigate to the Globus Connect Personal official page and scroll down to find the GCP MacOS version. Click on the \"INSTALL NOW\" button in the red box in the image below to be taken to the official installation instructions and download link for GCP for MacOS. The \"INSTALL NOW\" button is located below the text \"Globus Connect Personal for Mac\".</p> <p></p> </li> <li> <p>Click the \"Download Globus Connect Personal\" link on the instructions page, as shown below. This will redirect you to the GCP for MacOS installer.</p> <p></p> </li> <li> <p>As shown in the image below, click the \"Download Globus Connect Personal for MacOS\" button to download the installer to download the installer to your computer.</p> <p></p> </li> <li> <p>Find the installer on your computer and open it. A new window will pop-up asking you to drag the Globus Connect Personal app into the Application folder on your computer. Do so to install GCP.</p> <p></p> </li> <li> <p>When the above step is completed navigate to your Application folder and look for the \"Globus Connect Personal\" application. Open it to proceed.</p> <p></p> </li> <li> <p>When GCP has started, click on \"Log In\" to authenticate with Globus to begin the Collection setup process. This is a one-time setup to configure GCP to allow your machine to act as a Collection, enabling research data transfer with your computer.</p> <p></p> <p>Note that if you uninstall and reinstall GCP, you will need to complete this process again. You should not need to repeat this process otherwise.</p> </li> <li> <p>Grant the required consents. This is required to set up your computer as a Collection. Also provide a name for your Collection. We recommend choosing a name that is short, memorable, and related to the purpose for the Collection.</p> <p></p> </li> <li> <p>Enter the details for your GCP Collection, and click save to continue. The following list describes the fields in the form shown below.</p> <ul> <li>Owner Identity: is the person responsible for this Collection. This field should already be filled with UAB Campus or XIAS email address. If not, please select that email address here.</li> <li>Collection Name: is the name for the Collection. This should be filled with the name of the Collection from the previous step.</li> <li>Description: Feel free to enter descriptive information about the Collection here. This information will be displayed in the Globus Web App when the Collection is viewed by others.</li> <li>High Assurance: Only check this box if the Collection has or will have PHI, HIPAA, or other protected data. If this is the case, please ensure that you have already completed a risk assessment with UAB Enterprise IT.</li> </ul> <p></p> </li> <li> <p>GCP Setup is now complete on your computer. Your computer is now serving your new Globus Collection and may be used to transfer data. Click \"Exit Setup\" to close the window.</p> </li> <li> <p>After installation locate your MacOS notification, generally at the top-right of your display. In it you should see a white lowercase letter \"g\" in a fille black circle.</p> <p></p> </li> </ol> <p>By default the <code>/Users/</code> folder is listed in your new Globus Collection. To change it continue on with How Do I Share Specific Folders Using Globus Connect Personal?</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-choose-specific-folders-to-share-using-globus-connect-personal","title":"How Do I Choose Specific Folders to Share Using Globus Connect Personal?","text":"<p>Please follow the instructions in this section to share a folder on your computer with others through your Globus Connect Personal (GCP) Collection.</p> <p>We have instructions for the following Operating Systems.</p> <ul> <li>Windows</li> <li>MacOS</li> </ul> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#choose-specific-folders-on-windows","title":"Choose Specific Folders on Windows","text":"<ol> <li> <p>In your Windows system tray, locate the icon that looks like a small letter \"g\" in a circle. This is the icon for Globus Connect Personal. If you cannot locate the icon in the system tray, then open the Globus Connect Personal app on your computer and look for it again.</p> <p></p> </li> <li> <p>Right-click the icon to open the context menu and click \"Options...\".</p> <p></p> </li> <li> <p>A new window will appear with a tab labelled Access. In the Access tab is an interface to configure folders available on your GCP Collection. For most use cases, you should not check the writeable checkbox. Below is a summary of what each part of the menu does.</p> </li> <li> <p>(1) Accessible Folders table with Folder, Shareable and Writeable columns. Any folder listed here will appear on your GCP Collection. Your research data folder or directories must be listed here to be shareable.</p> </li> <li>(2) Shareable column checkboxes controlling which folders can be shared with other users. Each of your research data directories must have this checkbox ticked to be shareable from the Collection. Check this box only if you want to share your data with others.</li> <li>(3) Writeable column checkboxes controlling which folders can be written to by other users. If a folder is shared with other users, then they will be able to add, delete, or change the contents. We recommend against ticking these boxes for Research Cores serving data to customers. Check this box only if you want others to be able to change your data.</li> <li>(4) Plus <code>+</code> and minus <code>-</code> buttons that allow you to add or remove folders from the list.</li> <li> <p>(5) Save button which saves changes made to this tab of the options.</p> <p></p> </li> <li> <p>Use the plus <code>+</code> and minus <code>-</code> buttons to add your research data folders and remove other folders, as needed. Click the \"Shareable\" checkbox next to each research data folder. Click \"Save\" when finished.</p> <p>In this example, we removed the default <code>C:/Users/%username%/Documents</code> folder with the minus <code>-</code> button and added the <code>D:/data</code> folder with the <code>+</code> button and check the \"Shareable\" box. You will want to pick the folder where your research data is stored.</p> <p></p> </li> <li> <p>Click the \"General\" tab. The \"General\" tab enables you to control some settings for the application itself and which folder is the default folder. The default folder will be the first one shown when accessing the Collection.</p> </li> <li> <p>(1) Run when Windows starts checkbox enabling starting Globus Connect Personal when you start Windows. Check this box if GCP should always be on when the computer is on.</p> </li> <li>(2) Home Folder text field that lets you choose which folder will be the default folder for your Collection. We recommend setting this to your primary shared folder from the previous step to simplify navigating your Collection in the Globus Web App.</li> <li> <p>(3) Save button which saves changes made to this tab of the options. Be sure to click \"Save\" if you make changes here.</p> <p></p> </li> <li> <p>Check \"Run when Windows starts\" if needed. Change the \"Home Folder\" to match your research data folder. Click \"Save\" when done.</p> <p>In this example, we set the \"Home Folder\" to match the research data folder, <code>D:/data</code> we added in a previous step. If you have multiple research directories to share, you will need to choose just one for this field. Be sure to click save when you are done.</p> <p></p> </li> </ol> <p>To verify the existence and accessibility of your Collection proceed to How Do I Find Collections I Created or Own?</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#choose-specific-folders-on-macos","title":"Choose Specific Folders on MacOS","text":"<ol> <li> <p>In your MacOS notification area, locate the icon that looks like a small letter \"g\" in a circle. This is the icon for Globus Connect Personal. If you cannot locate the icon in the notification area, then open the Globus Connect Personal app on your computer and look for it again.</p> <p></p> </li> <li> <p>Right-click or command-click the icon to open the context menu. Click \"Preferences\u2026\u200b\".</p> <p></p> </li> <li> <p>A new window will appear with a tab labelled \"Access\". Click the \"Access\" tab if it is not already selected. In this \"Access\" tab is an interface to configure folders available on your GCP Collection. For most use cases, you should not check the writeable checkbox. Below is a summary of what each part of the menu does.</p> <ul> <li> <p>(1) Accessible Directories and Files table with \"Directory or File\", Shareable and Writeable columns. Any folder listed here will appear on your GCP Collection. Your research data folder or directories must be listed here to be shareable.</p> <p>Note</p> <p>The terms Directories and Folders are synonyms here.</p> </li> <li> <p>(2) Shareable column checkboxes controlling which folders can be shared with other users. Each of your research data directories must have this checkbox ticked to be shareable. Check this box only if you want to share your data with others.</p> </li> <li>(3) Writeable column checkboxes controlling which folders can be written to by other users. If a folder is shared with other users, then they will be able to add, delete, or change the contents. We recommend against ticking these boxes for Research Cores serving data to customers. Check this box only if you want others to be able to change your data.</li> <li>(4) Plus <code>+</code> and minus <code>-</code> buttons that allow you to add or remove folders from the list.</li> </ul> <p></p> </li> <li> <p>Use the plus <code>+</code> and minus <code>-</code> buttons to add your research data folders and remove other folders, as needed. Click the \"Shareable\" checkbox next to each research data folder. Click \"Save\" when finished.</p> </li> </ol> <p>To verify the existence and accessibility of your Collection proceed to How Do I Find Collections I Created or Own?</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-find-collections-i-created-or-own","title":"How Do I Find Collections I Created or Own?","text":"<p>To find a Collection you own, use the following steps.</p> <ol> <li> <p>Navigate to the Globus Web App using your browser. You should be at the File Manager page.</p> <p></p> </li> <li> <p>Click either of the Collection Search bar at the top of the File Manager page. This will take you to the Collection Search page.</p> <p></p> </li> <li> <p>Click the Your Collections tab to display a list of Collections you have created or own.</p> <p></p> </li> <li> <p>From here there are two options:</p> <ol> <li> <p>Click the name of the Collection to select it for a file transfer and be taken back to the File Manager page.</p> <p></p> </li> <li> <p>Click the three dots icon at the right side of the entry to be taken to the Collection details page.</p> <p></p> </li> </ol> </li> </ol> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-enable-collection-sharing-for-my-globus-account","title":"How Do I Enable Collection Sharing For My Globus Account?","text":"<p>Before you can share Collections from your Globus Connect Personal (GCP) Collection with others, you must do a one-time setup for your account. You will need to join the \"University of Alabama at Birmingham (HA)\" (UAB HA) subscription group. Sharing any Collection requires a paid subscription with Globus. UAB Research Computing has a subscription, but Globus does not know your BlazerID is part of our subscription until you join the subscription group. So, Globus also does not know the GCP Collection you created is part of our subscription. By joining our UAB HA group, you and your GCP Collection are confirmed to be part of our subscription, and you can share Collections from the GCP Collection.</p> <p>To join the UAB HA group, we need to receive both a UAB support request, and a request to join the UAB HA group within the Globus Web App. We need both because we sometimes get spam applications through Globus. Having a Support Request helps us filter the spam. Please use the following steps to join.</p> <ol> <li> <p>Submit a Support Request. In the request please include the following.</p> <ul> <li>Your BlazerID.</li> <li>The text \"Please add me to the Globus UAB HA subscription group.\"</li> <li>The reason you need to be able to share a Collection in Globus. For Research Cores, this would be to share data with your customers.</li> </ul> </li> <li> <p>In your browser get onto the Globus Web App.</p> </li> <li> <p>In the left hand navigation panel click \"Groups\" to be taken to the Groups page.</p> <p></p> </li> <li> <p>Ensure the \"MY GROUPS\" box is unchecked. In the \"Filter groups\" search bar enter \"University of Alabama at Birmingham\" to locate the \"University of Alabama at Birmingham (HA)\" (UAB HA) group.</p> <p></p> </li> <li> <p>Click on the UAB HA group name to be taken to the group details page. The Overview tab should be selected.</p> <p></p> </li> <li> <p>Click on the \"Join this Subscription\" button to be taken to the form to submit a request to join.</p> <p></p> </li> <li> <p>Fill in the form fields and click the \"Submit Application\" button when completed. This will send you to a page notifying you that your membership is pending. A request has been sent to Research Computing, so please wait until you see a reply in the support request.</p> <p></p> </li> <li> <p>When your membership has been accepted, you can verify by returning to the Globus Web App and navigating to the Groups page. Check the \"MY GROUPS\". You should see \"University of Alabama at Birmingham (HA)\" listed with a green, circled checkmark and the word \"ACTIVE\".</p> <p></p> </li> </ol> <p>When you have completed the steps in this section, proceed to creating a Collection.</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-create-a-collection","title":"How Do I Create a Collection?","text":"<p>There are three ways to create a Collection.</p> <ul> <li>Start from an existing Collection you created or own.</li> <li>Install Globus Connect Personal and share a folder to create a Collection.</li> <li>Install and configure one or more Collections with Globus Connect Server. To do this, please Contact Support to start a discussion.</li> </ul> <p>The instructions below assume you are starting from an existing Collection. The instructions will work to create a subset of your Globus Connect Personal Collection.</p> <ol> <li>Get onto the Globus Web App.</li> <li>Find the existing Collection and access its details page.</li> <li> <p>Click the \"Collection\" tab.</p> <p></p> </li> <li> <p>Click the \"+ Add Guest Collection\" button to be taken to a form to create the new Collection.</p> <p></p> </li> <li> <p>Fill out the form.</p> <ul> <li>(1) You Are Sharing: Shows which Collection you will be sharing all or part of. Helpful to verify you are sharing the right Collection</li> <li>(2) Path: Which path you wish to share. By default, the \"/\" path is the root of the original Collection. This may not be appropriate for your use case. Generally, when creating one Collection from another, you will want to pick a subfolder of the original Collection. Do this with the \"Browse\" button.</li> <li>(3) Display Name: The name people will see when accessing this Collection, and what they will search for when looking for the Collection.</li> <li>(4) Description: An optional description for the Collection.</li> <li>(5) Keywords: Optional keywords to help locating the Collection. May be useful for broadly or publicly shared data.</li> </ul> <p></p> </li> <li> <p>Click the \"Create Guest Collection\" button to create the Collection. You will be taken to the details page of the new Collection, on the Permissions tab.</p> <p></p> </li> </ol> <p>When you have created a Collection, you are ready to share the Collection with others.</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-share-a-collection-with-others","title":"How Do I Share a Collection with Others?","text":"<p>Before sharing a Collection with others, you will need to first create a Collection you administer. If the Collection is a Globus Connect Personal Collection, then you will also need to enable Collection sharing for your Globus account. If these prerequisites have been met, then you are ready to setup a Collection to be shared with others. Please follow the instructions below.</p> <ol> <li>Get onto the Globus Web App.</li> <li>Find the Collection you wish to share. In this case we are looking for the \"customer-share\" Collection.</li> <li> <p>On the \"customer-share\" Collection details page, click the Permissions tab.</p> <p></p> </li> <li> <p>Click the \"Add Permissions \u2013 Share With\" button.</p> <p></p> </li> <li> <p>Fill in the form.</p> <ul> <li>Path defaults to the root of the Collection. This may or may not be acceptable for your use case. Be sure to limit access to only the data that needs to be shared. Use the file browser available in the form to find the correct level of access. For now grant permission to the default.</li> <li>Share With: radio buttons to provide control over who to share with. In almost all cases, you will want to share with a single user, which is what we will show here.<ul> <li>User: Use the \"User\" search bar to find the specific user you wish to share with. If they have never accessed Globus before, they will not appear in the search results. You can safely enter their email address to add them anyway. For now grant permission to yourself for the purposes of experimenting.</li> </ul> </li> <li>Email Notification checkbox. We recommend sending an email notification as a convenience.<ul> <li>To entry field: who to send the email to. We recommend the same person as the \"User\" selected earlier. There may be future cases where you want to notify others, such as a supervisor, as well.</li> <li>Message text entry field: the optional content to send in the email message.</li> </ul> </li> <li>Permissions The \"read\" permission must be granted, as that is the point of sharing the Collection. You may additionally give \"write\" permission to create a two-way collaboration. We recommend Research Cores not grant \"write\" permission. If you are using a Globus Connect Personal Collection, then \"write\" permission requires you to correctly configure your Collection to make your shared folder writable.</li> </ul> <p></p> </li> <li> <p>Click the \"Add Permission\" button to grant permission. You should see a notification confirming the permissions granted. At this point permissions have been granted and the Collection is shared with another person. If you need to add more people, click the \"Add another Permision\" button and repeat the process. Otherwise click \"Done\". For now click \"Done\".</p> <p></p> </li> <li> <p>When you click \"Done\" you should be taken back to the Permissions tab of the Customer Share page. You should see a new entry with \"Path: /\". If you click the drop-down arrow you will see yourself listed with \"Read\" permission. If you need to revoke permissions, return to this page and click the icon that looks like a trash can.</p> <p></p> </li> </ol> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_group_tutorial/#how-do-i-share-data-with-a-research-core-customer","title":"How Do I Share Data With a Research Core Customer?","text":"<p>If you manage data for a Research Core, please Contact Support to start a conversation. The answer to this question is currently being developed, so we will need to work together to find the ideal solution.</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/","title":"Globus Tutorials for Individual Researchers","text":"<p>Do you have data and need to move it to a different computer system? Want to use a graphical interface to do it? Want a service that will attempt to resume interrupted transfers? Want enforced encryption for your transfers? Then Globus is right for you.</p> <p>If you are new to Globus, you are in the right place. If you represent a group looking to share data with others we recommend following these tutorials with our Globus Tutorials for Research Groups.</p> <p>These tutorials are intended for individual researchers who need to move data from one location to another. If you manage a Research Core or lab and have never used Globus before, you'll want to start on this page. Then you will want to proceed to Globus Tutorials for Research Groups.</p> <p>The tutorials below will teach you how to effectively use Globus for managing and transferring research data. We will explore what Globus is, why you might use it, how it works, and we'll walk you through the essential steps to get started. You will learn how to set up your Globus account, access the Globus application, find Collections shared with you, and search for Collections by name.</p> <p>Topics covered:</p> <ol> <li>Why Globus?</li> <li>How Does Globus Work?</li> <li>Tutorial Prerequisites</li> <li>How Do I Get Onto the Globus Web App?</li> <li>How Do I Search for Collections by Name?</li> <li>How Do I Find UAB Storage Collections?</li> <li>How Do I Find Collections Shared with Me?</li> <li>How Do I Transfer between a Collection and Cheaha?</li> <li>How Do I Transfer between a Collection and LTS?</li> <li>How Do I Transfer between LTS and Cheaha?</li> <li>How Do I Check Transfer Status?</li> </ol>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#why-globus","title":"Why Globus?","text":"<p>Globus is a data transfer ecosystem that simplifies the process of transferring, sharing, and managing large datasets. It is used by research institutions around the world to move research data between different storage devices, computer systems, and institutions. Globus has many research data oriented features, making it ideal for many research data transfer scenarios. Below is a list of features.</p> <ul> <li>Straight-forward, browser-based, graphical interface.</li> <li>Compatible with UAB Box.</li> <li>Compatible with Long-Term Storage.</li> <li>Can be used with your laptop, desktop, or lab workstation via Globus Connect Personal (GCP).</li> <li>Transfers are automatically retried in the event of network or computer system outages.</li> <li>Transfers are encrypted end-to-end. Globus never sees your data.</li> <li>Suitable for transferring PHI and HIPAA data. Note: a UAB Enterprise IT risk assessment is required.</li> </ul>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-does-globus-work","title":"How Does Globus Work?","text":"<p>Globus is an ecosystem of software intended to make research data transfer simpler. The Globus Web Application (Web App) at https://app.globus.org allows you to initiate transfers between any two Collections you have authorization to access. The Globus Connect Personal (GCP) software lets you turn any computer into a Globus Collection. At no point do Globus servers touch your research data. Instead, when you initiate a transfer between two Collections, the Globus application tells the two Collections that they need to talk to each other and data is sent directly between them. The Collections update the application with information you may need to know, such as how much data has transferred so far, how fast the transfer is proceeding, and any errors that occur. If the connection between Collections is interrupted for any reason, the Globus application will attempt to restart the transfer from where it left off.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#tutorial-prerequisites","title":"Tutorial Prerequisites","text":"<p>For these tutorials, you will need your BlazerID or XIAS ID and password to authenticate using UAB Single Sign-On (SSO).</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-get-onto-the-globus-web-app","title":"How Do I Get Onto the Globus Web App?","text":"<ol> <li> <p>Use your browser to navigate to https://app.globus.org. You should see a login page like below.</p> <p></p> </li> <li> <p>To login, first you must find and select our institution. Type \"UAB\" or \"University of Alabama at Birmingham\" into the search bar to locate UAB in the list. The image below shows the correct choice in a red box.</p> <p>Note</p> <p>If you are an external collaborator using a XIAS account (this is uncommon) to interact with UAB-owned storage you will still need to search for \"UAB\". Do not use your home institution login to access UAB storage systems, as you will only have access to UAB storage with your XIAS credentials.</p> <p></p> </li> <li> <p>Select \"University of Alabama at Birmingham\" from the drop-down menu.</p> <p></p> </li> <li> <p>Click \"Continue\" to be taken to the UAB Single Sign-On (SSO) form. Enter your BlazerID and password in the SSO form, then click \"Log In\". Complete the login process as usual.</p> <p></p> </li> <li> <p>You should be taken to the File Manager page of the Globus Web App. We will be revisiting this page frequently throughout the tutorials. We highly recommend taking some time to familiarize yourself with the page as you proceed. The next few steps outline the important features of the File Manager page and its purpose.</p> <p></p> </li> <li> <p>Recall that Globus is a data transfer application. A data transfer means moving data between two computers: source and destination. As we learn more about Globus, data stored on the source and destination will be visible on the File Manager page in the panels in the screenshot above.</p> <p>By default, only one panel is visible. We recommend selecting two-panel view mode for improved ease of use. Select two-panel view mode by clicking the button located near the top right corner, as shown in the red box below. Of the three available view mode buttons, the two-panel view mode button is in the center.</p> <p>From here on, tutorials will assume you are using two-panel view mode when we refer to the File Manager page.</p> <p></p> </li> <li> <p>Now that two-panel view mode has been selected, you will see two sets of features and panels side-by-side. Most of these features will be used extensively when using Globus.</p> <ul> <li>(1) Collection Search bar: Clicking here will open the Collection Search page, allowing you to find Collections of data shared by others, or find your own Collections. The next step will demonstrate features of the Collection Search page.</li> <li>(2) Path text field: After a Collection has been selected using (1), this text field allows you to enter the path to a specific folder, updating the files and folders shown in (5).</li> <li>(3) Start Transfer button: When you have selected files and folders in (5), use this button to start transferring.</li> <li>(4) Transfer Options drop down menu: Various options for the transfer are available in this menu. These options are not commonly needed and are not used in the tutorials.</li> <li>(5) File Browser panel: After a Collection has been selected in (1), a list of files and folders will appear here for the path shown in (2). You may navigate the Collection here like you would on your operating system.</li> <li>(6) Navigation panel: The blue bar at the left hand side of the Globus Web App is the navigation panel. It will be referred to and used many times throughout these tutorials.</li> </ul> <p></p> </li> <li> <p>To better prepare you for what to expect, the screenshot below shows the File Manager page with an example Collection selected, called \"my-gcp-collection\". This example shows a path in the Path text field and contents in the File Browser panel.</p> <ul> <li>(1) Select All checkbox: Select or unselect all files in the currently displayed folder.</li> <li>(2) Go Up One Level button: Go to the folder containing the currently displayed folder.</li> <li>(3) Refresh button: Refreshes the view of the currently displayed folder.</li> <li>(4) File Selection checkbox: If checked, then the file is selected for transfer. This file is selected.</li> <li>(5) Folder Selection checkbox: If checked, then the folder is currently selected for transfer. This folder is not selected.</li> </ul> <p></p> </li> </ol> <p>The File Manager page will be your most frequently-visited page when using Globus for data transfers. It is central to usage of the Globus Web Application. Please take some time to familiarize yourself with its look and feel. As you progress in the tutorials, please take time to experiment with transferring data to better understand how the interface works. Feel free to return here for guidance.</p> <p>From here you can proceed to How Do I Search for Collections by Name?</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-search-for-collections-by-name","title":"How Do I Search For Collections By Name?","text":"<p>Please follow these instructions to search for a specific Collection by name.</p> <ol> <li>Get onto the Globus Web App.</li> <li> <p>You should see the File Manager page. If not click File Manager in the left-hand navigation panel.</p> <p></p> </li> <li> <p>Click the Collection Search bar to open the Collection Search page.</p> <p></p> </li> <li> <p>In the example below, we typed \"uab box\" to search for the UAB Box Collection, which connects to UAB's Box.com service.</p> <p></p> </li> <li> <p>Click the name of the Collection to be taken back to the file manager page with the Collection filled in.</p> <p></p> </li> </ol> <p>If you can't find a particular Collection this way, but know it was shared with you, try finding Collections shared with me. Proceed on to learn how to find UAB storage Collections.</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-find-uab-storage-collections","title":"How Do I Find UAB Storage Collections?","text":"<p>UAB offers multiple storage resources. The following resources may be accessed through Globus Collections.</p> <ul> <li>Cheaha file system (GPFS) including individual directories and project directories.</li> <li>Long-Term Storage (LTS).</li> <li>UAB Box.</li> </ul> <p>To find these resources on Globus, use the following steps.</p> <ol> <li>Get onto the Globus Web App.</li> <li> <p>You should see the File Manager page. If not click File Manager in the left-hand navigation panel.</p> <p></p> </li> <li> <p>Click the Collection Search bar to open the Collection Search page.</p> <p></p> </li> <li> <p>In the search bar, type one of the following, depending on which resource you need, and select the appropriate entry.</p> <ul> <li> <p>Cheaha Filesystem (GPFS): Type \"UAB Cheaha\". There are two Collections, choose one based on where the other computer is located.</p> <ul> <li>(1) Transferring with a computer on the UAB Campus Network or UAB Wifi? Select \"Cheaha cluster on-campus (UAB Science DMZ)\".</li> <li>(2) Transferring with a computer on other networks? select \"Cheaha cluster off-campus (UAB Science DMZ)\".</li> </ul> <p></p> </li> <li> <p>Long-Term Storage (LTS): Type \"UAB LTS\" and select the entry labeled \"UAB Research Computing LTS (Long Term Storage aka S3)\".</p> <p></p> </li> <li> <p>UAB Box: Type \"UAB Box\" and select the entry labeled \"UAB Box\".</p> <p></p> </li> </ul> </li> </ol> <p>Proceed on to learn how to find Collections shared with you.</p> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-find-collections-shared-with-me","title":"How Do I Find Collections Shared with Me?","text":"<p>Globus allows users to share Collections with others. Other researchers, labs, and Research Cores on campus may invite you to their Collections to share data with you. The following instructions let you view Collections shared with you.</p> <ol> <li>Log in to Globus using your credentials</li> <li> <p>You should see the File Manager page. If not click File Manager in the left-hand navigation panel.</p> <p></p> </li> <li> <p>Click the Collection Search bar to open the Collection Search page.</p> <p></p> </li> <li> <p>Click the Shared With You tab. The list of Collections will be filtered down to all Collections others have granted you access to, which should help you find the Collections you need.</p> <p></p> </li> <li> <p>Click the name of the Collection to be taken back to the file manager page with the Collection filled in.</p> <p></p> </li> </ol> <p>If you can't find a particular Collection this way, but know its name, try searching for Collections.</p> <p>Proceed on to learn how to transfer between Collections.</p> <ul> <li>Between a Collection and Cheaha</li> <li>Between a Collection and LTS</li> <li>Between LTS and Cheaha</li> </ul> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-transfer-between-a-collection-and-cheaha","title":"How Do I Transfer between a Collection and Cheaha?","text":"<p>Please use the following instructions to transfer data from a shared Collection to Cheaha GPFS. These instructions may also be used for other buckets on LTS, provided you have access and know their file path.</p> <p>The direction of transfer may also be reversed to transfer data from Cheaha to the shared Collection. Note that some shared Collections may not allow you to transfer data back, such as some of those provided by Research Cores.</p> <p>These instructions can be generalized to any two Collections you have access to on Globus.</p> <ol> <li> <p>Log in to Globus using your credentials.</p> </li> <li> <p>Find a shared Collection by filtering or by searching in the left side Collection Search bar.</p> </li> <li> <p>Once you have selected the shared Collection you wish to transfer data from, repeat the process to search for a Cheaha Collection in the Collection Search bar on the right side of the file manager page. Search for \"Cheaha cluster\" to find them.</p> <p>Pay close attention in choosing which of the two you need. Choosing incorrectly could lead to slow transfers. Answer the following questions to help you decide. Is the first Collection you selected...</p> <ul> <li>...part of a lab or Research Core on campus? Select \"on-campus\".</li> <li>...on a computer on the UAB Campus Network or UAB Wifi or the UAB VPN? Select \"on-campus\".</li> <li>...at a different institution? Select \"off-campus\".</li> <li>...on a computer on a home network? Select \"off-campus\".</li> </ul> </li> <li> <p>When you select a Cheaha Collection, or any other High Assurance (HA) Collection or Collection, you will be prompted to re-authenticate. Click the Continue button to do so, then select your UAB email address.</p> <p></p> <p></p> </li> <li> <p>At this point, your file manager page should look something like the following image. At this point, both Collection Search bars should have a Collection or Collection filled in. The left side should be the Collection you wish to transfer from. The right side should be a Cheaha Collection. You should see files and folders on both sides.</p> <p></p> </li> <li> <p>Locate the source path on the shared Collection side. Either type the path into the Path field manually, or use the graphical selection field to click on folder names to navigate the filesystem.</p> </li> <li> <p>Repeat the process on the Cheaha Collection side to locate the destination path on the Cheaha Collection side.</p> </li> <li> <p>Select the file and folders you wish to transfer on the shared Collection side. Do so by clicking the checkboxes next to the file and folder names.</p> <p></p> </li> <li> <p>To start the transfer, click the \"Start\" button on the side you made your selections. A transfer will be started and you should see a green toast notification at the upper-right corner of the web page. Press the \"X\" button to dismiss the notification or click \"View Details\" to be taken to the Activity page to see more details about the transfer.</p> <p></p> </li> </ol> <p>From here you can proceed to other related tutorials to initiate other transfers or return to the index.</p> <ul> <li>How Do I check transfer status?</li> <li>How Do I Transfer Between a Collection and LTS?</li> <li>How Do I Transfer Between LTS and Cheaha?</li> </ul> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-transfer-between-a-collection-and-lts","title":"How Do I Transfer between a Collection and LTS?","text":"<p>Please use the following instructions to transfer data from a shared Collection to a bucket on LTS. These instructions may also be used for other directories on Cheaha, provided you have access and know their file path.</p> <p>The direction of transfer may also be reversed to transfer data from LTS to the shared Collection. Note that some shared Collections may not allow you to transfer data back, such as some of those provided by Research Cores.</p> <p>These instructions can be generalized to any two Collections or Collections you have access to on Globus.</p> <ol> <li> <p>Log in to Globus using your credentials.</p> </li> <li> <p>Find a shared Collection by filtering or by searching in the left side Collection Search bar.</p> </li> <li> <p>Once you have selected the shared Collection you wish to transfer data from, repeat the process to search for the LTS Collection in the Collection Search bar on the right side of the file manager page. Search for \"UAB LTS\" to find it.</p> </li> <li> <p>When you select the LTS Collection, or any other High Assurance (HA) Collection or Collection, you will be prompted to re-authenticate. Click the \"Continue\" button to do so, then select your UAB email address.</p> <p></p> <p></p> </li> <li> <p>At this point, your file manager page should look something like the following image. Both Collection Search bars should have a Collection or Collection filled in. The left side should be the Collection you wish to transfer from. The right side should be the Cheaha Collection. You should see files and folders on both sides.</p> <p></p> </li> <li> <p>Locate the source path on the shared Collection side. Either type the path into the Path field manually, or use the graphical selection field to click on folder names to navigate the filesystem.</p> </li> <li> <p>Repeat the process on the LTS Collection side to locate the destination path on the LTS Collection side.</p> </li> <li> <p>Select the file and folders you wish to transfer on the shared Collection side. Do so by clicking the checkboxes next to the file and folder names.</p> <p></p> </li> <li> <p>To start the transfer, click the \"Start\" button on the side you made your selections. A transfer will be started and you should see a green toast notification at the upper-right corner of the web page. Press the \"X\" button to dismiss the notification or click \"View Details\" to be taken to the Activity page to see more details about the transfer.</p> <p></p> </li> </ol> <p>From here you can proceed to other related tutorials to initiate other transfers or return to the index.</p> <ul> <li>How do I check transfer status?</li> <li>How Do I Transfer Between a Collection and Cheaha?</li> <li>How Do I Transfer Between LTS and Cheaha?</li> </ul> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-transfer-between-lts-and-cheaha","title":"How Do I Transfer between LTS and Cheaha?","text":"<p>Please use the following instructions to transfer data from a bucket on LTS to a project directory on Cheaha. These instructions may also be used for other buckets on LTS and directories on Cheaha, provided you have access and know their file paths.</p> <p>The direction of transfer may also be reversed to transfer data from Cheaha to LTS.</p> <p>These instructions can be generalized to any two Collections or Collections you have access to on Globus.</p> <ol> <li> <p>Log in to Globus using your credentials.</p> </li> <li> <p>Find the LTS Collection by searching in the Collection Search bar on the left side of the file manager page. Search for \"UAB LTS\" to find it.</p> </li> <li> <p>Once you have selected the LTS Collection, repeat the process to search for a Cheaha Collection in the Collection Search bar on the right side of the file manager page. Search for \"Cheaha cluster\" to find it.</p> <p>Pay close attention in choosing which of the two you need. Choosing incorrectly could lead to slow transfers. Answer the following questions to help you decide. Is the first Collection you selected...</p> <ul> <li>...part of a lab or Research Core on campus? Select \"on-campus\".</li> <li>...on a computer on the UAB Campus Network or UAB Wifi or the UAB VPN? Select \"on-campus\".</li> <li>...at a different institution? Select \"off-campus\".</li> <li>...on a computer on a home network? Select \"off-campus\".</li> </ul> </li> <li> <p>Both the LTS and Cheaha Collections are High Assurance (HA) Collections and you will be prompted to re-authenticate. Click the \"Continue\" button to do so, then select your UAB email address.</p> <p></p> <p></p> </li> <li> <p>At this point, your file manager page should look something like the following image. Both Collection Search bars should have a Collection or Collection filled in. The left side should be the LTS Collection. The right side should be a Cheaha Collection. You should see files and folders on both sides.</p> <p></p> </li> <li> <p>Locate the source path on the LTS Collection side. Either type the path into the Path field manually, or use the graphical selection field to click on folder names to navigate the filesystem.</p> </li> <li> <p>Repeat the process on the Cheaha Collection side to locate the destination path on the Cheaha Collection side.</p> </li> <li> <p>Select the file and folders you wish to transfer on the shared Collection side. Do so by clicking the checkboxes next to the file and folder names.</p> <p></p> </li> <li> <p>To start the transfer, click the \"Start\" button on the side you made your selections. A transfer will be started and you should see a green toast notification at the upper-right corner of the web page. Press the \"X\" button to dismiss the notification or click \"View Details\" to be taken to the Activity page to see more details about the transfer.</p> <p></p> </li> </ol> <p>From here you can proceed to other related tutorials to initiate other transfers or return to the index.</p> <ul> <li>How do I check transfer status?</li> <li>How Do I Transfer Between a Collection and Cheaha?</li> <li>How Do I Transfer Between a Collection and LTS?</li> </ul> <p>Return to the top.</p>"},{"location":"data_management/transfer/tutorial/globus_individual_tutorial/#how-do-i-check-transfer-status","title":"How Do I Check Transfer Status?","text":"<p>To check the status of your transfers, please follow the instructions below.</p> <ol> <li> <p>Log in to Globus using your credentials.</p> </li> <li> <p>In the Globus Web App, click \"Activity\" in the left navigation panel to go to the Activity page. There will be a list overview of transfers, with the most recent at the top.</p> <p></p> </li> <li> <p>To see more details about a transfer, click the transfer title. There will be two tabs. The Overview tab will have information and statistics about the transfer. The Event Log tab will have information about events that occurred during transfer, including start, stop, and any errors. The Event Log is useful for diagnosing issues with failed transfers.</p> <p></p> </li> </ol> <p>Return to the top.</p>"},{"location":"education/case_studies/","title":"Case Studies","text":""},{"location":"education/case_studies/#parabricks-for-performing-gpu-accelerated-genome-sequencing-analysis","title":"Parabricks for Performing GPU-accelerated Genome Sequencing Analysis","text":"<p>A GPU-accelerated genome sequencing analysis with high speedup and more accurate results can be achieved with NVIDIA Clara Parabricks. Pararbricks is a software suite for genomic analysis. Parabricks delivers accelerated analysis of next generation sequencing (NGS) data for researchers, RNA-seq, population studies, and many more usecases. More insights on its performance can be found here.</p> <p>For more information on Cheaha GPUs, please see our GPU Page</p>"},{"location":"education/case_studies/#licensing-policy","title":"Licensing Policy","text":"<p>A license is no longer required to use Clara Parabricks 4.x and later versions, and is free for the following groups,</p> <ol> <li>Academic Research.</li> <li>Research by non-profit institutions.</li> <li>For development, test and evaluation purposes without use in production.</li> </ol>"},{"location":"education/case_studies/#minimum-hardware-requirements-to-run-parabricks-on-cheaha-gpus","title":"Minimum Hardware requirements to run Parabricks on Cheaha GPUs","text":"<ol> <li>Access to the internet.</li> <li>Any GPU that supports CUDA architecture/compute capability 7.0, 7.5, 8.0, 8.6, 8.9 or 9.0.</li> <li>The GPU has 16 GB of GPU RAM or more. It has been tested on NVIDIA V100, NVIDIA A100, and NVIDIA T4 GPUs. For more information on Cheaha GPUs, please see our GPU Page.</li> <li>An NVIDIA driver with version 525.60.13 or greater.</li> </ol> <p>Note</p> <p>The recent versions of Parabricks requires 16GB of GPU RAM or more. If this requirement is not satisfied, it will lead to <code>out of memory</code> error. Therefore, <code>Pascalnodes</code> partition are not recommended to run Parabricks pipeline as it does not meet the hardware requirement.</p>"},{"location":"education/case_studies/#system-requirements","title":"System Requirements","text":"<ul> <li>2 GPU server should have 100GB CPU RAM, at least 24 CPU threads.</li> <li>4 GPU server should have 196GB CPU RAM, at least 32 CPU threads.</li> <li>8 GPU server should have 392GB CPU RAM, at least 48 CPU threads.</li> </ul>"},{"location":"education/case_studies/#parabricks-testing-on-cheaha","title":"Parabricks Testing on Cheaha","text":"<p>Parabricks software can be installed and used in the Cheaha platform on <code>amperenodes</code> partition.</p>"},{"location":"education/case_studies/#parabricks-4x-installation-on-cheaha","title":"Parabricks 4.x Installation on Cheaha","text":"<p>Parbaricks 4.x are available as containers in the NGC Catalog. It has generic container that comprises all the analyses pipeline that are referred in the Nvidia Documentation. It also has containers for specific tool category. The recent Parabricks 4.x documentation is available here.</p> <p>Parabricks 4.x container image can be installed on Cheaha using a Singularity container. More details on usage of Singularity container on Cheaha can be found in the Containers Page.</p> <p>To install Parabricks using Singulairty, load the <code>Singularity 3.x</code> module from Cheaha as,</p> <pre><code>module load Singularity/3.5.2-GCC-5.4.0-2.26\n</code></pre> <p>Go to the NGC catalog page and copy the image path to pull the desired containers of Parabricks using Singularity. Here, the generic container is pulled using Singularity.  The image path is in \u201cnvcr.io/nvidia/clara/clara-parabricks\" and the tag is 4.2.0-1. The container image name <code>parabricks-4.2.0-1.sif</code> is an user-derived name.</p> <p></p> <pre><code>singularity pull parabricks-4.2.0-1.sif docker://nvcr.io/nvidia/clara/clara-parabricks:4.2.0-1\n</code></pre> <p>After the image <code>parabricks-4.2.0-1.sif</code> is successfully created, you can run singularity image <code>parabricks-4.2.0-1.sif</code> with all input and output parameters. Various ways of running singularity image can be found in the Containers Page.</p> <p>Running <code>singularity shell</code> helps to navigate through the containers directory to verify the path of the software executable and later use the path outside the container to run the software. Following are the commands to run the container using <code>singularity shell</code> and traverse through the directories inside the contianer.</p> <pre><code>singularity shell parabricks-4.2.0-1.sif\n</code></pre> <pre><code>Singularity&gt; ls /bin/pbrun\n/bin/pbrun\nSingularity&gt; /bin/pbrun version\nPlease visit https://docs.nvidia.com/clara/#parabricks for detailed documentation\n\npbrun: 4.2.0-1\n</code></pre> <p>If the above commands are successfully executed, then the Parabricks software is installed correctly.</p>"},{"location":"education/case_studies/#downloading-parabricks-sample-use-case","title":"Downloading Parabricks Sample Use Case","text":"<p>Sample test case for Parabricks can be found here. Download the sample data using <code>wget</code>,</p> <pre><code>wget -O parabricks_sample.tar.gz https://s3.amazonaws.com/parabricks.sample/parabricks_sample.tar.gz\n</code></pre> <p>Untar the <code>parabricks_sample.tar.gz</code> file,</p> <pre><code>tar -xzvf parabricks_sample.tar.gz\n</code></pre>"},{"location":"education/case_studies/#parabricks-testing-on-amperenodes-on-cheaha","title":"Parabricks Testing on <code>amperenodes</code> on Cheaha","text":"<p>Once the sample data is downloaded, you can execute the pipeline using the executable <code>pbrun</code> which is located in /bin/pbrun within the container.</p> <p>You will have to load the compatible <code>CUDA</code> module to access GPUs as below.</p> <pre><code>module load CUDA/11.6.0\n</code></pre> <p>In the below script, the <code>--nv</code> option enables the use of NVIDIA GPUs within the container. The singualrity container <code>parabricks-4.2.0-1.sif</code> is executed using the command <code>singualrity run</code> over the executable <code>/bin/pbrun</code>.</p> <pre><code>singularity run --nv parabricks-4.2.0-1.sif /bin/pbrun fq2bam \\\n--ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta \\\n--in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz \\\n--out-bam output.bam\n</code></pre> <p>You can execute Parabricks on Cheaha using <code>amperenodes</code> partition. Maximum number of GPUs you can request in <code>amperenodes</code> partition to run Parabricks is 2. Below is a sample job script to run Parabricks on <code>amperenodes</code> partition on 2 GPUs.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=24\n#SBATCH --mem=100G\n#SBATCH --time=2:00:00\n#SBATCH --partition=amperenodes\n#SBATCH --job-name=parabricks-ampere\n#SBATCH --gres=gpu:2\n#SBATCH --error=%x-%j_gpu2.ampere.err\n#SBATCH --output=%x-%j_gpu2.ampere.out\n#SBATCH --mail-user=$USER@uab.edu\n\n#Load the Singularity and CUDA Toolkit modules\nmodule load Singularity/3.5.2-GCC-5.4.0-2.26\nmodule load CUDA/11.6.0\n\n#Run the \"pbrun\" executable from the singularity image \"parabricks-4.2.0-1.sif\", and pass the CUDA lib path to make it accessible within the container\nsingularity run --nv parabricks-4.2.0-1.sif /bin/pbrun fq2bam \\\n--ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta \\\n--in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz \\\n--out-bam output.bam\n</code></pre> <p>You can also request the required resources using <code>srun</code> using the below command, and execute the commands required to run Parabricks.</p> <pre><code>srun --ntasks=24 --mem=100G --time=1:00:00 --partition=amperenodes --job-name=parabricks-ampere --gres=gpu:2 --pty /bin/bash\n</code></pre>"},{"location":"education/case_studies/#illustration-on-fq2bam-tool-analyses","title":"Illustration on <code>fq2bam</code> tool analyses","text":"<p>The above execution script performs <code>fq2bam</code> pipeline analyses. The <code>fq2bam</code> tool aligns, sorts (by coordinate), and marks duplicates in pair-ended FASTQ file data. The data files used in this example are taken from the sample data downloaded in the previous section.</p> <p>If you execute the above batch script using Parabricks sample data on <code>amperenodes</code> with 2 GPUs, the results will be reported as below.</p> <pre><code>[PB Info 2023-Nov-03 11:54:50] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:54:50] ||                 Parabricks accelerated Genomics Pipeline                 ||\n[PB Info 2023-Nov-03 11:54:50] ||                              Version 4.2.0-1                             ||\n[PB Info 2023-Nov-03 11:54:50] ||                       GPU-BWA mem, Sorting Phase-I                       ||\n[PB Info 2023-Nov-03 11:54:50] ------------------------------------------------------------------------------\n[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[PB Info 2023-Nov-03 11:54:55] GPU-BWA mem\n[PB Info 2023-Nov-03 11:54:55] ProgressMeter    Reads           Base Pairs Aligned\n[PB Info 2023-Nov-03 11:55:09] 5043564          600000000\n[PB Info 2023-Nov-03 11:55:13] 10087128 1180000000\n[PB Info 2023-Nov-03 11:55:18] 15130692 1740000000\n[PB Info 2023-Nov-03 11:55:22] 20174256 2320000000\n[PB Info 2023-Nov-03 11:55:26] 25217820 2900000000\n[PB Info 2023-Nov-03 11:55:30] 30261384 3490000000\n[PB Info 2023-Nov-03 11:55:33] 35304948 4050000000\n[PB Info 2023-Nov-03 11:55:37] 40348512 4640000000\n[PB Info 2023-Nov-03 11:55:41] 45392076 5230000000\n[PB Info 2023-Nov-03 11:55:45] 50435640 5790000000\n[PB Info 2023-Nov-03 11:57:59]\nGPU-BWA Mem time: 184.615934 seconds\n[PB Info 2023-Nov-03 11:57:59] GPU-BWA Mem is finished.\n\n[main] CMD: /usr/local/parabricks/binaries//bin/bwa mem -Z ./pbOpts.txt -F 0 /home/prema/projects/parabricks_testing/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta /home/prema/projects/parabricks_testing/parabricks_sample/Data/sample_1.fq.gz /home/prema/projects/parabricks_testing/parabricks_sample/Data/sample_2.fq.gz @RG\\tID:HK3TJBCX2.1\\tLB:lib1\\tPL:bar\\tSM:sample\\tPU:HK3TJBCX2.1\n[main] Real time: 188.762 sec; CPU: 2037.057 sec\n[PB Info 2023-Nov-03 11:57:59] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:57:59] ||        Program:                      GPU-BWA mem, Sorting Phase-I        ||\n[PB Info 2023-Nov-03 11:57:59] ||        Version:                                           4.2.0-1        ||\n[PB Info 2023-Nov-03 11:57:59] ||        Start Time:                       Fri Nov  3 11:54:50 2023        ||\n[PB Info 2023-Nov-03 11:57:59] ||        End Time:                         Fri Nov  3 11:57:59 2023        ||\n[PB Info 2023-Nov-03 11:57:59] ||        Total Time:                            3 minutes 9 seconds        ||\n[PB Info 2023-Nov-03 11:57:59] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:00] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:00] ||                 Parabricks accelerated Genomics Pipeline                 ||\n[PB Info 2023-Nov-03 11:58:00] ||                              Version 4.2.0-1                             ||\n[PB Info 2023-Nov-03 11:58:00] ||                             Sorting Phase-II                             ||\n[PB Info 2023-Nov-03 11:58:00] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:00] progressMeter - Percentage\n[PB Info 2023-Nov-03 11:58:00] 0.0       0.00 GB\n[PB Info 2023-Nov-03 11:58:05] 52.8      0.00 GB\n[PB Info 2023-Nov-03 11:58:10] Sorting and Marking: 10.001 seconds\n[PB Info 2023-Nov-03 11:58:10] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:10] ||        Program:                                  Sorting Phase-II        ||\n[PB Info 2023-Nov-03 11:58:10] ||        Version:                                           4.2.0-1        ||\n[PB Info 2023-Nov-03 11:58:10] ||        Start Time:                       Fri Nov  3 11:58:00 2023        ||\n[PB Info 2023-Nov-03 11:58:10] ||        End Time:                         Fri Nov  3 11:58:10 2023        ||\n[PB Info 2023-Nov-03 11:58:10] ||        Total Time:                                     10 seconds        ||\n[PB Info 2023-Nov-03 11:58:10] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:11] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:11] ||                 Parabricks accelerated Genomics Pipeline                 ||\n[PB Info 2023-Nov-03 11:58:11] ||                              Version 4.2.0-1                             ||\n[PB Info 2023-Nov-03 11:58:11] ||                         Marking Duplicates, BQSR                         ||\n[PB Info 2023-Nov-03 11:58:11] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:58:11] Using PBBinBamFile for BAM writing\n[PB Info 2023-Nov-03 11:58:11] progressMeter -  Percentage\n[PB Info 2023-Nov-03 11:58:21] 24.9      0.08 GB\n[PB Info 2023-Nov-03 11:58:31] 48.1      0.07 GB\n[PB Info 2023-Nov-03 11:58:41] 69.8      0.09 GB\n[PB Info 2023-Nov-03 11:58:51] 85.9      0.11 GB\n[PB Info 2023-Nov-03 11:59:01] 100.0     0.00 GB\n[PB Info 2023-Nov-03 11:59:01] BQSR and writing final BAM:  50.044 seconds\n[PB Info 2023-Nov-03 11:59:01] ------------------------------------------------------------------------------\n[PB Info 2023-Nov-03 11:59:01] ||        Program:                          Marking Duplicates, BQSR        ||\n[PB Info 2023-Nov-03 11:59:01] ||        Version:                                           4.2.0-1        ||\n[PB Info 2023-Nov-03 11:59:01] ||        Start Time:                       Fri Nov  3 11:58:11 2023        ||\n[PB Info 2023-Nov-03 11:59:01] ||        End Time:                         Fri Nov  3 11:59:01 2023        ||\n[PB Info 2023-Nov-03 11:59:01] ||        Total Time:                                     50 seconds        ||\n[PB Info 2023-Nov-03 11:59:01] ------------------------------------------------------------------------------\n</code></pre>"},{"location":"education/case_studies/#monitoring-gpu-usage-during-runtime-on-cheaha","title":"Monitoring GPU Usage During Runtime on Cheaha","text":"<p>The <code>nvidia-smi</code> command helps to montior the GPU usage during runtime. You need to <code>ssh</code> to the assigned GPU node, and type in the following command.</p> <pre><code>ssh GPU_node\n</code></pre> <pre><code>module load CUDA/11.6.0\nnvidia-smi\n</code></pre> <p>The <code>nvidia-smi</code> reports the GPU memory usage and the 2 GPU process running details as shown below.</p> <pre><code>$ module load CUDA/11.6.0\n\n$ nvidia-smi\n\nFri Nov  3 12:38:24 2023\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100 80GB PCIe          Off | 00000000:25:00.0 Off |                    0 |\n| N/A   35C    P0             165W / 300W |  16631MiB / 81920MiB |     97%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA A100 80GB PCIe          Off | 00000000:81:00.0 Off |                    0 |\n| N/A   32C    P0             112W / 300W |  16631MiB / 81920MiB |     96%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     65905      C   .../local/parabricks/binaries//bin/bwa    16594MiB |\n|    1   N/A  N/A     65905      C   .../local/parabricks/binaries//bin/bwa    16594MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"education/case_studies/#runtime-evaluation-of-parabricks-sample-test-case-on-amperenodes-partition","title":"Runtime Evaluation of Parabricks Sample Test Case on <code>amperenodes</code> Partition","text":"<p>Parabricks is tested and works with CUDA version &gt;= 11.6.0 on Cheaha. Empirical results on running Parabricks sample test case on <code>amperendoes</code> partitions are illustrated in the below table. For this test case, runtime of 1 GPU is better than 2 GPU. Generally, large-scale data scales well with increase in number of GPUs, and the real-world science simulations may vary in their speedup.</p> Partitions No. of GPUs Execution Time (s) amperenodes 1 239 amperenodes 2 270 <p>Applications show 2x performance with Parabricks &gt; 4.0 version. You can refer here to performance tuning ideas to achieve best performance with Parabricks.</p>"},{"location":"education/courses/","title":"Courses","text":""},{"location":"education/courses/#data-science-journal-club-course","title":"Data Science Journal Club Course","text":"<p>Intended for students interested in learning more about data science and its applications to research, the course is 1 credit hour pass/fail and offered every session (Fall, Spring, Summer). The course is listed as <code>GBSC 720-VTR JC- Data Science Club</code> and sometimes also as <code>IDNE 790-VTR JC- Data Science Club</code>. Students are largely expected to lead their own journey, with instructors facilitating and offering advice. We expect a good-faith effort to learn and grow in the course and the course has most value when students step outside their comfort zones. There are three student-led demonstrations required to pass, where we expect students to demonstrate how they have learned and grown from their exploration of the material.</p> <p>Some of the topics covered by past students include:</p> <ul> <li>Learning to use High Performance Computing (HPC) resources</li> <li>Learning software development good practices</li> <li>Learning git and github</li> <li>Learning data organization good practices</li> <li>Jupyter notebooks</li> <li>R Markdown notebooks</li> <li>Data visualization</li> <li>Data analytics</li> <li>Data processing pipelines</li> <li>Machine learning</li> <li>Deep learning</li> </ul> <p>The only prerequisite for the course is feeling comfortable using a computer. We are flexible with topics and welcome students of all levels of data science experience.</p> <p>If you have questions about the course please feel free to reach out to Support.</p>"},{"location":"education/courses/#syllabus","title":"Syllabus","text":"<p>The syllabus is available as a public webpage at https://s3.lts.rc.uab.edu/uab-rc-dsjc/syllabus.html, hosted on LTS.</p>"},{"location":"education/training_resources/","title":"Training Resources","text":""},{"location":"education/training_resources/#internal-resources","title":"Internal Resources","text":"<ul> <li>AI at UAB: AI-related resources, available to UAB Employees and the broader community, encompassing safety protocols, ethical principles, and diverse applications of AI, including current trends.</li> <li>Informatics Club: A UAB group for students and trainees to learn more about informatics.<ul> <li>Code, Chat &amp; Collab: A recurring meet-up hosted by the Informatics Club where students, staff and faculty can meet to discuss informatics, data analysis, software development, and Cheaha use.</li> <li>Events and Resources: A collection of useful resources provided by the Informatics club.</li> </ul> </li> <li>LinkedIn Learning: Online courses available for free to UAB employees. Topics include programming, data science, and more.</li> </ul>"},{"location":"education/training_resources/#external-resources","title":"External Resources","text":""},{"location":"education/training_resources/#the-carpentries","title":"The Carpentries","text":"<p>The Carpentries is a 501(c)3 non-profit organization dedicated to educating researchers on software and data skills. Using inclusive and accessibly best practices, they build, maintain and promote high-quality instructional material for a range of software and data-oriented concepts.</p> <ul> <li>Software Carpentries Lessons: Primary page for the Software Carpentry lessons. The following lessons, and more, are available.<ul> <li>The Unix Shell</li> <li>Version Control with Git</li> <li>Programming with Python</li> <li>Programming with R</li> </ul> </li> <li>Data Carpentries Lessons: Primary page for the Data Carpentry lessons. Instructional material is available for a variety of fields.<ul> <li>Genomics Curriculum</li> <li>Image Processing Curriculum</li> </ul> </li> <li>HPC Carpentries: a page dedicated to sharing HPC-oriented, high-quality, peer-reviewed lessons following Carpentries' teaching standards. Start your journey into High-Performance Computing with the Introduction to HPC page.</li> </ul>"},{"location":"education/training_resources/#rigor-reproducibility-tools","title":"Rigor &amp; Reproducibility Tools","text":"<ul> <li>Community for Rigor: a group of scientists from various universities and research institutions in the United States has collaborated to develop educational materials aimed at facilitating the teaching and learning of the principles and practices of scientific rigor.</li> <li>Rigor &amp; Reproducibility Tool Repository: Contains various tools related to rigor and reproducibility, along with links to relevant landing pages.</li> </ul>"},{"location":"education/training_resources/#other-external-resources","title":"Other External Resources","text":"<ul> <li>Rosalind.info: A platform for learning bioinformatics and programming.</li> <li>Transcriptomics: A course teaching students to independently analyze high-throughput sequencing data from gene expression (RNA-seq) studies using lightweight, open-source software and the R programming language with Bioconductor packages.</li> <li>MATLAB upcoming events: MATLAB offers a range of upcoming events, including data-oriented, programming-oriented, and domain-specific training sessions. Many of these events are free to attend.</li> </ul>"},{"location":"education/training_resources/#suggested-reading","title":"Suggested Reading","text":"<ul> <li>Simple rules for efficient scientific programming: provide 10 rules for efficient code writing to boost research productivity.</li> <li>Good enough practices in scientific computing: a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. Further reading available at Software Carpentries github</li> <li>Cookiecutter Data Science: an adaptable project structure for performing and sharing data science works.</li> </ul>"},{"location":"education/training_resources/#our-social-media-presence","title":"Our Social Media Presence","text":"<ul> <li>YouTube</li> <li>GitHub: Contains several repos used on the YouTube channel.</li> </ul>"},{"location":"education/research_computing_days/","title":"UAB Research Computing Day","text":"<p>Research Computing Day is a dialog within the UAB research communityabout leveraging the power of computers to grow the depth of our investigation into the nature of the world that surrounds us. The annual event welcomes discussions on science, engineering, the arts and humanities focused on the drive to open new research frontiers with advances in technology.</p> <p>Whether computers are used to increase the accuracy of a model, interpret the ever-growing stream of data from new image collections and instruments, or engage with peers around the globe, UAB's status as a leading research community depends on the ability to incorporate these capabilities into the research process. By participating in the dialog of Research Computing Day at UAB, researchers can share how they are using these methods to enhance their research, gain new insights from peers, and contribute their voices to the growth of research at UAB.</p>"},{"location":"education/research_computing_days/#background","title":"Background","text":"<p>Since 2007, The Office of the Vice President for Information Technology has sponsored an annual dialog on the role of technology in research. These events joined UAB with national dialogs on the role of Cyberinfrastructure in research held at campuses across the country.</p>"},{"location":"education/research_computing_days/#previous-uab-research-computing-days","title":"Previous UAB Research Computing Days","text":"<ul> <li>2007 -- Co-hosted along with the ASA site visit, providing an overview of new services and upcoming launch of the UABgrid pilot. (No web record)</li> <li>2008 -- Focus on grid computing and collaboration technologies, in particular the caBIG program with guest speakers from Booz Allen Hamilton who managed the NCI caBIG program and SURA (agenda currently offline)</li> <li>2010 -- Featured introduction to Galaxy platform for genetic sequencing by Dell staff scientist (agenda currently offline)</li> <li>2011 -- Understanding growth of research computing support at peer institutions UNC and Emory</li> <li>2012 -- Growing data sciences at UAB</li> <li>2013 -- OpenStack at UAB</li> <li>2016 -- HPC Expansion</li> <li>2017 -- GPU expansion</li> <li>2018 -- Use Cases and Strategic Engagement</li> <li>2019 -- Building Gateways to Science</li> </ul>"},{"location":"grants/facilities/","title":"UAB IT Research Computing (UABRC) Resources and Cybersecurity Facilities Document","text":"<p>The link below contains a plain-text facilities document designed to support research grant applications.</p> <p>Download the facilities document file as:</p> <ul> <li>Word (.docx)</li> <li>plaintext</li> </ul> <p>If you believe that information within is inaccurate or there are missing details, please feel free to contact Support.</p>"},{"location":"grants/facilities/#detailed-hardware-information","title":"Detailed Hardware Information","text":"<p>For more detailed information on compute hardware please see: Detailed Hardware Information</p>"},{"location":"grants/opportunities/","title":"Funding Opportunities","text":"<ul> <li>Research Software Funding Opportunities: https://www.researchsoft.org/funding-opportunities/</li> </ul>"},{"location":"grants/publications/","title":"Acknoweldgement in Publications","text":"<p>To acknowledge the use of Cheaha in published work, for compute time or substantial technical assistance, please consider adding the following to the acknowledgements section of your publication:</p> <p>The authors gratefully acknowledge the resources provided by the University of Alabama at Birmingham IT-Research Computing group for high performance computing (HPC) support and CPU time on the Cheaha compute cluster.</p> <p>If Globus was used to transfer data to/from Cheaha, please consider adding the following to the acknowledgements section of your publication:</p> <p>This work was supported in part by the National Science Foundation under Grants Nos. OAC-1541310, the University of Alabama at Birmingham, and the Alabama Innovation Fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the University of Alabama at Birmingham.</p>"},{"location":"help/faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>We have moved our FAQ page to a new home at Ask.Cyberinfrastructure. Feel free to subscribe for updates.</p> <p></p>"},{"location":"help/support/","title":"How to Request Support","text":"<p>Before reaching out to us, try searching this documentation for keywords related to your issue. If you aren't able to find anything, please try checking our FAQ located on ask.cyberinfrastructure. If you still need help, please read on for how to send in a ticket and how to work with our ticketing system.</p>"},{"location":"help/support/#how-do-i-create-a-support-ticket","title":"How Do I Create a Support Ticket?","text":"<p>To Create a support ticket, send a descriptive email to support@listserv.uab.edu to create a ticket. Bonus points for including the following details.</p> <p>For general issues:</p> <ol> <li>What is your goal?</li> <li>What steps were taken?</li> <li>What was expected?</li> <li>What actually happened?</li> <li>How was the cluster accessed? Web Portal, SSH, VNC, etc.?</li> <li>What software were you using? Please be as specific as possible. The command <code>module list</code> can be helpful here.</li> </ol> <p>For outages:</p> <ol> <li>What part of the cluster is affected? Please list any relevant affected nodes or other hardware that is not accessible. If you are unable to access the cluster please state that instead.</li> <li>What were you working on when you noticed the outage?</li> <li>How were you accessing the cluster? Web Portal, SSH, VNC, etc.?</li> </ol>"},{"location":"help/support/#how-do-i-work-with-tickets-ive-created","title":"How Do I Work With Tickets I've Created?","text":"<p>UAB IT and Research Computing use ServiceNow for ticket management. When any email is sent to support@listserv.uab.edu, AskIT is notified and a ticket is created in ServiceNow and the ticket details are forwarded by email to every member of Research Computing staff.</p> <p>When any email is sent to support@listserv.uab.edu the following happens:</p> <ol> <li>An RITM ticket is created in ServiceNow and assigned to Research Computing.</li> <li>The email is routed to Research Computing staff and the ticket creator from the <code>support@listserv.uab.edu</code> email list. Please do not reply. Replies create additional tickets and cause service delays.</li> <li>A monitoring email is sent from <code>support-watch@listserv.uab.edu</code> to Research Computing staff and the ticket creator. Please do not reply to this email. Your email will not make it through, because the email address is closed to Research Computing staff.</li> <li> <p>A ticket creation email is sent from <code>askit@uab.edu</code> to Research Computing staff and the ticket creator. Please do reply to this email. Replies to <code>askit@uab.edu</code> with the correct subject line format will add your reply as a comment to the ticket. Before replying, please delete all previous quoted replies to avoid the accumulation of noise in the ticket. The images below show how to, and how not to, reply to <code>askit@uab.edu</code> emails.</p> <ul> <li> <p>Please reply like this:     </p> </li> <li> <p>Not like this:     </p> </li> </ul> </li> </ol> <p>If you prefer to use email to manage your ticket, please use the method of replying to emails from <code>askit@uab.edu</code>. Note that you will not be able to see any attachments this way. They are stripped off and added to the ticket in the ServiceNow web interface, available through the <code>RITM0000000</code> link in the emails you've received.</p> <p>If you prefer to use the ServiceNow web interface, please click the <code>RITM0000000</code> link in the email replies.</p> <p>Important</p> <p>The UAB email server strips all potentially executable files from emails. This includes attachments with <code>.log</code>, <code>.sh</code>, <code>.py</code> and <code>.exe</code> suffixes. Zip files containing those files will also be stripped. If you need to send script examples to us please rename the files to have a <code>.txt</code> suffix and inform us of their original nature.</p>"},{"location":"help/support/#what-types-of-files-can-i-attach-to-tickets","title":"What Types of Files can I Attach to Tickets?","text":"<p>Certain common file extensions are filtered when sent via email or attached to tickets. Generally, any file that is potentially executable is filtered. Below is a list of file extensions known to be filtered, and there may be others.</p> <ul> <li>Executable files: <code>.exe</code>, <code>.dat</code>, <code>.cab</code></li> <li>Windows registry files: <code>.reg</code></li> <li>Code and script files: <code>.sh</code>, <code>.bash</code>, <code>.py</code>, <code>.m</code>, <code>.r</code></li> <li>Log files: <code>.log</code></li> <li>Rich document files containing macros. Essentially, any file extension from Microsoft Office ending with <code>m</code>, e.g., <code>.docm</code>.</li> <li>Files with special characters in the filename.</li> </ul> <p>To ensure your files are attached to tickets as expected, try the following.</p> <ul> <li>Images, use <code>.png</code> or <code>.jpg</code>.</li> <li>Log files, rename <code>output.log</code> to <code>output.txt</code></li> <li>Code files, rename <code>code.sh</code> or <code>code.py</code>, etc., to <code>code.txt</code>.</li> <li>Change the name to only use ASCII letters, numbers, hyphen, and underscore.</li> </ul> <p>Please do not try to attach executable files to tickets. If you have a legitimate need to give us executable files, mention it in the ticket and we can find a path forward on a case-by-case basis.</p>"},{"location":"help/support/#how-do-i-request-or-change-shared-storage","title":"How Do I Request Or Change Shared Storage?","text":"<p>Please see our Storage page for more information.</p>"},{"location":"help/support/#how-do-i-request-new-software-installed","title":"How do I request new software installed?","text":"<p>Before making a request for new software on Cheaha, please try searching our modules or searching for packages on Anaconda.</p> <p>If you are not able to find a suitable module or package and would like software installed on Cheaha, please create a ticket with the name of the software, the version number, and a link to the installation instructions.</p>"},{"location":"help/support/#office-hours","title":"Office Hours","text":"<p>For our office hours links please see Contact Us.</p>"},{"location":"help/support/#status-updates","title":"Status Updates","text":"<p>For status updates affecting our systems or services please visit https://uabstatus.statuscast.com/#!/incidentlist?componentId=34990.</p> <p>At this page you can subscribe to notifications using the bell icon next to the name \"Research Computing\" near the top-left of the page.</p> <p></p>"},{"location":"kubernetes/startup/","title":"UAB Kubernetes","text":"<p>Construction</p> <p>This page is a stub and is under construction.</p> <p>Kubernetes (K8s, named because there are 8 letters between 'K' and 's') can be used to automate workflows of all kinds. Researchers can use KSoftware engineers can use it to create GitLab runners to execute continuous integration workflows to deploy code updates seemlessly.</p>"},{"location":"kubernetes/startup/#uab-kubernetes-computation-resources","title":"UAB Kubernetes Computation Resources","text":"<ul> <li>4 DGX A100 nodes<ul> <li>GPUs aren\u2019t virtualized, but one DGX node can be split into 1/7th of a GPU</li> <li>8 A100s per DGX node * 7/7ths = 56/7ths of a GPU possible</li> <li>Can be used for classes learning to use GPUs</li> </ul> </li> <li>Finding containers to use<ul> <li>K8s will search DockerHub and other container registries automagically.</li> <li>Can add additional registries</li> </ul> </li> </ul>"},{"location":"national_ci/","title":"National Cyberinfrastructre","text":"<p>Construction</p> <p>This page is a stub and is under construction.</p>"},{"location":"national_ci/nih/","title":"National Institutes of Health (NIH) Cyberinfrastructure","text":"<p>Construction</p> <p>This page is a stub and is under construction.</p>"},{"location":"national_ci/nsf_access/","title":"National Science Foundation (NSF) Cyberinfrastructure","text":"<p>Construction</p> <p>This page is a stub and is under construction.</p>"},{"location":"national_ci/osg/","title":"The Open Science Grid (OSG)","text":"<p>Construction</p> <p>This page is a stub and is under construction.</p>"},{"location":"uab_cloud/","title":"UAB Cloud","text":"<p>Our cloud.rc portal, based on OpenStack cloud software, provides a home for more permanent research applications such as web pages and database hosting, as well as a place where researchers can more fluidly develop applications for high performance compute.</p> <p>Resource quotas are set to ensure that every researcher has a fair share. Please be sure to free up resources when they are no longer needed by deleting instances and volumes.</p> <p>Currently, access to cloud.rc must be made while on the UAB Campus Network or on the UAB Campus Virtual Private Network (VPN). For more information about using the UAB Campus VPN, please visit VPN - UAB IT. The VPN requires Duo 2FA.</p>"},{"location":"uab_cloud/#first-steps","title":"First Steps","text":"<p>To get started using cloud.rc, please navigate to https://dashboard.cloud.rc.uab.edu/. You will be taken to a login page that looks like below. To login you will need an account, please request one by contacting Support. When requesting an account, please tell us how you intend to use the service. Some reasonable use-cases are listed in Cloud Usage Philosophy below.</p> <p></p> <p>There are three fields that must be filled out:</p> <ul> <li>Domain: must always be <code>uab</code>, lowercase.</li> <li>User Name: your BlazerID or XIAS email.</li> <li>Password: whatever your current password is, not necessarily the same as your Single Sign-On password. If you have forgotten your password please contact Support.</li> </ul> <p>Once these fields are filled, click \"Sign In\" to login.</p> <p>Once logged in, you will see the OpenStack dashboard. An example is shown below.</p> <p></p> <p>To get the most out of cloud.rc, you'll want to make sure you have a working familiarity with the Linux terminal.</p> <p>Cloud.rc runs on Openstack. If you are new to Openstack or to cloud.rc, it is highly recommended to follow our Tutorial to learn how to set up all of the necessary components of a virtual machine (VM) setup. The tutorial is intended to be followed in order. Doing it out of order may result in errors and issues. If you encounter any unexpected issues, unclear instructions or have questions or comments, please contact Support.</p>"},{"location":"uab_cloud/#cloud-usage-philosophy","title":"Cloud Usage Philosophy","text":"<p>An important philosophy of cloud services is that virtual machines are disposable, and cloud platforms are designed to facilitate this philosophy. If a virtual machine fails, breaks or becomes misconfigured, it is designed to be destroyed and recreated from scratch.</p> <p>With that in mind, there are many possible use-cases for cloud.rc. Broadly speaking, a few common ones:</p> <ol> <li>Experimental workflow development prior to Containerization and batch processing on Cheaha or a National Cyberinfrastructure platform.</li> <li>Packaging software into Containers.</li> <li>Temporary hosting of server software for scientific development or workflows.</li> </ol> <p>The downside to disposable machines is losing configuration specifics. Software exists that can assist with reproducible virtual machine configuration, including Ansible, or even just a custom shell script. We are unable to provide assistance with deployment of virtual machine internals.</p>"},{"location":"uab_cloud/#naming-conventions","title":"Naming Conventions","text":"<p>Entities on cloud.rc must be named a certain way or difficult-to-diagnose errors may occur. Entities includes instances, volumes, networks, routers, and anything else that you are allowed to give a name to.</p> <p>Please use the following rules when naming entities:</p> <ul> <li>Must: use only letters, numbers, dash <code>-</code> and underscore <code>_</code>.</li> <li>Must: have the first character in the name be a letter.</li> <li>Should: use short, descriptive, memorable names.</li> </ul>"},{"location":"uab_cloud/installing_software/","title":"Installing Software on Instances","text":"<p>An important part of managing instances is the installation of software. This page assumes you have a working instance and can SSH into it. This page assumes you are using an Ubuntu image.</p> <p>We highly recommend building your research software stack into a Container. While there is a learning curve and some setup time, containers make replicating and sharing environments simpler. Everything you develop is packaged into a self-contained unit that can be run on virtually any modern Linux system.</p> <p>A particular command <code>sudo</code> will be used extensively. Be warned that <code>sudo</code> grants any commands used administrator privileges. If you use <code>sudo</code> with untrustworthy software, you may be allowing an attacker to compromise your system.</p> <p>Danger</p> <p>The <code>sudo</code> command should be used carefully and judiciously, as it creates security risks. Use with caution.</p> <p>Important</p> <p>Much of the information and examples on this page require a working knowledge of terminal commands and the shell. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources.</p>"},{"location":"uab_cloud/installing_software/#before-installing-software","title":"Before Installing Software","text":"<p>Before installing software, good practice is updating and upgrading operating system packages. For some software this is required. These updates often include critical security and bug fixes. To update the instance operating system, enter the following at the command line.</p> <pre><code>sudo apt update\nsudo apt upgrade\n</code></pre>"},{"location":"uab_cloud/installing_software/#installing-software","title":"Installing Software","text":"<p>Most common software packages and NVIDIA drivers are available as <code>apt</code> packages. Some packages are available using custom installers. Be sure the website and installer author are trustworthy before installing!</p>"},{"location":"uab_cloud/installing_software/#finding-packages","title":"Finding Packages","text":"<ol> <li>Try using Google to locate the name of the package with something like <code>ubuntu apt &lt;keywords&gt;</code></li> <li>Try using https://packages.ubuntu.com</li> <li>Try <code>apt-cache search &lt;keyword&gt;</code></li> <li>Ask Support for help</li> </ol>"},{"location":"uab_cloud/installing_software/#installing-packages","title":"Installing Packages","text":"<p>If the software is available via <code>apt</code> then use <code>sudo apt install &lt;package&gt;</code>. An example would be <code>sudo apt install git</code> to install git software.</p> <p>If the software uses a custom installer, then follow the instructions provided by the software's documentation. An example would be Miniconda, where a shell script is downloaded and then executed using <code>bash installer.sh</code>.</p>"},{"location":"uab_cloud/installing_software/#installing-server-software","title":"Installing Server Software","text":"<p>If you wish to set up server software, you'll need to open ports for that software by creating Security Groups. Then you may want to test the ports are open and verify your software is listening. It is highly recommended to verify the ports are configured properly and remotely accessible before you spend time setting up and configuring the server software.</p>"},{"location":"uab_cloud/installing_software/#testing-server-ports","title":"Testing Server Ports","text":"<p>If you intend to use your instance as a server host, you'll likely need to set up additional Security Groups for any ports the server expects to communicate on. It can be helpful to verify that those ports are open before configuring the server software. Assuming you know which ports are needed, the simplest way to do this is outlined below.</p> <ol> <li>Set up Security Groups for the ports your server will need to communicate on.</li> <li>SSH into the instance.</li> <li>Prepare the <code>netcat</code> software command <code>nc</code>:<ul> <li>For Ubuntu, the command <code>nc</code> should already be available.</li> <li>For other OSes, you may need to Install <code>nc</code> or <code>netcat</code>.</li> </ul> </li> <li> <p>For one of your <code>&lt;port&gt;</code> of interest, start a TCP listener with <code>nc -l &lt;port&gt;</code>.</p> <p>Note</p> <p><code>nc -l &lt;port&gt;</code> will only listen for a single connection attempt, and then close. To emulate indefinite listening behavior, use it within a loop like so.</p> <pre><code>while true; do nc -l &lt;port&gt;; done\n</code></pre> </li> <li> <p>Open a new terminal on your local machine.</p> </li> <li> <p>Probe the <code>&lt;port&gt;</code>:</p> <ul> <li> <p>Using the Windows command prompt:</p> <ol> <li>Enter the command <code>telnet &lt;floating-ip&gt; &lt;port&gt;</code>.</li> <li> <p>If the terminal window goes blank, then the connection was successful.</p> <p></p> <p>Otherwise you will see a message like <code>Could not open connection to the host</code>.</p> <p></p> </li> <li> <p>To exit <code>telnet</code> press Ctrl+], then type <code>q</code>, then press Enter.</p> <p></p> </li> </ol> </li> <li> <p>Using any Linux-based prompt, MacOS, or Git Bash on Windows:</p> <ol> <li>Ensure <code>nc</code> is installed locally.</li> <li>Enter the command <code>nc -nvz &lt;floating-ip&gt; &lt;port&gt;</code>.<ul> <li><code>n</code> uses numeric output, which minimizes unhelpful warnings about hostname lookups. It is also faster.</li> <li><code>v</code> uses verbose output, i.e., print the output we care about.</li> <li><code>z</code> scans for listeners on the remote.</li> </ul> </li> <li> <p>If the connection is successful you should see something close to the following, with <code>&lt;floating-ip&gt;</code> and <code>&lt;port&gt;</code> replaced by the values you supplied earlier.</p> <pre><code>(UNKNOWN) [&lt;floating-ip&gt;] &lt;port&gt; (?) open\n</code></pre> <p></p> <p>If the connection is unsuccessful you will see <code>Connection refused</code> instead of <code>open</code>.</p> <p></p> </li> </ol> </li> </ul> </li> </ol> <p>Now you should have more information on whether your VM port configuration was successful. Feel free to repeat the steps above for each port, as needed.</p>"},{"location":"uab_cloud/installing_software/#verify-server-software-is-listening","title":"Verify Server Software is Listening","text":"<p>Once you have the server set up, you can check which processes are listening on which ports using the following command.</p> <pre><code>sudo ss -lnptu\n</code></pre> <p>The program is <code>ss</code> for \"socket statistics\", which can display information linking ports and processes. The flags are</p> <ul> <li><code>l</code> displays only listening sockets</li> <li><code>n</code> uses numeric output, which minimizes unhelpful warnings about hostname lookups. It is also faster.</li> <li><code>p</code> shows which process is using each socket.</li> <li><code>t</code> displays tcp sockets.</li> <li><code>u</code> displays udp sockets.</li> </ul> <p>An example of the output is shown below. The most useful columns for us are <code>Local Address:Port</code>, to verify they match the configured Security Group ports, and <code>Process</code>, to verify the server software is listening on the correct ports.</p> <p></p>"},{"location":"uab_cloud/installing_software/#common-examples","title":"Common Examples","text":"<p>Below are a few examples of installing certain common softwares that may be useful to scientific applications. We are not able to provide diagnostic or troubleshooting support for installation of any software. If you believe these instructions are outdated or in error, please reach out and let us know.</p>"},{"location":"uab_cloud/installing_software/#installing-nvidia-drivers","title":"Installing NVidia Drivers","text":"<ol> <li>Run the commands in Before Installing Software.</li> <li><code>sudo apt install ubuntu-drivers-common</code></li> <li><code>ubuntu-drivers devices</code></li> <li>Find the line with \"recommended\" and install the package on that line with <code>sudo apt install nvidia-driver-###</code></li> <li>Reboot the instance</li> </ol>"},{"location":"uab_cloud/installing_software/#installing-miniconda","title":"Installing Miniconda","text":"<p>Miniconda is a lightweight version of Anaconda. While Anaconda's base environment comes with Python, the Scipy stack, and other common packages pre-installed, Miniconda comes with no packages installed. This is an excellent alternative to the full Anaconda installation for environments where minimal space is available or where setup time is important. We recommend installing Miniconda on cloud.rc instances, as opposed to Anaconda, to conserve storage space. For more information on how to use Anaconda see the Using Anaconda. Need some hands-on experience, you can find instructions on how to install PyTorch and TensorFlow using Anaconda in this tutorial.</p> <ol> <li>Run the commands in Before Installing Software.</li> <li><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</code></li> <li><code>bash Miniconda3-latest-Linux-x86_64.sh</code></li> </ol>"},{"location":"uab_cloud/installing_software/#installing-singularity","title":"Installing Singularity","text":"<p>Follow the instructions located at https://docs.sylabs.io/guides/3.9/user-guide/quick_start.html#install-system-dependencies under \"Debian-based systems\".</p> <ol> <li>Run the commands in Before Installing Software.</li> <li> <p>Run the following</p> <pre><code>sudo apt-get install -y \\\nbuild-essential \\\nlibseccomp-dev \\\npkg-config \\\nsquashfs-tools \\\ncryptsetup\n</code></pre> </li> <li> <p>Install Go language using the following</p> <pre><code>export VERSION=1.17.2 OS=linux ARCH=amd64 &amp;&amp; \\  # Replace the values as needed\nwget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\ # Downloads the required Go package\nsudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\ # Extracts the archive\nrm go$VERSION.$OS-$ARCH.tar.gz    # Deletes the ``tar`` file\n\necho 'export PATH=/usr/local/go/bin:$PATH' &gt;&gt; ~/.bashrc &amp;&amp; \\\nsource ~/.bashrc\n</code></pre> </li> <li> <p>Download SingularityCE</p> <pre><code>export VERSION=3.9.5 &amp;&amp; # adjust this as necessary \\\nwget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz &amp;&amp; \\\ntar -xzf singularity-ce-${VERSION}.tar.gz &amp;&amp; \\\ncd singularity-ce-${VERSION}\n</code></pre> </li> <li> <p>Compile SingularityCE</p> <pre><code>./mconfig &amp;&amp; \\\nmake -C builddir &amp;&amp; \\\nsudo make -C builddir install\n</code></pre> </li> </ol> <p>Note</p> <p>For other versions of the Singularity documentation, visit https://sylabs.io/docs/.</p>"},{"location":"uab_cloud/installing_software/#installing-jupyter-server","title":"Installing Jupyter Server","text":"<p>Jupyter Notebooks are a staple of modern research computing, especially when developing new workflows or evaluating the usefulness of software packages.</p> <p>The setup process for cloud.rc is more involved than for Cheaha. Before using cloud.rc for Jupyter Notebooks, be sure that Open OnDemand on Cheaha does not meet your needs.</p> <p>To install, you will need the following pre-requisites. If you are unfamiliar with the terminology or new to cloud.rc, it is highly recommended to first start with our Introduction and follow the tutorial completely.</p> <ol> <li>Run the commands in Before Installing Software.</li> <li>A Cloud Instance with attached [Floating IP]network_setup_basic.md#floating-ips).</li> <li>A Security Group for the intended Jupyter Server port. For the purposes of this tutorial, the port will be set to <code>9999</code>.</li> <li>Miniconda installed on the instance. Miniconda is a lightweight version of Anaconda.</li> </ol> <p>Once the prerequisites are complete, the following steps must be performed to install and setup Jupyter Notebook Server. It is highly recommended to build an Anaconda Environment using a reproducible Environment File. The steps below belong to the official Jupyter documentation available at https://jupyter-server.readthedocs.io/en/stable/operators/public-server.html.</p> <p>Warning</p> <p>Leaving your Jupyter Notebook Server unsecured may mean that other people on the UAB Campus Network are able to access your notebooks and other files stored on that cloud instance.</p> <ol> <li> <p>Install Jupyter Notebook Server using Miniconda. You will need the following packages.</p> <ul> <li><code>conda-forge</code> channel<ul> <li><code>notebook</code></li> <li><code>nb_conda_kernels</code></li> <li>[Optional] <code>jupyter_contrib_nbextensions</code></li> </ul> </li> <li><code>anaconda</code> channel<ul> <li><code>ipykernel</code> for python users</li> <li><code>r-irkernel</code> for R users</li> <li>[Optional] <code>pip</code></li> </ul> </li> </ul> </li> <li> <p>Because floating IPs are, by default, reachable by anyone on the UAB Campus Network, you'll need to secure the server using the steps below.</p> <ol> <li>Generate a notebook config file using <code>jupyter notebook --generate-config</code>. [official docs]</li> <li>Prepare a password using <code>jupyter notebook password</code>. [official docs]</li> <li> <p>Set up SSL for an encrypted connection. For now create a self-signed certificate using the following command. [official docs]</p> <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem\n</code></pre> <p>Warning</p> <p>When you connect to your Jupyter Server, your browser will warn you that the connection may be insecure. This is because self-signed certificates are not trusted by your operating system's root certificates. It is possible to fix this with some additional work using notes at the official docs. Generally the security warning can be bypassed without issue in this case.</p> </li> </ol> </li> <li> <p>Configure the notebook server by locating lines like the following in <code>~/.jupyter/jupyter_notebook_config.py</code> and updating them with the right-hand side of each variable assignment (equals sign <code>=</code>). This file was created as part of the first step of these instructions. [official docs]</p> <p>Note</p> <p>If you used <code>jupyter notebook password</code> the hashed password will be located in <code>jupyter_notebook_config.json</code> instead of <code>.py</code>.</p> <p>Note</p> <p>The lines below may not appear together depending on the version of Jupyter installed. The file <code>jupyter_notebook_config.py</code> contains over a thousand lines. You may need to search using a text editor search or find feature. If you are using <code>nano</code> please visit our nano page to learn how to search.</p> <pre><code>c.NotebookApp.certfile = u'/absolute/path/to/your/certificate/mycert.pem'\nc.NotebookApp.keyfile = u'/absolute/path/to/your/certificate/mykey.key'\nc.NotebookApp.ip = '*'\nc.NotebookApp.password = u'sha1:bcd259ccf...&lt;your hashed password here&gt;'\nc.NotebookApp.open_browser = False\n\n# It is a good idea to set a known, fixed port for server access\nc.NotebookApp.port = 9999\n</code></pre> </li> <li> <p>Start the server with <code>jupyter notebook</code>.</p> </li> <li>Access the server with the browser on your local machine by navigating to <code>https://&lt;floating-ip&gt;:&lt;port&gt;</code>. In this case the port was set to be <code>9999</code>, and <code>&lt;floating-ip&gt;</code> comes from the prerequisites for this section. The port must match that used for the security group to allow traffic between your local machine and the cloud instance. You must also be on the UAB Campus VPN.</li> </ol> <p>Important</p> <p>Some browsers may default to using <code>http</code> instead of <code>https</code> when given a raw IP address. Make sure to fully type out <code>https://&lt;floating-ip&gt;:&lt;port&gt;</code>.</p>"},{"location":"uab_cloud/remote_access/","title":"Remote Access to Instances","text":"<p>All of the access methods described below are built on top of <code>ssh</code> and require completion of the steps in Basic Security Setup to use with <code>cloud.rc</code>. Some of these steps are referenced in that document.</p>"},{"location":"uab_cloud/remote_access/#command-line-via-ssh","title":"Command Line via SSH","text":"<p>SSH stands for Secure SHell and is a powerful tool for executing terminal commands on remote machines. It is widely used and ubiquitous, and a number of other technologies are built on top of SSH, like <code>sftp</code> and <code>scp</code> for transferring files. It is also the primary mode of command line communication with Research Computing technologies like Cheaha and cloud.rc.</p>"},{"location":"uab_cloud/remote_access/#install-an-ssh-client","title":"Install an SSH Client","text":"<p>There are two main steps to working with SSH efficiently. The first is to ensure you have an SSH client installed, which will let your local machine communicate with remote machines. The second is to ensure you have <code>ssh-agent</code> running in each terminal window to automate management of key files. The <code>ssh-agent</code> software comes with most SSH clients, but does not always run automatically. How to start the <code>ssh-agent</code> software automatically varies depending on operating system and shell flavor, which we will describe below.</p>"},{"location":"uab_cloud/remote_access/#terminal-multiplexers","title":"Terminal Multiplexers","text":"<p>Terminal multiplexers are software that aggregate multiple SSH client sessions into one location. They often have the added benefit of keeping sessions alive on the remote machine, so short internet outages won't require you to log in again.</p> <ul> <li>For Linux, try using <code>tmux</code>: https://github.com/tmux/tmux/wiki/Installing#installing-tmux</li> <li>For Windows, try using Windows Terminal: https://apps.microsoft.com/detail/9n0dx20hk701?hl=en-us&amp;gl=US</li> </ul>"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-linux","title":"Install an SSH Client (Linux)","text":"<p>Virtually all Linux distributions come with SSH preinstalled and configured appropriately for ease of use, including automatically starting the <code>ssh-agent</code>.</p>"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-macos","title":"Install an SSH Client (MacOS)","text":"<p>MacOS comes with an SSH client installed.</p> <p>If you are on version Leopard <code>10.5.1</code> or lower, you may want to have the <code>ssh-agent</code> start automatically using the command <code>sudo touch /var/db/useLS</code> at a terminal window. Versions newer than Leopard <code>10.5.1</code> start the <code>ssh-agent</code> automatically.</p>"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-windows","title":"Install an SSH Client (Windows)","text":"<p>There are several options for installing an SSH client on Windows, described below. It is highly recommended to install Windows Subsystem for Linux (WSL) as it provides a complete Linux environment within Windows.</p>"},{"location":"uab_cloud/remote_access/#windows-subsystem-for-linux-wsl","title":"Windows Subsystem For Linux (WSL)","text":"<p>Follow the instructions starting here to install Windows Subsystem for Linux.</p> <p>WSL shells do not automatically start or share the <code>ssh-agent</code>. To fix this we recommend installing <code>keychain</code> to automatically manage the <code>ssh-agent</code>. Run the following command depending on your Linux distribution.</p> <ul> <li>DEB-based (Debian, Ubuntu): <code>sudo apt install keychain</code></li> <li>RPM-based (CentOS, Fedora, openSUSE): <code>sudo yum install keychain</code></li> </ul> <p>Then modify the <code>.*rc</code> file for your shell, generally <code>.bashrc</code> or <code>.zshrc</code>, to automate the <code>ssh-agent</code> by adding the following line.         - <code>eval `keychain -q --eval --agents ssh`</code></p> <p>Tip</p> <p>You can access WSL files from within Windows in two ways.</p> <p>In the WSL terminal, enter <code>explorer.exe .</code> to open a File Explorer window in the current directory.</p> <p>In Windows, open a File Explorer window, click in the top navigation bar and enter <code>\\\\wsl$</code>. Then select your distribution from the file window to access the filesystem of that WSL operating system.</p>"},{"location":"uab_cloud/remote_access/#openssh-for-windows","title":"OpenSSH for Windows","text":"<p>Follow the instructions here to install the OpenSSH client. The client software is all that is needed to connect with instances from your Windows PC. Only install the OpenSSH server if you are sure you need it (this is very uncommon). The instructions at the link should work for Windows 10 and Windows 11.</p> <p>Once the OpenSSH client is installed, you'll want to enable the OpenSSH Agent service on your local machine to streamline adding and using keys.</p> <ul> <li>Open the Start Menu and search for \"Services\", and open the result shown in the image.     </li> <li>Find the \"OpenSSH Authentication Agent\" service in the list. Double click it, or right-click it and select \"Properties\".     </li> <li>In the dialog box, under the \"General\" tab, look for \"Startup Type\". Click the drop-down menu and select \"Automatic (Delayed Start)\". Click \"Apply\" at the bottom-right corner. This will cause the <code>ssh-agent</code> service to start when Windows starts.</li> <li>The \"Start\" button under the horizontal line should become enabled. Click it to start the <code>ssh-agent</code> service now.     </li> </ul>"},{"location":"uab_cloud/remote_access/#git-bash-terminal-git-for-windows","title":"Git Bash terminal (Git for Windows)","text":"<p>The fine folks at Git have worked very hard to package everything needed to use Git on Windows into one installer. This includes a Linux command line interface emulator, Bash and SSH. Visit https://git-scm.com to download and install. Follow the installer instructions. It is recommended to use all of the default installation options. Once installed, locate \"Git Bash\" on your machine to open the Bash terminal. It should be searchable in the Start Menu.</p> <p>To automate running <code>ssh-agent</code> add the following block to the file <code>.bash_profile</code> in the <code>~</code> directory within Git Bash. Then use <code>source .bash_profile</code> to start the <code>ssh-agent</code>, or open a new terminal.</p> <p>Note</p> <p>If such a file does not exist, please add it using <code>nano .bash_profile</code> to create a new file in the nano text editor. Copy and paste the block below into the text editor window. The character <code>^</code> means Ctrl. Use <code>^x</code> (Ctrl+X) to exit, and continue following the prompts to save the file, using Ctrl shortcuts as needed.</p> <pre><code>env=~/.ssh/agent.env\n\nagent_load_env () { test -f \"$env\" &amp;&amp; . \"$env\" &gt;| /dev/null ; }\n\nagent_start () {\n    (umask 077; ssh-agent &gt;| \"$env\")\n    . \"$env\" &gt;| /dev/null ; }\n\nagent_load_env\n\n# agent_run_state: 0=agent running w/ key; 1=agent w/o key; 2= agent not running\nagent_run_state=$(ssh-add -l &gt;| /dev/null 2&gt;&amp;1; echo $?)\n\nif [ ! \"$SSH_AUTH_SOCK\" ] || [ $agent_run_state = 2 ]; then\n    agent_start\n    ssh-add\nelif [ \"$SSH_AUTH_SOCK\" ] &amp;&amp; [ $agent_run_state = 1 ]; then\n    ssh-add\nfi\n</code></pre> <p>Tip</p> <p>Git Bash can also be used with Windows Terminal using this stackoverflow answer: https://stackoverflow.com/questions/56839307/adding-git-bash-to-the-new-windows-terminal/57369284.</p>"},{"location":"uab_cloud/remote_access/#generating-key-pairs","title":"Generating Key Pairs","text":"<p>The instructions for generating key pairs are identical for all operating systems. GitHub maintains excellent documentation on generating key pairs. The gist of those instructions follows.</p> <ol> <li>Open a terminal window.</li> <li>Use the command <code>ssh-keygen -t ed25519 -C \"your_email@example.com\"</code></li> <li>You will be prompted to choose a location to store the key, including a file name.</li> <li>You will be prompted to enter a passphrase to secure the key. It is highly recommended to secure your key pair with a passphrase to minimize risk.</li> </ol>"},{"location":"uab_cloud/remote_access/#managing-keys","title":"Managing Keys","text":"<p>The instructions below are the same for all operating systems with one small exception noted below.</p> <p>Important</p> <p>If at any point you encounter an error like below, please check to be sure your <code>ssh-agent</code> is running based on how you Installed your SSH Client.</p> <pre><code>Could not open a connection to your authentication agent.\n</code></pre>"},{"location":"uab_cloud/remote_access/#starting-the-ssh-agent-for-a-single-session","title":"Starting the SSH Agent for a Single Session","text":"<p>If <code>ssh-agent</code> isn't already running and you encounter an error, use the following commands to start the <code>ssh-agent</code> depending on your environment. It is highly recommended to use the most appropriate method described in Install an SSH Client to have <code>ssh-agent</code> start automatically.</p> <ul> <li>Linux, MacOS, Git Bash, WSL: <code>eval $(ssh-agent -s)</code></li> <li>Windows OpenSSH: <code>start-ssh-agent</code></li> </ul>"},{"location":"uab_cloud/remote_access/#add-a-private-key","title":"Add a Private Key","text":"<ul> <li>Move the key file to the <code>.ssh</code> directory under your home directory.</li> <li>Navigate to the <code>.ssh</code> folder in a terminal window.</li> <li> <p>Run <code>ssh-add &lt;private_key_file&gt;</code></p> <p></p> <p>Bug</p> <p>For Linux users and WSL on Windows users. If you experience a <code>Warning: Unprotected Private Key File</code> error when using <code>ssh-add</code>, your <code>ssh</code> file and directory permissions may be incorrect. To fix, please use the following commands.</p> <pre><code>sudo chmod 600 ~/.ssh/&lt;private_key_file&gt;\nsudo chmod 644 ~/.ssh/known_hosts  # if you have ever connected to a remote machine\nsudo chmod 644 ~/.ssh/config  # if you have a config file\nsudo chmod 755 ~/.ssh\n</code></pre> <p>Tip</p> <p>MacOS allows storing passphrases to the builtin Keychain with a special flag. Use <code>ssh-add -K &lt;path/to/private_key_file&gt;</code> to permanently store the passphrase that goes with the key file.</p> </li> </ul>"},{"location":"uab_cloud/remote_access/#remove-a-private-key","title":"Remove a Private Key","text":"<p>Run <code>ssh-add -d &lt;path/to/private_key_file&gt;</code></p>"},{"location":"uab_cloud/remote_access/#push-a-new-public-key-file-to-a-remote-machine","title":"Push a New Public Key File to a Remote Machine","text":"<p>To push a new public key file to a remote machine, please use the <code>ssh-copy-id</code> command. If your <code>ssh-agent</code> is running and has a known-good private key added, then the command below will work as expected and add the <code>&lt;new_public_keyfile&gt;.pub</code> to the remote machine. You must also have the private key counterpart <code>&lt;new_private_keyfile&gt;</code> with the same name as the public key file, without the <code>.pub</code> extension.</p> <pre><code>ssh-copy-id -i ~/.ssh/&lt;new_public_keyfile&gt; &lt;user&gt;@&lt;remote_ip&gt;\n</code></pre> <p>The value <code>&lt;user&gt;</code> should be replaced with the remote user you will login as. The value <code>&lt;remote_ip&gt;</code> should be replaced with the IP address of the remote machine.</p> <p>To verify, use <code>ssh -i ~/.ssh/&lt;new_private_keyfile&gt;.pub &lt;user&gt;@&lt;remote_ip&gt;</code>.</p>"},{"location":"uab_cloud/remote_access/#remove-an-invalid-host-fingerprint","title":"Remove an Invalid Host Fingerprint","text":"<p>Danger</p> <p>The following command should only be run when reusing a floating IP for a new instance in a cloud context. Using it arbitrarily for remote machines you do not control can result in a security breach. Be absolutely certain you trust the source of the key change.</p> <p>A \"Remote Host Identification Has Changed\" error can be resolved by using the following command. It looks like the image below.</p> <p>Run <code>ssh-keygen -R &lt;hostname&gt;</code> where <code>&lt;hostname&gt;</code> is the URL or IP address of the remote machine.</p> <p></p>"},{"location":"uab_cloud/remote_access/#setting-up-a-configuration-file","title":"Setting up a Configuration File","text":"<p>SSH configuration files help streamline the process of logging in to remote terminals by storing commonly-used arguments and flags for each host. To create a configuration file, navigate to your <code>.ssh</code> directory. Create a new plain text file called <code>config</code> with no extension. Open the file and add content like the following. Note that indent matters. Variable values in <code>&lt;&gt;</code> will be replaced with appropriate values before saving.</p> <pre><code>Host &lt;host&gt;\n  HostName &lt;remote_ip&gt;\n  User &lt;user&gt;\n  IdentityFile &lt;absolute_path_to_private_key_file&gt;\n</code></pre> <ul> <li>Be sure to give a meaningful name under <code>&lt;host&gt;</code> so you can easily refer back to this config later and for ease of typing when using <code>ssh</code> with this configuration. Only letters, numbers, dashes and underscores are allowed, and it must start with a letter.</li> <li>The value <code>&lt;remote_ip&gt;</code> can be any remote machine relevant to your work. For cloud.rc it should be whatever IP was assigned in Creating a Floating IP.</li> <li>The value <code>&lt;user&gt;</code> should be whatever user name you will log in as. For cloud.rc, <code>ubuntu</code> or <code>centos</code> are typical, depending on instance operating system.</li> <li>The value <code>&lt;path_to_private_key_file&gt;</code> is the absolute path to the private key file, e.g. the path to your <code>.ssh</code> folder followed by the <code>&lt;private_key_file&gt;</code> file name. For cloud.rc this will be whatever private key file was generated in Creating a Key Pair.</li> </ul> <p>Save the <code>config</code> file. Start a new terminal and use the command <code>ssh &lt;host&gt;</code>, with no other flags, to test.</p>"},{"location":"uab_cloud/remote_access/#ssh-client-usage","title":"SSH Client Usage","text":"<p>If you've Set up a Configuration File, simply use <code>ssh &lt;host&gt;</code>, using the configuration name, to connect.</p> <p>If you haven't set up a configuration file, use the following.</p> <pre><code>ssh &lt;user&gt;@&lt;remote_ip&gt; -i &lt;private_key_file&gt;\n</code></pre> <p>Where <code>user</code> is the remote username, <code>remote_ip</code> is the IP address of the remote machine, and <code>&lt;private_key_file&gt;</code> is the private key file used for access the remote machine. See Generating Key Pairs for general instructions on creating a key pair, or Creating a Key Pair for cloud.rc specific instructions.</p>"},{"location":"uab_cloud/remote_access/#make-instances-publically-accessible-from-the-internet","title":"Make Instances Publically Accessible From the Internet","text":"<p>It is possible to make instances publically accessible from the external internet. Floating IPs are pulled from a limited and fixed pool of public IP addresses assigned from the overall UAB IP pool. By default, these IP addresses are unable to communicate beyond the UAB Internet Border firewall, for security reasons. To make your instance publically accessible, a Firewall Security Exception must be filed. The result of the security exception is to create a firewall rule to allow traffic between the internet and an application on your instance. This section will go over how to make your instance publically accessible.</p>"},{"location":"uab_cloud/remote_access/#expectations","title":"Expectations","text":"<p>The expectation of making an instance publically accessible is to advance UAB's mission, so be sure you've configured and thoroughly tested your instance in the UAB Network before proceeding. The following list is intended as a helpful reminder.</p> <ul> <li>Have an instance with some research application or server that advances UAB's mission.</li> <li>The instance is configured with a floating IP address.</li> <li>The server and any applications follow appropriate UAB IT policies.</li> <li>For public-facing portions of the server which requiring login information, each authorized user must have their own independent credentials.</li> <li>Thoroughly test your application on the UAB Campus Network prior to requesting public access.</li> </ul>"},{"location":"uab_cloud/remote_access/#process-for-granting-public-access","title":"Process For Granting Public Access","text":"<p>Proceed to the UAB IT Security Exception (Firewall Rule Change) form at UAB ServiceNow and fill it in. You may need to login with your UAB BlazerID credentials. We have included information on a few of the</p> <ul> <li> <p>For the Section \"Firewall\" there are three checkbox options:UAB Internet Border; Department/Internal; UAB IT Data Center. Be sure only \"UAB Internet Border\" is checked.</p> <p></p> </li> <li> <p>For the Section \"System/Application Information\" there are two checkbox options.</p> <ul> <li>Be sure \"Is this a mission critical system?\" is left unchecked. For IT purposes, mission critical systems include things like the Outlook Exchange server, BlazerNet, Taleo, and other similar systems affecting University operations.</li> <li>If your application involves the storage, processing or acquisition of sensitive or restricted/PHI data then you must check \"Does this system/application store or process sensitive/restricted data?\".<ul> <li>If you checked this box, please select the type or types in the next section \"What type of sensitive or restricted data is stored or processed?\"</li> <li>IMPORTANT As of Feb 29, 2024, Cloud.rc is not suitable for use with restricted/PHI data.</li> </ul> </li> </ul> </li> <li>For the Section \"Justification/Description\", in addition to your own description, be sure to mention that your application \"Involves one or more virtual machine instances on the UAB IT Research Computing Cloud.rc platform\".</li> </ul> <p>When you have completed the form, press the \"Submit\" button to make the request.</p>"},{"location":"uab_cloud/remote_access/#configuring-security-groups-for-server-software","title":"Configuring Security Groups for Server Software","text":"<p>Remotely accessing server software, publicly or otherwise, requires configuration of Security Groups to open ports the server will communicate on. Please see our information on Installing Server Software for more details.</p>"},{"location":"uab_cloud/remote_access/#special-notes-for-web-page-servers-and-apps","title":"Special Notes for Web Page Servers and Apps","text":"<p>Dynamic web pages may be served from OpenStack instances. Doing so requires the instance and its floating IP being granted public access. Additionally, web page servers require an SSL Certificate. It may also interest reseachers to know that Custom Domains are possible for your applications.</p>"},{"location":"uab_cloud/remote_access/#https-and-web-page-applications-require-an-ssl-certificate","title":"HTTPS and Web Page Applications Require an SSL Certificate","text":"<p>If your application serves a web page over HTTPS, you will need to obtain an SSL Certificate. UAB IT provides this service at no additional cost. To obtain a certificate, please visit the UAB IT SSL Server Certificate service page and follow the instructions there.</p>"},{"location":"uab_cloud/remote_access/#custom-domains-for-https-and-web-page-applications","title":"Custom Domains for HTTPS and Web Page Applications","text":"<p>For applications serving a web pages over HTTPS, it is possible to request a custom domain name like \"department.uab.edu\". This type of domain name is called a third-level domain, and there is a one-time fee charged by UAB IT to set this up. Once the third-level domain is set up, fourth-level domains like \"group.department.uab.edu\" may be set up at no additional cost.</p> <p>To learn more about setting up a third-level domain, please have your department's IT Network Contact visit the ServiceNow Knowledge Base Article.</p> <p>Note</p> <p>The above information about costs is true as of March 2024. Specifics may change over time.</p>"},{"location":"uab_cloud/remote_access/#data-transfer","title":"Data Transfer","text":""},{"location":"uab_cloud/remote_access/#scp","title":"SCP","text":"<p>SCP stands for Secure CoPy and works like the <code>cp</code> command, but allows transferring files and directories with remote machines. SCP is built on top of SSH and is installed with most SSH Clients. To install SCP, see Install an SSH Client.</p> <p>Warning</p> <p>The OpenSSH developers recommend using SFTP instead of SCP. Future releases of OpenSSH will have SCP use SFTP protocol.</p> <p>The value <code>&lt;user&gt;</code> is the user you will login as on the remote machine <code>&lt;hostname&gt;</code>. Note that if you are using an SSH Configuration File with Host <code>&lt;host&gt;</code>, then replace all of <code>&lt;user&gt;@&lt;hostname&gt;</code> with just <code>&lt;host&gt;</code>, as you would with SSH.</p> <pre><code>scp &lt;source_file&gt; &lt;user&gt;@&lt;hostname&gt;:&lt;destination_file&gt;  # single file\nscp &lt;source_file&gt; &lt;user&gt;@&lt;hostname&gt;:  # retains the file name\n\nscp -r &lt;source_directory&gt; &lt;user&gt;@&lt;hostname&gt;:&lt;destination_directory&gt;  # full directory\nscp -r &lt;source_directory&gt; &lt;user&gt;@&lt;hostname&gt;:  # retains the directory name\n</code></pre> <p>Examples:</p> <pre><code># file\nscp script.py cheaha:\nscp script.py user@cheaha.rc.uab.edu:~/existing/shared_script.py\nscp script.py user@&lt;cloud_vm_ip&gt;:\n\n# directory\nscp -r my_scripts/ user@cheaha.rc.uab.edu:\nscp -r my_scripts/ cheaha:my_shared_scripts/\n</code></pre>"},{"location":"uab_cloud/remote_access/#sftp","title":"SFTP","text":"<p>SFTP stands for Secure File Transfer Protocol and allows transferring files and directories with remote machines. SFTP is built on top of SSH and is installed with most SSH Clients. To install SFTP, see Install an SSH Client.</p> <p>SFTP works differently from SCP, as it has an interactive prompt. When connected to a remote, the prompt <code>sftp&gt;</code> will appears and enable use of SFTP commands. SFTP can also be used in batch mode with the <code>-b &lt;batch_file&gt;</code> argument. The plaintext <code>&lt;batch_file&gt;</code> should contain one SFTP command per line.</p> <p>To connect, use <code>sftp &lt;user&gt;@&lt;hostname&gt;</code> where <code>&lt;user&gt;</code> is the user you will login as on the remote machine <code>&lt;hostname&gt;</code>. If you are using an SSH Configuration File with Host <code>&lt;host&gt;</code>, you may use <code>sftp &lt;host&gt;</code>. You may optionally use <code>sftp &lt;host&gt;:/path/to/dir</code> to start in a specific directory.</p> <p>Some examples of commands are given below. A complete list is available here</p> <pre><code># general commands\nsftp&gt; pwd  # remote current directory\nsftp&gt; lpwd  # local current directory\n\nsftp&gt; ls  # contents of remote pwd\nsftp&gt; lls  # contents of local pwd\n\nsftp&gt; mkdir my_dir  # create my_dir on remote\nsftp&gt; lmkdir my_dir  # create my_dir on local\n\nsftp&gt; cd my_dir  # change directory on remote\nsftp&gt; lcd my_dir  # change directory on local\n\n\n# copy to remote\nsftp&gt; put &lt;local_file&gt; &lt;optional_remote_path&gt;\nsftp&gt; put -r &lt;local_directory&gt; &lt;optional_remote_path&gt;\n# if optional remote path is not supplied, uses pwd\n\n# file\nsftp&gt; put script.py  # copies file to pwd, same name\nsftp&gt; put script.py shared_script.py  # copies file to pwd, renames\nsftp&gt; put script.py all_scripts/  # copies to pwd existing subdirectory of pwd\n\n# directory, must use trailing '/' character!\nsftp&gt; put -r my_scripts/  # copies directory to pwd\nsftp&gt; put -r my_scripts/ all_scripts/  # copies to existing subdirectory of pwd\n\n\n# copy from remote, same syntax as put, reversed direction\nsftp&gt; get &lt;remote_file&gt; &lt;optional_local_path&gt;\nsftp&gt; get -r &lt;remote_directory&gt; &lt;optional_local_path&gt;\n# if optional local path is not supplied, uses lpwd\n</code></pre>"},{"location":"uab_cloud/remote_access/#rclone","title":"RClone","text":"<p>Please see our RClone page for more information on using RClone with the SFTP remote option.</p>"},{"location":"uab_cloud/sharing_cloud_environment/","title":"What is a Project Space?","text":"<p>UAB's cloud infrastructure is based on OpenStack cloud software. A project space is referred to as the cloud environment that houses projects on OpenStack. Providing the platform to manage cloud resources for storage, security and networking via a web interface. Every UAB Cloud user has their Individual Cloud Environment that is their personal project space. An individual can also be part of a shared project space which is referred to as a Shared Cloud Environment. Both environments are OpenStack Project Spaces, with the exception that the shared cloud environment allows multiple UAB cloud users to collaborate in the shared project space. Dependent on an individual's UAB status, they can request that a Shared Cloud environment be created for their Lab and/or Core, this shared project space will include members of their research lab, or persons they want to collaborate with. This article provides information on why using a shared cloud environment is important, how to get an additional shared OpenStack Project (Shared Cloud Environment) and how best to utilize this environment.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#why-do-i-need-a-shared-cloud-environment","title":"Why Do I Need a Shared Cloud Environment","text":"<p>The dedicated Shared Cloud Environment available on UAB Cloud offer research Labs and/or Cores a number of solutions for managing their various and intensive computational needs. These Shared Cloud Environments will provide a secure and collaborative environment where members can access, share, and manage resources efficiently. By using this resource, your organization can ensure that their research projects have the necessary computing power and flexibility to advance scientific inquiry, facilitate collaboration, and optimize resource utilization. The benefits of having a Shared Cloud Environment on UAB Cloud are;</p> <ol> <li>Improved collaborations between members of your lab, as they all have access to the same resources and data to facilitate research.</li> <li>Specific environments can be created from Instances that support highly specialized research tools only available on particular OS, and hardware. These specific environments can be created to significantly improve compute times for your research.</li> <li>Use of the Cloud platform can help your Lab save on a number of operations costs, particularly around the procurement and maintenance of resources.</li> <li>Your Lab can scale their project resources down or up dependent on research needs with ease and without significant delays, this way you avoid incurring additional costs for purchasing new hardware or under utilize already purchased resources.</li> <li>Created Instances in your Shared Cloud Environment, provide security and privacy that can help to further protect your research data.</li> </ol> <p>The benefits of creating and using a Shared Cloud Environment for your Lab/Core are way more than those mentioned above, but the above capture the purpose to which this resource helps to improve your collaboration in producing high quality research.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#how-do-i-create-a-shared-cloud-environment-for-my-labcore","title":"How Do I Create a Shared Cloud Environment for My Lab/Core","text":"<p>To use the shared cloud resources available, you will need to send in a request to the UAB IT Research Computing Team via email support@listserv.uab.edu. In the email, please state clearly your needs and the resources you would require for your lab. Your request should also include members of your lab to be included in the Shared Cloud Environment, and a preferred name (usually same as lab project folder on Cheaha). Please follow the naming conventions for requesting a shared storage on our platforms in your request.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#how-do-i-switch-project-spaces","title":"How Do I Switch Project Spaces?","text":"<p>As a UAB Cloud user, you can easily switch between your Individual Cloud Environment and other Shared Cloud Environments you are a part of. From the dashboard of the homepage after login, navigate to the \"Domain\" and \"Projects\" drop down button, located in the top pane (upper left quadrant of the page) as shown in the image below. You can then select from the list of project spaces you belong to.</p> <p></p> <p>The project space with your <code>BlazerID</code> is your Individual Cloud Environment, other project spaces listed are the Shared Cloud Environments for Labs, Cores or Projects you are a part of. As of this time only a Lab Principal Investigator (PI), and/or a Research Core Director can request for a shared Shared Cloud Environment.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#creating-an-instance-in-a-shared-cloud-environment","title":"Creating an Instance in a Shared Cloud Environment","text":"<p>The steps for creating an Instance in a project are the same as creating an Instance in your Individual Cloud Environment, you can find a detailed guide here in our Cloud Tutorial. You would however, need to contact us to create a Shared Cloud Environment for your lab, as well as add users who you would like to be members of the Shared Cloud Environment.</p> <p>All members of a Shared Cloud Environment can see (and manage) all Instances within the Shared Cloud Environment. They can do this by navigating through the landing page that doubles as the dashboard, the image below highlights the exact section. While members can see and create images or snapshots of an Instance, not all members can access the Instance. To do this, SSH key pairs would have to be created and added, see section on Using a Key Pair to SSH.</p> <p></p> <p>When in the Shared Cloud Environment, select \"Compute\" and then \"Instances\" to see available Instances in your Shared Cloud Environment.</p> <p>Note</p> <p>Please note, all members of your Shared Cloud Environment, can create, and delete Instances. In essence, all members of the Shared Cloud Environment, have the same user privileges.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#sharing-an-instance-in-a-shared-cloud-environment","title":"Sharing an Instance in a Shared Cloud Environment","text":"<p>There are a couple of ways to collaborate while using an Instance, the sections below show some easy-to-use ways to use an Instance in a Shared Cloud Environment.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#creating-an-image-from-an-instance-using-snapshots","title":"Creating an Image From an Instance Using Snapshots","text":"<p>Snapshots in OpenStack are used like Images, thereby making it relatively easy to create Images from an Instance. To create an Image that can then be shared with other members of your Shared Cloud Environment, so they can replicate your Instance, please follow the below steps;</p> <ol> <li> <p>Go to the Compute pane and then navigate to the \"Instances\" tab in your Shared Cloud Environment dashboard. You will then see a list of available Instances.</p> <p></p> </li> <li> <p>Select the Instance you want to create an Image for. Under the Actions, click on \"Create Snapshot\".</p> <p></p> </li> <li> <p>Insert a name for the Snapshot, and then click \"Create Snapshot\".</p> <p></p> </li> <li> <p>You will be redirected to the <code>Images</code> page, where you newly created Image will appear amongst a list of other available images.</p> <p></p> </li> </ol> <p>The created Image can then be launched, following the same instructions for creating an Instance. This method would be most ideal if you want to recreate an environment for performing an analysis, but would prefer the workflow be run on different VMs, or to separate datasets or create some form of access restriction on particular research.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#creating-an-image-from-a-volume","title":"Creating an Image From a Volume","text":"<p>There are detailed instructions on how to create an image from a Volume, here.</p>"},{"location":"uab_cloud/sharing_cloud_environment/#using-a-key-pair-to-ssh","title":"Using a Key Pair to SSH","text":"<p>Another way to access a created Instance would be to create a public key and private key for your local machine, and then share this public key with the creator of the Instance. As creator of the Instance you would need to add individual public keys of persons who you would want to access the created VM into the <code>authorized_keys</code> file. You can edit this file by using the command below, add the shared public key in a new line inside the file (copy and paste). Save the file and follow instructions here for remote accessing your Instance using SSH.</p> <pre><code># access and edit the file using\n\nnano cd ~/.ssh/authorized_keys\n</code></pre> <p>Alternatively you can share the private key file you created for the Instance, with members of your Shared Cloud Environment. But in this case, your project members would have to save the shared private key (usually a <code>.pem</code> file) in their <code>$HOME/.ssh</code> folder. Members of your Shared Cloud Environment can SSH into the shared Instance, by doing the following;</p> <ol> <li> <p>Add the private key into your <code>~/.ssh</code> folder and run the command below.</p> <pre><code>    ssh-add ~/.ssh/&lt;keypair_filename&gt;.pem\n</code></pre> </li> <li> <p>Change the file permission.</p> <pre><code>    sudo chmod 600 ~/.ssh/&lt;keypair_filename&gt;.pem\n</code></pre> </li> </ol> <p>Note</p> <p>Please note Images created from an Instance would inherit the key-pair of the parent Instance.</p>"},{"location":"uab_cloud/snapshots/","title":"Working with Snapshots","text":"<p>Snapshots are instances or volumes frozen at a moment in time, able to be used in the future. Think of snapshots as a photograph of the state of an instance or volume. Anything done to an instance or volume after the snapshot is taken won't affect the snapshot. We can also create a new instance or volume from an existing snapshot, and continue from that point in time.</p> <p>An instance snapshot is referred to as an image. Volume snapshots do not have a special name.</p>"},{"location":"uab_cloud/snapshots/#images-or-instance-snapshots","title":"Images or Instance Snapshots","text":""},{"location":"uab_cloud/snapshots/#creating-an-image","title":"Creating an Image","text":"<p>Images are a helpful way to store the state of an instance for later use. Repeating tedious tasks like Software Installs can be avoided by taking a snapshot at a known-good point during set up of an instance environment, saving time in the future if something goes wrong. Images may also be shared with other users to simplify workflows and onboarding new collaborators. To create an image please follow the steps below. We assume you are already logged in at cloud.rc</p> <ol> <li>Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page.</li> <li> <p>To take a snapshot of a particular instance, click the drop down menu under the \"Actions\" column in the row of the desired instance. Then click \"Create Snapshot\".</p> <p></p> </li> <li> <p>A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions.</p> <p></p> </li> <li> <p>You will be taken to the \"Images\" page, where your new image will appear in its own row in the table.</p> <p></p> <p>Note</p> <p>Notice the image has a size of zero bytes, which is expected and does not affect the ability to create instances. Images are a convenience pointer to the underlying volume snapshot, so they have no size themselves. The underlying volume snapshot does have a fixed size. To see the size of the underlying volume snapshot, click \"Volumes\" and then \"Snapshots\" in the left hand navigation menu.</p> </li> </ol>"},{"location":"uab_cloud/snapshots/#creating-an-instance-from-an-image","title":"Creating an Instance from an Image","text":"<p>To create an instance from an image, follow the directions below, assuming you have Created an Image.</p> <ol> <li>Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page.</li> <li> <p>Click the \"Launch Instance\" button.</p> <p></p> </li> <li> <p>A dialog box will open. Follow the instructions at Basic Instance Setup until you get to the \"Source\" tab.</p> </li> <li> <p>In the \"Source\" tab, select \"Instance Snapshot\" under the \"Select Boot Source\" drop down menu.</p> <p></p> </li> <li> <p>The \"Available\" table will change, and should contain your previously created instance snapshots.</p> </li> <li> <p>Press the up arrow in the appropriate row of the \"Available\" table to move that instance snapshot to the \"Allocated\" table.</p> <p></p> <p>Note</p> <p>On the \"Flavor\" tab, only flavors with large enough disk capacity to hold the snapshot will be allowed. Flavors that are too small will show a yellow triangular caution symbol. Examples are shown below for a 40 GB instance snapshot.</p> <p></p> </li> <li> <p>Continue following the instructions at Basic Instance Setup to start the instance.</p> </li> </ol>"},{"location":"uab_cloud/snapshots/#deleting-an-image","title":"Deleting an Image","text":"<p>To delete an image, return to the \"Images\" page using the left-hand navigation pane. In the table, find the row with the image you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Image\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Image\" again to delete the image permanently.</p> <p></p> <p>Important</p> <p>You will not be able to delete the image if it has an associated volume snapshot or volume. They will need to be removed or deleted first.</p>"},{"location":"uab_cloud/snapshots/#volume-snapshots","title":"Volume Snapshots","text":""},{"location":"uab_cloud/snapshots/#creating-a-volume-snapshot","title":"Creating a Volume Snapshot","text":"<p>Volume snapshots are a helpful way to store the state of a volume for later use. They are used as the backing for Images, or Instance Snapshots, and have the same benefits. Most volume snapshots are created as part of an instance, but to create a volume snapshot directly please follow the steps below. We assume you are already logged in at cloud.rc</p> <ol> <li>Navigate to \"Volumes\" and then \"Volumes\" in the left-hand navigation menu to open the \"Volumes\" page.</li> <li> <p>To take a snapshot of a particular volume, click the drop down menu under the \"Actions\" column in the row of the desired volume. Then click \"Create Snapshot\".</p> <p></p> </li> <li> <p>A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions.</p> <p></p> </li> <li> <p>You will be taken to the \"Volume Snapshots\" page, where your new snapshot will appear in its own row in the table.</p> <p></p> </li> </ol>"},{"location":"uab_cloud/snapshots/#deleting-a-volume-snapshot","title":"Deleting a Volume Snapshot","text":"<p>To delete a volume snapshot, return to the \"Volume Snapshots\" page using the left-hand navigation pane. In the table, find the row with the volume snapshot you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume Snapshot\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Volume Snapshot\" again to delete the volume snapshot permanently.</p> <p></p>"},{"location":"uab_cloud/tutorial/","title":"cloud.rc Tutorial","text":"<p>Welcome to the cloud.rc tutorial!</p> <p>Through this tutorial we will show you how to use cloud.rc to create your very own VM for scientific research on our OpenStack cloud service. The tutorial works best if you read and follow the instructions carefully, and follow the steps in order.</p> <p>Note</p> <p>Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.</p>"},{"location":"uab_cloud/tutorial/#prerequisites","title":"Prerequisites","text":"<p>To access cloud.rc, you must either be on the UAB Campus Network or the UAB Campus VPN.</p> <p>Please visit https://cloud.rc.uab.edu to ensure you are able to access the site.</p> <p>You will also need an account created for you by our team. If you believe you need an account but do not have one, please Contact Us.</p>"},{"location":"uab_cloud/tutorial/#tutorial-sections","title":"Tutorial Sections","text":"<ol> <li>Network - To do anything meaningful with a VM, it will need to be accessible to you and any collaborators. To make it accessible, you will first need to construct a virtual network by which it can communicate with the UAB Campus Network and beyond. This section will show you how to set up a bare-minimum network configuration.</li> <li>Security - Being able to communicate with the internet means we must take security precautions. This is not only good practice, but an IT Policy requirement. Some layers of security are enforced by how we've configured the cloud service. Your responsibility starts with ensuring the VM is only accessible on specific ports and by specific people. This section will show you how to do that.</li> <li>Instances - Now that we've got a secure network, we are ready to create an instance. Instances are the virtual computing devices used to do the scientific processing for your research. Your responsibility starts with selecting or designing the software you will use, understanding how to install and configure it correctly for your application, and how to use and troubleshoot it. We are not able to provide any support for software within virtual machines.</li> <li>Volumes - Optionally, you can set up persistent storage volumes.</li> </ol>"},{"location":"uab_cloud/tutorial/#advanced-information","title":"Advanced Information","text":"<p>Once you have completed the tutorial, you might read over some of the following information to advance your understanding and facilitate usage of the tutorial.</p> <ul> <li>Remote Access - Different ways to access instances.</li> <li>Installing Software - Good practices and examples of installing common software.</li> <li>Snapshots - Creating reusable custom images based on VMs you've spent time configuring.</li> <li>Make an instance accessible by the public Internet.</li> </ul>"},{"location":"uab_cloud/tutorial/instances/","title":"Instance Setup and Tutorial","text":"<p>Instances are the basic unit of compute on cloud.rc. Requesting an instance involves a number of steps, and requires that a Network has already been setup, along with certain Security settings and features. It is also possible to attach persistent reusable Volumes to instances.</p> <p>Important</p> <p>If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.</p> <p>Note</p> <p>Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.</p>"},{"location":"uab_cloud/tutorial/instances/#creating-an-instance","title":"Creating an Instance","text":"<p>Creating an instance is possibly a step you'll perform often, depending on your workflow. There are many smaller steps to create an instance, so please take care to check all the fields when you create an instance.</p> <p>These instructions require that you've set up a Network and followed all of the instructions on the linked page. You should have a Network, Subnet, Router and Floating IP. You will also need to setup a Key Pair and an SSH Security Group.</p> <ol> <li> <p>Click \"Compute\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Instances\".</p> <p></p> </li> <li> <p>Click \"Launch Instance\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box completely. There are several tabs that will need to be completed.</p> <p></p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#details-tab","title":"Details Tab","text":"<ol> <li>Enter an \"Instance Name\". See Naming Conventions.</li> <li>Enter a \"Description\".</li> <li>Select \"nova\" in the \"Availability Zone\" drop down box.</li> <li>Select \"1\" in the \"Count\" field.</li> <li>Click \"Next &gt;\" to move to the \"Source\" tab.</li> </ol>"},{"location":"uab_cloud/tutorial/instances/#source-tab","title":"Source Tab","text":"<p>Sources determine what operating system or pre-defined image will be used as the starting point for your operating system (OS).</p> <ol> <li>Select \"Image\" in the \"Select Boot Source\" drop down box.</li> <li>Select \"Yes\" under \"Create New Volume\".</li> <li>Choose an appropriate \"Volume Size\" in <code>GB</code>. Note that for many single-use instances, <code>20 GB</code> is more than enough. If you need more because you have persistent data, please create a <code>persistent volume&lt;volume_setup_basic&gt;</code>.</li> <li> <p>Select \"Yes\" or \"No\" under \"Delete Volume on Instance Delete\"</p> <ol> <li>\"Yes\" is a good choice if you don't care about reusing the OS.</li> <li>\"No\" is a good choice if the OS volume will be reused.</li> </ol> <p></p> </li> <li> <p>Pick an image from the list under the \"Available\" section.</p> <ol> <li>Use the search box to help find the image that best suits your research needs.</li> <li>When you find the best image, click the button with an up arrow next to the image.</li> <li>The image will move to the \"Allocated\" section above the \"Available\" section.</li> </ol> <p></p> </li> <li> <p>Click \"Next &gt;\" to move to the \"Flavor\" tab.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#flavor-tab","title":"Flavor Tab","text":"<p>Flavors determine what hardware will be available to your instance, including cpus, memory and gpus.</p> <ol> <li> <p>Pick an instance flavor form the list under the \"Available\" section.</p> <ol> <li>Use the search box to help find the flavor that best suits your needs.</li> <li>When you find the best flavor, click the button with an up arrow next to the flavor.</li> <li>The flavor will move to the \"Allocated\" section above the \"Available\" section.</li> </ol> <p></p> </li> <li> <p>Click \"Next &gt;\" to move to the \"Networks\" tab.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#networks-tab","title":"Networks Tab","text":"<p>Networks determine how your instance will talk to the internet and other instances. If you are following along with the tutorial, you should already have a Network set up. See Network for more information.</p> <ol> <li> <p>Pick a network from the list under the \"Available' section.</p> <ol> <li>A Network may already be picked in the \"Allocated\" section. If this is not the correct Network, use the down arrow next to it to remove it from the \"Allocated\" section. If the Network is correct, skip (ii.) through (iv.).</li> <li>Use the search box to help find the Network that best suits your needs.</li> <li>When you find the best Network, click the button with an up arrow next to the Network.</li> <li>The Network will move to the \"Allocated\" section above the \"available\" section.</li> </ol> <p></p> </li> <li> <p>Click \"Next &gt;\" to move to the \"Network Ports\" tab.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#network-ports-tab","title":"Network Ports Tab","text":"<ol> <li> <p>Leave this tab empty.</p> <p></p> </li> <li> <p>Click \"Next &gt;\" to move to the \"Security Groups\" tab.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#security-groups-tab","title":"Security Groups Tab","text":"<p>Security Groups allow for fine-grained control over external access to your instance. If you are following along with the tutorial, you should already have an \"ssh\" Security Group set up. For more information see Creating a Security Group for more information.</p> <ol> <li>Pick the \"ssh\" Security Group from the \"Available\" section by pressing the up arrow next to it.</li> <li> <p>The \"default\" Security Group should already be in the \"Allocated\" section.</p> <p></p> </li> <li> <p>Click \"Next &gt;\" to move to the \"Key Pair\" tab.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#key-pair-tab","title":"Key Pair Tab","text":"<p>Key Pairs allow individual access rights to the instance via SSH. If you are following along with the tutorial, you should already have a key pair set up. For more information see Creating a Key Pair.</p> <ol> <li> <p>Pick one or more key pairs from the list under the \"Available\"     section.</p> <ol> <li>A Key Pair may already be picked in the \"Allocated\" section. If this is not the correct \"Key Pair\", use the down arrow next to it to remove it form the \"Allocated\" section. If the Key Pair is correct, skip (ii.) through (iv.).</li> <li>Use the search box to help find the Key Pair that best suits your needs.</li> <li>When you find the best Key Pair(s), click the button with an up arrow next to the Key Pair(s).</li> <li>The Key Pair(s) will move to the \"Allocated\" section above the \"Available\" section.</li> </ol> <p></p> </li> <li> <p>Click \"Next &gt;\" to move to the \"Configuration\" tab.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#configuration-tab","title":"Configuration Tab","text":"<ol> <li>Skip this tab.</li> <li>Click \"Next &gt;\" to move to the \"Server Groups\" tab.</li> </ol>"},{"location":"uab_cloud/tutorial/instances/#server-groups-tab","title":"Server Groups Tab","text":"<ol> <li>Skip this tab.</li> <li>Click \"Next &gt;\" to move to the \"Scheduler Hints\" tab.</li> </ol>"},{"location":"uab_cloud/tutorial/instances/#scheduler-hints-tab","title":"Scheduler Hints Tab","text":"<ol> <li>Skip this tab.</li> <li>Click \"Next &gt;\" to move to the \"Metadata\" tab.</li> </ol>"},{"location":"uab_cloud/tutorial/instances/#metadata-tab","title":"Metadata Tab","text":"<ol> <li>Skip this tab.</li> </ol>"},{"location":"uab_cloud/tutorial/instances/#launching-the-instance","title":"Launching the Instance","text":"<p>Click \"Launch Instance\" to launch the instance.</p> <ol> <li>Redirects to the \"Instances\" page.</li> <li>There should be a new entry in the table.</li> <li> <p>The instance will take some time to build and boot. When the     Status column entry says \"Active\" please move to the next steps.</p> <p></p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#associate-a-floating-ip","title":"Associate a Floating IP","text":"<p>If you are following along with the tutorial, you should already have a floating IP set up.</p> <ol> <li>In the \"Actions\" column entry, click the drop down triangle and select \"Associate Floating IP\".</li> <li>A dialog box will open.</li> <li>Select an IP address in the \"IP Address\" drop down box.</li> <li>Select a port in the \"Port to be associated\" drop down box.</li> <li> <p>Click \"Associate\" to return to the \"Instances\" page and associate the selected IP.</p> <p></p> </li> </ol>"},{"location":"uab_cloud/tutorial/instances/#instances-failing-to-start","title":"Instances Failing to Start","text":"<p>There are a number of reasons an instance might fail. We are able to provide direct support for instances which fail to start for reasons outside the instance itself. To help us correct the error, you'll need to have information from the instance page. Below is an example of a failed instance in the \"Instances\" table, helpfully named <code>failed_instance</code>. Note the \"Error\" label under the \"Status\" column.</p> <p></p> <p>In the \"Instances\" table, click the name of your failed instance. You should see a page like below, with some basic metadata about the instance as well as a \"Fault\" section.</p> <p></p> <p>We will need to \"ID\" and the reason for the fault. In this case, the instance failed because it could not allocate a GPU, as all GPUs were allocated at the time of its creation. It is not possible to diagnose the specifics without consulting us, so please feel free to contact Support.</p> <p>Instances can fail for other reasons as well, please contact Support with the \"ID\" and \"Fault\" information.</p> <p>For instances which fail due to internal reasons, i.e. while using SSH or an application, we are still able to provide support but it will have to be on a case-by-case basis. Be prepared to walk us through the steps you took to set up the instance and any software, as well as any data processing steps, leading up to the failure.</p>"},{"location":"uab_cloud/tutorial/instances/#ssh-into-the-instance","title":"SSH Into the Instance","text":"<p>If you are following the tutorial, then at this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You will need to Install an SSH Client Once your machine has an ssh client, use the following command. If your image uses an operating system other than Ubuntu, such as CentOS, replace the user <code>ubuntu</code> with <code>centos</code> or whatever is appropriate. The value <code>&lt;floating ip&gt;</code> should be whatever IP was assigned in Creating a Floating IP, and the value <code>&lt;private_key_file&gt;</code> should be whatever your key pair file was named from Creating a Key Pair.</p> <ol> <li>Install an SSH Client to use SSH from your local machine to your cloud instance.</li> <li>Manage Your Private Key<ul> <li>Start the SSH Agent to enable your system to remember your private key.</li> <li>Add a Private Key to the ssh agent to remember it for future use.</li> </ul> </li> <li> <p>Verify the SSH Client Works. Use the following command to connect</p> <pre><code>ssh ubuntu@&lt;floating ip&gt; -i ~/.ssh/&lt;private_key_file&gt;\n</code></pre> <ul> <li>If your image uses an operating system other than Ubuntu, such as CentOS, replace the user <code>ubuntu</code> with <code>centos</code>, or whatever may be appropriate.</li> <li>The value <code>&lt;floating ip&gt;</code> should be whatever IP was assigned in Creating a Floating IP.</li> <li>The value <code>&lt;private_key_file&gt;</code> should be whatever your key pair file was named from Creating a Key Pair.</li> </ul> <p></p> </li> <li> <p>(optional, but helpful) Set Up a Configuration File to simplify the command used to make a connection.</p> </li> </ol> <p>Note</p> <p>Reusing a floating IP for a new instance can result in a \"Remote Host Identification Has Changed\" error, preventing connection. Please see Remove an Invalid Host Fingerprint.</p>"},{"location":"uab_cloud/tutorial/instances/#streamlining-ssh","title":"Streamlining SSH","text":"<p>Refer to Setting up a Configuration File in Cloud Remote Access.</p>"},{"location":"uab_cloud/tutorial/instances/#next-steps","title":"Next Steps","text":"<p>Now you are ready to Install Software, set up Security Groups for Servers, and optionally Create a Persistent Volume.</p>"},{"location":"uab_cloud/tutorial/instances/#deleting-an-instance","title":"Deleting an Instance","text":"<p>Note</p> <p>Deleting Instances is not part of the tutorial, and is here as a reference.</p> <p>To delete an instance, return to the \"Instances\" page using the left-hand navigation pane. In the table, find the row with the instance you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Instance\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Instance\" again to delete the instance permanently.</p> <p></p> <p>Warning</p> <p>It is highly recommended to shut off an instance before deleting it.</p>"},{"location":"uab_cloud/tutorial/instances/#help-my-instance-is-stuck-or-not-working","title":"Help my Instance is Stuck or not Working","text":"<p>If your instance is stuck or otherwise not working as expected, first try deleting it and starting over. If you are unable to delete it or it gets stuck while deleting, please contact Support and copy the the instance ID as shown below.</p>"},{"location":"uab_cloud/tutorial/instances/#where-is-my-instance-id","title":"Where is my Instance ID?","text":"<p>To find your instance ID, navigate to the \"Instances\" table. Click the \"Instance Name\" for the instance you are interested in to load an information page for that instance.</p> <p></p> <p>In the instance information page, navigate to the \"Overview\" tab. Near the top is a field labeled \"ID\". The value to the right of \"ID\" is a Universally Unique ID (UUID) which uniquely names your instance. We need that ID to delete a stuck instance, so please provide it when requesting cloud.rc instance support.</p> <p></p>"},{"location":"uab_cloud/tutorial/instances/#continuing-the-tutorial","title":"Continuing the Tutorial","text":"<p>Now that you have set up a Network, Security Policies and an Instance, you are done with the tutorial, congratulations! There is one remaining optional step. If you need a persistent data volume to move between instances, please check our Volumes page.</p>"},{"location":"uab_cloud/tutorial/networks/","title":"Network Setup and Tutorial","text":"<p>Networking setup should be a one-time setup. While Floating IPs fall under the Networking fold-out, they should be allocated and released together with instances to maximize security.</p> <p>Important</p> <p>If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.</p> <p>Note</p> <p>Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.</p>"},{"location":"uab_cloud/tutorial/networks/#networks","title":"Networks","text":""},{"location":"uab_cloud/tutorial/networks/#creating-a-network","title":"Creating a Network","text":"<ol> <li> <p>Click \"Network\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Networks\" in the fold-out menu.</p> <ol> <li>The \"Networks\" page will open.</li> <li> <p>The \"uab_campus\" network entry should already be in the table.</p> <p></p> </li> </ol> </li> <li> <p>Click \"+ Create Network\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box. Only the \"Network\" tab is important, we     will create a subnet as a separate step.</p> <ol> <li> <p>Enter a \"Network Name\". See Naming Conventions.</p> </li> <li> <p>Leave \"Enable Admin State\" checked.</p> </li> <li> <p>Uncheck \"Create Subnet\". We will do this as a separate step. The other tabs should be removed.</p> </li> <li> <p>Leave the \"Availability Zone Hints\" box empty.</p> <p></p> </li> </ol> </li> <li> <p>Click \"Create\".</p> <ol> <li>Redirects to the \"Networks\" page.</li> <li> <p>There should be a new entry in the table with the name given in (4.a)</p> <p></p> </li> </ol> </li> </ol>"},{"location":"uab_cloud/tutorial/networks/#deleting-a-network","title":"Deleting a Network","text":"<p>Note</p> <p>Deleting Networks is not part of the tutorial, and is here as a reference.</p> <p>To delete a network, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the network you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Network\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Network\" again to delete the network permanently.</p> <p></p> <p>Important</p> <p>You will not be able to delete the network if it has a subnet with any connected routers or ports. They will need to be removed or deleted first.</p>"},{"location":"uab_cloud/tutorial/networks/#subnets","title":"Subnets","text":""},{"location":"uab_cloud/tutorial/networks/#creating-a-subnet","title":"Creating a Subnet","text":"<ol> <li> <p>Click \"Network\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Networks\" in the fold-out menu.</p> <ol> <li> <p>The \"Networks\" page will open.</p> </li> <li> <p>The \"uab_campus\" network should already be an entry in the table.</p> </li> <li> <p>At least one other entry must be in the table. See <code>Creating a Network</code>.</p> <p></p> </li> </ol> </li> <li> <p>Under the \"Actions\" column, select the drop-down triangle button in the row corresponding to the network you want to add a subnet to.</p> <p></p> </li> <li> <p>Click \"Create Subnet\" in the drop-down to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li> <p>The \"Subnet\" tab.</p> <ol> <li> <p>Enter a \"Subnet Name\". See Naming Conventions.</p> </li> <li> <p>Enter <code>192.168.0.0/24</code> as the \"Network Address\". The trailing <code>/24</code> allocates the entire range from <code>192.168.0.0</code> through <code>192.168.0.255</code> to the subnet.</p> </li> <li> <p>Ensure \"IPv4\" is selected in the \"IP Version\" drop-down box.</p> </li> <li> <p>Leave \"Gateway IP\" empty to use the default value of <code>192.168.0.0</code>.</p> </li> <li> <p>Leave \"Disable Gateway\" unchecked.</p> </li> <li> <p>Click the \"Next &gt;&gt;\" button to move to the \"Subnet Details\" tab.</p> <p></p> </li> </ol> <p>Note</p> <p>If you receive an error like</p> <pre><code>Failed to create subnet `192.168.0.0/24`...\nInvalid input for operation: Gateway is not valid on a subnet.\n</code></pre> <p>Try changing the gateway IP address to <code>192.168.0.1</code> and trying again.</p> </li> <li> <p>The \"Subnet Details\" tab.</p> <ol> <li>Leave \"Enable DHCP\" checked.</li> <li>Enter <code>192.168.0.20,192.168.0.100</code> in the \"Allocation Pools\" box. The IP addresses in that range will be assigned to instances on this subnet.</li> <li>Leave \"DNS Name Servers\" empty.</li> <li> <p>Leave \"Host Routes\" empty.</p> <p></p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Create\".</p> <ol> <li> <p>Redirects to the \"Overview\" page for the network the subnet was added to.</p> <p></p> </li> <li> <p>Click the \"Subnets\" tab next to \"Overview\" to verify the subnet was added to the table for this network.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"uab_cloud/tutorial/networks/#deleting-a-subnet","title":"Deleting a Subnet","text":"<p>Note</p> <p>Deleting Subnets is not part of the tutorial, and is here as a reference.</p> <p>To delete a subnet, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the associated subnet, and click the name of the network to go to that network's page.</p> <p></p> <p>Click on the \"Subnets\" tab to go to the subnets table. In the table, find the row with the subnet you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Subnet\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Subnet\" again to delete the subnet permanently.</p> <p></p> <p>Important</p> <p>You will not be able to delete the subnet if it is associated with any routers or ports. They will need to be removed or deleted first.</p>"},{"location":"uab_cloud/tutorial/networks/#routers","title":"Routers","text":""},{"location":"uab_cloud/tutorial/networks/#creating-a-router","title":"Creating a Router","text":"<p>To follow these directions for creating a router, a Network and Subnet must already exist.</p> <ol> <li> <p>Click \"Network\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Routers\" in the fold-out menu.</p> <p></p> </li> <li> <p>Click \"+ Create Router\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li>Enter a \"Router Name\". See Naming Conventions.</li> <li>Leave \"Enable Admin State\" checked.</li> <li>Select \"uab-campus\" in the \"External Network\" drop down box.</li> <li>Leave the \"Availability Zone Hints\" box empty.</li> </ol> <p></p> </li> <li> <p>Click \"Create Router\".</p> <ol> <li>Redirects to the \"Routers\" page.</li> <li>There should be a new entry in the table with the name given in (4.a)</li> </ol> <p></p> </li> <li> <p>Now we need to connect the router to our subnet. Click the name of the new entry under the \"Name\" column to open the router \"Overview\" page.</p> <p></p> </li> <li> <p>Click the \"Interfaces\" tab.</p> <p></p> </li> <li> <p>Click \"+ Add Interface\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li>Select an existing network-subnet pair in the \"Subnet\" drop down box.</li> <li>If this is your only router on the selected subnet, leave \"IP Address\" empty to use the subnet gateway.</li> </ol> <p></p> </li> <li> <p>Click \"Submit\"</p> <ol> <li>Redirects to the \"Interfaces\" page for the router.</li> <li>There should be a new entry in the table.</li> </ol> <p></p> </li> </ol>"},{"location":"uab_cloud/tutorial/networks/#deleting-a-router","title":"Deleting a Router","text":"<p>Note</p> <p>Deleting Routers is not part of the tutorial, and is here as a reference.</p> <p>To delete a router, return to the \"Routers\" page using the left-hand navigation pane. In the table, find the row with the router you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Router\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Router\" again to delete the router permanently.</p> <p></p>"},{"location":"uab_cloud/tutorial/networks/#floating-ips","title":"Floating IPs","text":""},{"location":"uab_cloud/tutorial/networks/#creating-a-floating-ip","title":"Creating a Floating IP","text":"<p>Floating IPs are required if you want an instance to talk to devices on the internet. These IPs are a shared resource, so they must be allocated when needed and released when no longer needed.</p> <ol> <li> <p>Click \"Network\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Floating IPs\".</p> <p></p> </li> <li> <p>Click \"Allocate IP to Project\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li>Select \"uab-campus\" in the \"Pool\" drop down box.</li> <li>Enter a \"Description\".</li> <li>Leave \"DNS Domain\" empty.</li> <li>Leave \"DNS Name\" empty.</li> </ol> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> <ol> <li>Redirects to the \"Floating IPs\" page.</li> <li>There should be a new entry in the table.</li> </ol> <p></p> </li> </ol>"},{"location":"uab_cloud/tutorial/networks/#releasing-a-floating-ip","title":"Releasing a Floating IP","text":"<p>Note</p> <p>Releasing Floating IPs is not part of the tutorial, and is here as a reference.</p> <p>To release a floating IP, return to the \"Floating IPs\" page using the left-hand navigation pane. In the table, find the row with the floating IP you wish to release, and click the drop-down arrow under \"Actions\" in that row. Then click \"Release Floating IP\" to open a confirmation dialog.</p> <p></p> <p>Click \"Release Floating IP\" again to release the floating IP.</p> <p></p>"},{"location":"uab_cloud/tutorial/networks/#continuing-the-tutorial","title":"Continuing the Tutorial","text":"<p>Now that you have set up a Network, the next step is to apply Security Policies to be able to communicate with it. To continue the tutorial, please visit Security Policies next.</p>"},{"location":"uab_cloud/tutorial/security/","title":"Security Policy Setup and Tutorial","text":"<p>These instructions show you how to prepare to use SSH with your instances. At a minimum, an SSH security group and at least one key pair must be created. Other security groups can and should be added as needed for additional services.</p> <p>Important</p> <p>If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.</p> <p>Note</p> <p>Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.</p>"},{"location":"uab_cloud/tutorial/security/#creating-a-security-group","title":"Creating a Security Group","text":"<p>Security Groups are used to set rules for how external devices can connect to your instances. Here we will create an SSH Security Group using a method that can be applied to other types of connections. Applications you develop may need other ports opened, so you may need to create additional security groups to handle those. Security groups may be reused across multiple instances.</p> <ol> <li> <p>Click \"Networks\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Security Groups\" in the fold out menu.</p> <p></p> </li> <li> <p>Click \"+ Create Security Group\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li>Under \"Name\" enter <code>ssh</code>.</li> <li>Leave \"Description\" empty.</li> </ol> <p></p> </li> <li> <p>Click \"Create Security Group\".</p> <ol> <li>Redirects to the \"Manage Security Group Rules: ssh\" page.</li> <li>There should be an entry for \"Egress IPv4\" and \"Egress IPv6\". Leave these alone.</li> </ol> <p></p> </li> <li> <p>Click \"+ Add Rule\" to open a dialog box.</p> <ol> <li>Select \"SSH\" in the \"Rule\" drop down box. This will change the remaining fields.</li> <li>Leave \"Description\" empty.</li> <li>Select \"CIDR\" in the \"Remote\" drop down box.</li> <li>Type <code>0.0.0.0/0</code> in the \"CIDR\" box. For the sake of this tutorial, this value is fine. For properly securing virtual machines, see the \"Warning\" below for more information on better practice.</li> </ol> <p></p> <p>Warning</p> <p>Using the value <code>0.0.0.0/0</code> for CIDR is short-hand for saying \"All possible IP addresses\". While cloud.rc is protected from external sources by the UAB firewall, using <code>0.0.0.0/0</code> does expose your virtual machine to all machines on the UAB internal network. Using the value <code>0.0.0.0/0</code> is the same as saying \"I trust the UAB firewall to protect my VM, and I trust UAB faculty, staff and students to not harm my VM\".</p> <p>Better practice is to limit the CIDR scope to only the IP address ranges that are relevant to your goals. As with all of cybersecurity, there is a security/convenience tradeoff to be made, and properly scoping CIDR will take more work than just using <code>0.0.0.0/0</code>. CIDR calculators are available on the internet to assist with calculation, just search for <code>CIDR Calculator</code>.</p> </li> <li> <p>Click \"Add\".</p> <ol> <li>Redirects to the \"Manage Security Group Rules: ssh\" page.</li> <li>There should be a new entry in the table.</li> </ol> <p></p> </li> </ol> <p>Important</p> <p>If you plan to Install Server Software, you will need to revisit this section to set up additional security groups for server ports.</p>"},{"location":"uab_cloud/tutorial/security/#deleting-a-security-group","title":"Deleting a Security Group","text":"<p>Note</p> <p>Deleting Security Groups is not part of the tutorial, and is here as a reference.</p> <p>To delete a security group, return to the \"Security Groups\" page using the left-hand navigation pane. In the table, find the row with the security group you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Security Group\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Security Group\" again to delete the security group permanently.</p> <p></p>"},{"location":"uab_cloud/tutorial/security/#creating-a-key-pair","title":"Creating a Key pair","text":"<p>A Key Pair is required for SSH access to cloud.rc instances for security reasons. To use a Key Pair and SSH, you will need to Install an SSH Client on your local machine.</p> <p>Key Pairs are security devices used to authenticate and connect to a remote machine, like Cheaha or cloud.rc instances, and use Public-key cryptography to encrypt the connection. As the name suggests, there are two parts: a public key which is placed on the remote machine, and a private key which is kept secret on your personal machine.</p> <p>While key pairs can be reused between instances, we highly recommend using a new key pair with each instance to minimize risk if a private key becomes compromised. See Good Practices for more information.</p> <p>There are a number of pitfalls and potential issues that can arise during this process. For information on these pitfalls and for more information on managing key pairs, see Managing Keys.</p> <p>There are two ways to create a key pair:</p> <ol> <li>Use the cloud.rc interface to generate a key pair remotely and download the private key file.</li> <li>Use your personal computer to generate a key pair locally and upload the public key file.</li> </ol>"},{"location":"uab_cloud/tutorial/security/#good-practices","title":"Good Practices","text":"<p>Good practice is to only use one key pair per person and per local machine. So if you have two computers, each will need its own key pair. Using the same key pair for multiple machines means that they all become compromised when that key is compromised. Using different key pairs for each means only one machine becomes compromised.</p> <p>If you have two people, each will need their own key pair. Private keys are secrets and should not be passed between people, because there is no way to control it once it has been shared with even one other person. Copying the key increases the risk of the system being compromised by an attacker. If the key has to be revoked, you revoke access for every user at once. If you must share access, create a key pair for each additional person to increase security and convenience.</p> <p>Using a password protected Key Pair is highly recommended for additional security, as it buys time to revoke a key pair if it is compromised by an attacker. Currently, this is only possible by uploading a custom public key generated on your local machine.</p>"},{"location":"uab_cloud/tutorial/security/#generating-a-key-pair-on-cloudrc","title":"Generating a Key Pair on cloud.rc","text":"<ol> <li> <p>Click \"Compute\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Key Pairs\".</p> <p></p> </li> <li> <p>Click \"+ Create Key Pair\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li> <p>Enter a \"Key Pair Name\". See Naming Conventions.</p> </li> <li> <p>Select \"SSH Key\" in the \"Key Type\" drop down box.</p> <p></p> </li> </ol> </li> <li> <p>Click \"+ Create Key Pair\"</p> <ol> <li>Opens a download file dialog box in your browser to download a file containing the secret private key. The file may have extension <code>.pem</code> or <code>.crt</code> depending on your operating system.</li> <li> <p>Download the private key file. For security reasons this will be your only chance to ever obtain the private key from cloud.rc. If you lose this file you will have to generate a new Key Pair.</p> <p></p> </li> <li> <p>Redirects to the \"Key Pairs\" page.</p> </li> <li> <p>There should be a new entry in the table.</p> <p></p> </li> </ol> </li> <li> <p>To add the private key on your local machine please see \"Add key\" under Add a Private Key.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/security/#generating-a-key-pair-locally","title":"Generating a Key Pair Locally","text":"<p>To generate a key pair, see instructions located at Generating Key Pairs.</p> <ol> <li> <p>Click \"Import Public Key\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li>Enter a \"Key Pair Name\". See Naming Conventions.</li> <li>Select \"SSH Key\" in the \"Key Type\" drop-down box.</li> <li>Click \"Browse...\" to upload a public key file from your custom key pair OR copy-paste the content of that key file into the \"Public Key\" box.</li> </ol> <p></p> </li> </ol> <p>Danger</p> <p>Do not upload your private key file! The private key file must stay secret to ensure proper security, and it should never leave the computer it was generated on.</p>"},{"location":"uab_cloud/tutorial/security/#using-the-key-pair","title":"Using the Key Pair","text":"<p>Please see SSH Into the Instance for more information on using the Key Pair.</p>"},{"location":"uab_cloud/tutorial/security/#revoking-a-key-pair","title":"Revoking a Key Pair","text":"<p>Note</p> <p>Revoking Key Pairs is not part of the tutorial, and is here as a reference.</p> <p>Revoking a key pair from cloud.rc is simple. First, log on to the interface.</p> <ol> <li> <p>Click \"Compute\" in the left-hand navigation pane to open the fold-out menu.</p> <p></p> </li> <li> <p>Click \"Key Pairs\".</p> <p></p> </li> <li> <p>Find the key pair you wish to revoke and click the \"Delete Key Pair\" button in that row.</p> </li> <li> <p>Optionally, Remove the Private Key from your local machine. This step is not necessary to ensure security, but can help maintain a clean environment.</p> </li> </ol>"},{"location":"uab_cloud/tutorial/security/#continuing-the-tutorial","title":"Continuing the Tutorial","text":"<p>Now that you've set up a Network and Security Policies, you're ready to create a virtual machine (VM) Instance to work with. To continue the tutorial, please visit Instances next.</p>"},{"location":"uab_cloud/tutorial/volumes/","title":"Volume Setup and Tutorial","text":"<p>These instructions are intended for researchers who want to setup a persistent volume for use across instances. To follow these instructions you'll need to have already setup an Instance.</p> <p>Important</p> <p>If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.</p> <p>Note</p> <p>Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.</p>"},{"location":"uab_cloud/tutorial/volumes/#creating-a-volume","title":"Creating a Volume","text":"<ol> <li> <p>Click the \"Volumes\" fold-out in the left-hand navigation pane - the fold-out should open.</p> <p></p> </li> <li> <p>Click \"Volumes\" within the fold-out to open the \"Volumes\" table page.</p> <p></p> </li> <li> <p>Click \"+ Create Volume\" to open a dialog box.</p> </li> <li> <p>Fill out the dialog box.</p> <ol> <li>Enter a \"Volume Name\". See Naming Conventions.</li> <li>Enter a \"Description\".</li> <li>Select \"No source, empty volume\" in the \"Volume Source\" drop-down box to create an empty volume.</li> <li>Select \"__DEFAULT__\" in the \"Type\" drop down box.</li> <li>Select a size in GB appropriate for your needs.</li> <li>Select \"nova\" in the \"Availability Zone\" drop down box.</li> <li>Select \"No group\" in the \"Group\" drop down box.</li> </ol> <p></p> </li> <li> <p>Click \"Create Volume\"</p> <ol> <li>Returns to the \"Volumes\" table page.</li> <li> <p>There will be a new entry in the \"Volumes\" table.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"uab_cloud/tutorial/volumes/#attaching-a-volume-to-a-running-instance","title":"Attaching a Volume to a Running Instance","text":"<p>To attach a volume you must have already created at least one using the cloud.rc interface. More information can be found in [link]</p> <ol> <li> <p>Open the instances table by clicking \"Compute\" in the left-hand navigation pane and clicking \"Instances\".</p> </li> <li> <p>In the \"Actions\" column entry, click the drop down triangle button and select \"Attach Volume\".</p> <p></p> </li> <li> <p>A dialog box will open.</p> </li> <li> <p>Select a volume in the \"Volume ID\" drop down box.</p> <p></p> </li> <li> <p>Click \"Attach Volume\".</p> </li> </ol> <p>Now the volume should be attached to the instance. From here you may format the volume and mount it.</p>"},{"location":"uab_cloud/tutorial/volumes/#formatting-a-volume","title":"Formatting a Volume","text":"<p>To format a volume, you must have created a volume and attached it to an instance capable of formatting it correctly. These instructions assume a Linux operating system.</p> <ol> <li> <p>Click \"Compute\" in the left-hand navigation pane, then open the \"Instances\" menu. Click the name of any instance you wish to use to format the volume. Then click \"Overview\".</p> </li> <li> <p>Scroll down to \"Volumes Attached\" and make note of the <code>&lt;mount&gt;</code> part of <code>&lt;volume-name&gt; on &lt;mount&gt;</code> for your attached volume as it will be used in later steps.</p> <p></p> </li> <li> <p>SSH into the instance from your local machine or from Cheaha.</p> </li> <li> <p>Verify the volume is attached by using <code>sudo fdisk -l | egrep \"&lt;mount&gt;\"\"</code></p> <p></p> </li> <li> <p>Format the volume using <code>sudo fdisk \"&lt;mount&gt;\"</code></p> <ol> <li>You will be in the <code>fdisk</code> utility.</li> <li>Enter <code>n</code> to create a new partition.</li> <li>Enter <code>p</code> to make it the primary partition.</li> <li>Enter numeral <code>1</code> to make it the first partition.</li> <li>Press enter to accept the default first sector.</li> <li>Press enter to accept the default last sector.</li> <li>Enter <code>t</code> to change partition type.</li> <li>Enter numerals <code>83</code> to change to Linux partition type.</li> <li>Enter <code>p</code> to display the partition setup. Note that the partition will be labeled <code>&lt;mount&gt;1</code>. This literally whatever <code>&lt;mount&gt;</code> was from earlier followed by the numeral <code>1</code>. Further steps will refer to this as <code>&lt;pmount&gt;</code></li> <li>Enter <code>w</code> to execute the setup prepared in the previous substeps.</li> </ol> <p></p> </li> <li> <p>Verify the volume is not mounted using <code>sudo mount | egrep \"&lt;mount&gt;\"</code>. If there is no output, then move to the next step. If there is some output then use <code>sudo umount -l \"&lt;mount&gt;\"</code> to unmount the volume and verify again.</p> <p></p> </li> <li> <p>Create the filesystem using <code>sudo mkfs.ext4 \"&lt;pmount&gt;\"</code>. Ensure that the output looks like the following:</p> <pre><code>ubuntu@my-instance:~$ sudo mkfs.ext4 /dev/vdb1\nmke2fs 1.45.5 (07-Jan-2020)\nDiscarding device blocks: done\nCreating filesystem with 26214144 4k blocks and 6553600 inodes\nFilesystem UUID: 335704a9-2435-440a-aeea-8ae29438ac64\nSuperblock backups stored on blocks:\n      32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 654208,\n      4096000, 7962624, 11239424, 20480000, 23887872\n\nAllocating group tables: done\nWriting inode tables: done\nCreating journal (131072 blocks): done\nWriting superblocks and filesystem accounting information: done\n</code></pre> <p></p> </li> </ol> <p>The volume is now formatted and ready for mounting within an attached instance OS. You will need to make note of <code>&lt;pmount&gt;</code> for when you are ready to mount the volume to an instance.</p>"},{"location":"uab_cloud/tutorial/volumes/#mounting-a-volume-in-an-instance","title":"Mounting a Volume in an Instance","text":"<p>Mounting a volume needs to be done once per instance it will be attached to. It is assumed you've already created and formatted a volume and attached it to some instance. You'll need the <code>&lt;pmount&gt;</code> label from when you formatted the volume.</p> <ol> <li> <p>SSH into the instance from your local machine or from Cheaha.</p> </li> <li> <p>Obtain the uuid of the volume using <code>sudo blkid | egrep \"&lt;pmount&gt;\"</code>. This will be referred to as <code>&lt;uuid&gt;</code> in future steps.</p> <p></p> </li> <li> <p>Create a directory to mount the volume as. A good choice is <code>sudo mkdir /mnt/&lt;volume-name&gt;</code> where <code>&lt;volume-name&gt;</code> is something meaningful for you or your project. This directory will be referred to as <code>&lt;directory&gt;</code> in future steps.</p> </li> <li> <p>Mount the volume to the directory using <code>sudo mount -U &lt;uuid&gt; &lt;directory&gt;</code>.</p> </li> <li> <p>Verify the volume is mounted using <code>df -h | egrep &lt;pmount&gt;</code></p> <p> </p> </li> <li> <p>Edit the <code>fstab</code> file to make mounting persistent across instance reboots.</p> <ol> <li>Edit the file using <code>sudo nano /etc/fstab</code>.</li> <li>Add the following line to the file:</li> </ol> <pre><code>/dev/disk/by-uuid/&lt;uuid&gt; &lt;directory&gt; auto defaults,nofail 0 3\n</code></pre> <p></p> </li> <li> <p>Verify fstab was modified correctly by soft rebooting the instance and verifying the mount again using <code>df -h | egrep \"&lt;pmount&gt;\"</code>.</p> <p></p> </li> <li> <p>Set access control using the following commands:</p> <pre><code>sudo apt install acl # or yum install, etc., if not already installed\nsudo setfacl -R -m u:&lt;username&gt;:rwx &lt;directory&gt;\n</code></pre> <p></p> </li> <li> <p>Verify the access controls were modified correctly by creating a test file and then listing files in <code>&lt;directory&gt;</code> to ensure the file was created. The following commands will achieve this:</p> <pre><code>cd &lt;directory&gt;\ntouch testfile\nls\n</code></pre> <p></p> </li> </ol> <p>The volume is now mounted to your instance and ready for use and re-use across sessions and reboots.</p>"},{"location":"uab_cloud/tutorial/volumes/#deleting-a-volume","title":"Deleting a Volume","text":"<p>Note</p> <p>Deleting a Volume is not part of the tutorial, and is here as a reference.</p> <p>To delete a volume, return to the \"Volumes\" page using the left-hand navigation pane. In the table, find the row with the volume you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume\" to open a confirmation dialog.</p> <p></p> <p>Click \"Delete Volume\" again to delete the volume permanently.</p> <p></p> <p>Important</p> <p>It will not be possible to delete a volume if it has an associated volume snapshot. The snapshot will need to be deleted first.</p>"},{"location":"workflow_solutions/getting_containers/","title":"Software Containers","text":"<p>Containers help to manage software installations and all their dependencies in a single large image. These containers are a self-contained operating system along with any software the container creator added. Containers avoid software conflicts due to versioning as well as OS incompatibility and can be run on most, if not all, operating systems.</p> <p>The most common container engine is called Docker. Docker is an open-source platform for building, deploying, running, updating, and managing containers and has distributions for Linux, Windows, and Mac. Singularity is another common container engine specialized for use on HPC systems such as Cheaha where Docker cannot be used.</p>"},{"location":"workflow_solutions/getting_containers/#fantastic-containers-and-where-to-find-them","title":"Fantastic Containers and Where to Find Them","text":"<p>Docker containers are available in https://hub.docker.com/. This docker hub repository allows to share containers and use pre-existing docker images.</p> <p></p> <p>It is often a good idea to search the Github repo for an application or pipeline to see if a container has already been provided by the authors.</p>"},{"location":"workflow_solutions/getting_containers/#containers-on-cheaha","title":"Containers on Cheaha","text":"<p>Using containers on Cheaha bypasses the need to message support to install necessary software. Containers can be downloaded by any user into their personal space and used immediately without admin permission. as mentioned above, you will need to use Singularity containers on Cheaha. You can find all of the Singularity modules using the following command:</p> <pre><code>module spider Singularity\n</code></pre> <p>It's highly recommended to only use Singularity versions 3+.</p>"},{"location":"workflow_solutions/getting_containers/#pull-singularity-images","title":"Pull Singularity Images","text":"<p>Singularity can pull images from a variety of sources, including Dockerhub, and convert them to the proper format automatically. In order to download an image, use the <code>pull</code> subcommand followed by the output image name and the URI of the image. The general form of the command for pulling from Dockerhub is as follows:</p> <pre><code>singularity pull &lt;output.sif&gt; docker://&lt;account&gt;/&lt;image&gt;[:&lt;version_tag&gt;]\n</code></pre> <p>For example, if we wanted to pull the lolcow container:</p> <pre><code>singularity pull lolcow.sif docker://godlovedc/lolcow\n</code></pre> <p>We now have the <code>lolcow.sif</code> image we can run or share with other researchers. It's important to remember that containers are just independent files that can be moved, copied, or deleted the same as any other file.</p>"},{"location":"workflow_solutions/getting_containers/#running-singularity-images","title":"Running Singularity Images","text":"<p>There are 3 ways to run Singularity images, all with their unique purposes and are as follows:</p> <ol> <li><code>singularity run</code>: run a container using a default command set by the author. Generally, this will be used when a container encompasses a full pipeline controlled by a single command. The general form for this command is <code>singularity run &lt;image.sif&gt; [options]</code> where <code>[options]</code> are defined by the default command. You can use <code>singularity run &lt;image.sif&gt; --help</code> to see what those options are.</li> <li><code>singularity exec</code>: run any command available in the container. This provides more flexibility than <code>run</code> and would be useful in the cases where a container has more modular components as opposed to a single control script. The general form for this would be <code>singularity exec &lt;image.sif&gt; &lt;command&gt; [options]</code>. You can add the <code>--help</code> option to see what a given command does and its inputs.</li> <li><code>singularity shell</code>: allow interactive use of the container through the terminal. This changes your active environment to that in the container. You can traverse the container's directory tree and search for various files and commands as if it was a virtual machine. This is very useful for interactive development as well as investigation of a container's contents. The general form of the command is <code>singularity shell &lt;image.sif&gt;</code>.</li> </ol> <p>It's important to note that both <code>run</code> and <code>exec</code> enter the container as part of their execution and then exit back to the original shell environment afterwards whereas <code>shell</code> keeps you in the container until you either close the terminal or use the <code>exit</code> command.</p> <p>Important</p> <p><code>singularity shell</code> is not executable via shell scripts. Any singularity commands in a batch script should be <code>run</code> or <code>exec</code> instead.</p>"},{"location":"workflow_solutions/getting_containers/#singularity-paths","title":"Singularity Paths","text":"<p>By default, Singularity containers have limited access to the general filesystem. Containers get default access to the <code>/home</code> directory as well as the directory the container was run from. If you run the container from <code>$HOME</code> but try to access files in <code>$USER_DATA</code>, you will see an error. In order to give a container access to other directories, use the <code>-B</code> or <code>--bind</code> option when invoking the container. For instance, if I wanted to use <code>run</code> on a container that had an input option called <code>-i</code> and give the container access to a subfolder called <code>my_data</code> in a project space called <code>UABRC</code>, the singularity command would look like:</p> <pre><code>singularity run --bind /data/project/UABRC/my_data image.sif -i /data/project/UABRC/my_data\n</code></pre> <p>You can also alias the bind path to a shorter name and use it in the command. In that case, the bind option would look like <code>--bind &lt;/directory_path&gt;:&lt;/alias&gt;</code>. For example, if I was running a container and was giving the <code>my_data</code> directory as an input, I could alias it to <code>/input_data</code> and use it in the command like so:</p> <pre><code>singularity run --bind /data/project/UABRC/my_data:/input_data image.sif -i /input_data\n</code></pre> <p>These bind paths can be used in both <code>exec</code> and <code>shell</code> subcommands as well.</p> <p>Note</p> <p>Bind paths cannot grant access to folders and files your account does not have access to. For instance, you cannot use a container to access data in another user's account unless that user has explicitly given you the correct permissions via <code>chmod</code> or ACLs.</p>"},{"location":"workflow_solutions/getting_containers/#using-containers-on-uab-rc-cloud-cloudrcuabedu","title":"Using Containers on UAB RC Cloud (cloud.rc.uab.edu)","text":"<p>To access docker containers, install <code>Docker</code> in your system. To install docker desktop on your computer, follow this link: Docker Desktop Page.</p>"},{"location":"workflow_solutions/getting_containers/#docker-installation-on-uab-rc-cloud","title":"Docker Installation on UAB RC Cloud","text":"<p>Following are the installation instructions to install <code>Docker</code> on UAB RC Cloud with Ubuntu operating system. Tested the installation on Ubuntu 20.04. Setting up UAB RC Cloud account can be found in UAB RC Cloud.</p> <pre><code>sudo apt-get update\nsudo apt install docker.io\n</code></pre>"},{"location":"workflow_solutions/getting_containers/#using-a-docker-container-from-dockerhub","title":"Using a Docker Container from DockerHub","text":"<p>We can start pulling a container named <code>alpine</code> from the Docker hub. <code>alpine</code> is a general-purpose Linux distribution. Look for the container <code>alpine</code> in the docker hub, copy the pull command, and paste it into your terminal.</p> <p></p> <pre><code>sudo docker pull alpine\n</code></pre> <p></p> <p>Once the image is pulled, you can verify if the image exists using the below command. Note that if you do not specify the tag/version of the container, the recent version is built, and the tag is listed as <code>latest</code>.</p> <pre><code>sudo docker images\n</code></pre> <p></p> <p>If you prefer to pull a particular version of the <code>alpine</code> container, you need to mention the tag details in your pull command. You can see the available tags/versions of <code>alpine</code> from the Docker hub.</p> <p></p> <p>To pull particular version of <code>alpine</code> container, use the below syntax.</p> <pre><code>sudo docker pull container_name:tag\n</code></pre> <p>Here the <code>container_name</code> is <code>alpine</code>, and the tag is <code>3.14</code>.</p> <pre><code>sudo docker pull alpine:3.14\n</code></pre> <p>The existing image looks like,</p> <p></p>"},{"location":"workflow_solutions/getting_containers/#create-your-own-docker-container","title":"Create Your Own Docker Container","text":"<p>You can create your own Docker container, build it, and upload/share them in the Docker hub or UAB GitLab container registry.</p> <p>Let us take a synthetic python code and formulate the packages/dependencies required to build your software container. Below is a python script that requires packages, namely, numpy, scipy, and matplotlib. Next, the steps to create a <code>Dockerfile</code> is illustrated. Let us name this script <code>python_test.py</code>.</p> <pre><code>import numpy as np\nimport matplotlib\nimport pylab\nimport matplotlib.pylab as plt\nimport scipy.integrate as integrate\n\na = np.array([0, 10, 20, 30, 40])\nprint(a)\n\nb = np.arange(-5, 5, 0.5)\nprint(b)\n\nt = np.arange(0,20.5,0.5)\nprint(t)\n\nresult = integrate.quad(np.sin, 0, np.pi)\nprint(result)\n\nplt.plot([1, 2, 3, 4], [1, 4, 9, 16])\nplt.show()\nplt.savefig('testing.png')\n</code></pre>"},{"location":"workflow_solutions/getting_containers/#create-a-dockerfile-that-has-miniconda-installed","title":"Create a Dockerfile that has Miniconda Installed","text":"<p>We require numpy, scipy, and matplotlib libraries to execute the above Python script. Following are the steps to create a specification file and build a container image.</p> <ol> <li> <p>Create an empty directory <code>miniconda</code>.</p> <pre><code>mkdir miniconda\n</code></pre> </li> <li> <p>Create a <code>Dockerfile</code> within the <code>miniconda</code> directory with the following contents. The file name <code>Dockerfile</code> is case-sensitive.</p> <p></p> <pre><code># You may start with a base image\n# Always use a specific tag like \"4.10.3\", never \"latest\"!\n# The version referenced by \"latest\" can change, so the build will be\n# more stable when building from a specific version tag.\nFROM continuumio/miniconda3:4.12.0\n\n# Use RUN to execute commands inside the miniconda image\nRUN conda install -y numpy\"&gt;=1.16.5, &lt;1.23.0\"\n\n# RUN multiple commands together\n# Last two lines are cleaning out the local repository and removing the state\n# information for installed package\nRUN apt-get update \\\n&amp;&amp; conda install -y scipy=1.7.3 \\\n&amp;&amp; conda install -y matplotlib=3.5.1 \\\n&amp;&amp; apt-get --yes clean \\\n&amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>This is the specification file. It provides Docker with the software information, and versions, it needs to build our new container. See the Docker Container documentation for more information https://docs.docker.com/reference/dockerfile/.</p> <p>In the Dockerfile, we start with an existing container <code>continuumio/miniconda3:4.12.0</code>. This container is obtained from Dockerhub; here, <code>continuumio</code> is the producer, and the repo name is <code>continuumio/miniconda3</code>.</p> <p></p> <p>You may specify the required version from the <code>Tag</code> list. Here the tag/version is <code>4.12.0</code>. Also its a very good practice to specify the version of packages for numpy, scipy, and matplotlib for better reproducibility.</p> <p>Containers and Reproducibiliy</p> <p>Always include version numbers for Anaconda, package managers, software you are installing, and the dependencies for those software. Containers are not by nature scientifically reproducible, but if you include versions for as much software in the container as possible, they can be reproducible years later.</p> </li> <li> <p>To build your container, change the directory to <code>miniconda</code> and use the below syntax to build the <code>Dockerfile</code>. Here we use <code>.</code> to say \"current directory.\" This will only work if you are in the directory with the <code>Dockerfile</code>.</p> <pre><code>sudo docker build -t repository_name:tag .\n</code></pre> <p>Here the repository_name is <code>py3-miniconda</code> and the tag is <code>2022-08</code>.</p> <pre><code>cd miniconda\nsudo docker build -t py3-miniconda:2022-08 .\n</code></pre> <p></p> </li> </ol> <p>Note</p> <p>The <code>.</code> at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside. Also, you may rename the <code>repository_name</code> and <code>tag</code> as you prefer.</p> <pre><code>sudo docker images\n</code></pre> <p></p>"},{"location":"workflow_solutions/getting_containers/#running-the-built-miniconda-docker-container-interactively","title":"Running the Built Miniconda Docker Container Interactively","text":"<p>To run docker interactively and execute commands inside the container, use the below syntax. Here <code>run</code> executes the command in a new container, and <code>-it</code> starts an interactive shell inside the container. After executing this command, the command prompt will change and move into the bash shell.</p> <pre><code>sudo docker run -it repository_name:tag /bin/bash\n</code></pre> <p>To execute your container <code>py3-miniconda</code> interactively, run this command with the tag `2022-08'.</p> <pre><code>sudo docker run -it py3-miniconda:2022-08 /bin/bash\ncd /opt/conda/bin/\n</code></pre> <p>The <code>python</code> executables to execute our synthetic python script are within the directory structure <code>/opt/conda/bin</code>.</p> <p></p> <p></p>"},{"location":"workflow_solutions/getting_containers/#mounting-data-onto-a-container","title":"Mounting Data Onto a Container","text":"<p>Before we mount data onto a container, remember you initially created the python script <code>python_test.py</code> when creating your own container. Move <code>python_test.py</code> within <code>miniconda</code> directory. Now you have your <code>miniconda/python_test.py</code> outside the container. To access the files outside the container you should mount the file path along with the <code>docker run</code> command.</p> <p></p> <p>To mount a host directory into your docker container, use the <code>-v</code> flag.</p> <pre><code>sudo docker run -v /host/directory/:/container/directory  -other-options\n</code></pre> <p>So the command for our example will be,</p> <pre><code>sudo docker run -v /home/ubuntu/:/home  -it py3-miniconda:2022-08 /bin/sh\n</code></pre> <p>Here we are mounting the $HOME directory <code>/home/ubuntu</code> from a host into containers' $HOME directory. Note that you may mount a particular directory according to your preference. The following shows the list of files in containers' $HOME directory with and without mounting.</p> <p>Before mounting, there are no files found within the $HOME directory.</p> <p></p> <p>After mounting using <code>-v</code> flag, files show up within the $HOME directory. The highlighted <code>miniconda</code> is our working directory with python script.</p> <p></p> <p>We can now execute the script, python_test.py using this command.</p> <pre><code>python python_test.py\n</code></pre> <p></p> <p>More lessons on Docker can be found in this link: Introduction to Docker and Docker Documentation.</p>"},{"location":"workflow_solutions/getting_containers/#sharing-containers-using-uab-gitlab-container-registry","title":"Sharing Containers Using UAB GitLab Container Registry","text":"<p>If you prefer to share your container with a particular team/group, then the UAB GitLab container registry is the best and most secure option.</p> <p>The following steps help you to create a container registry in UAB GitLab:</p> <ol> <li>Create a UAB Gitlab account following the guidelines from the UAB GitLab page.</li> <li> <p>Create a <code>new_project</code> on UAB GitLab and click <code>Package and Registries</code>, and then go to <code>Container Registry</code>. Initially, the container registry looks empty because there are no container images in the registry.</p> <p></p> <p>Note</p> <p>Copy these CLI commands for future reference. It contains commands (1) to login to your project UAB GitLab container registry (2) Add an image to the registry using the push/build command. We will use the <code>push</code> command as we already have the existing container in our system.</p> </li> <li> <p>Login to UAB GitLab Registry using your <code>registry_name:ID</code>.</p> <pre><code>sudo docker login gitlab.rc.uab.edu:4567\n</code></pre> <p>Note</p> <p>The registry_name and ID shown in this examples are for understanding and not meant for testing. Please use your GitLab registry name and ID for testing.</p> <p>Note: For securing concerns, use an access token to log in. Create an access token in UAB GitLab to push/pull the docker container in the container registry (Secure token and guidelines to follow are shown next).</p> <pre><code>sudo docker login gitlab.rc.uab.edu:4567 -u username \u2013p access_token\n</code></pre> </li> <li> <p>Creating an Access Token: From the UAB GitLab page, you can create an access token instead of using a password to log in to the UAB GitLab registry. Goto Edit profile -&gt; Click <code>Access Tokens</code>. Then enter:</p> <ul> <li> <p>Token name.:Suggestion: \"container\"_\"repository-name\"</p> </li> <li> <p>Expiry date. Suggestion: Default is 30 days. You can set your expiry date 3 months from the date you are making it.</p> </li> <li> <p>Under select scopes, check read and write registry  (to push images to the registry) -&gt; Then click <code>create personal access token</code>.</p> </li> </ul> </li> </ol> <p>Once you create the token, copy the new personal access token since it\u2019s a one-time step and hard to retrieve after a refresh. Use the personal access token for login.</p> <p></p> <p></p> <p></p> <p>Warning</p> <p>Running <code>docker login</code> leads to a warning message that your password is stored unencrypted in <code>/root/.docker/config.json</code> (or) <code>$HOME/.docker/config.json</code>. To ignore this warning, follow the instructions in this Github page or the Docker credentials store page.</p>"},{"location":"workflow_solutions/getting_containers/#push-alpine-container-from-your-system-to-uab-gitlab-container-registry","title":"Push Alpine Container from your System to UAB GitLab Container Registry","text":"<ul> <li>List the docker images on your local computer using the <code>docker images</code> command. An <code>alpine</code> image exists already on this computer. Your container will likely have a different name.</li> </ul> <pre><code>sudo docker images\n</code></pre> <ul> <li>Tag <code>alpine</code> to push into UAB GitLab registry. We need to have the UAB GitLab registry name to push. It will show the default command on the container registry page. Copy these commands for future reference. The tag is <code>test</code> here.</li> </ul> <pre><code>sudo docker tag alpine:latest gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test\n</code></pre> <p>You can see the tag <code>test</code> associated with the <code>alpine</code> image.</p> <pre><code>sudo docker images\n</code></pre> <p></p> <p>The below first command is the syntax to push the Docker image to the UAB GitLab container registry from your computer. The second command is an example of pushing a Docker image to the UAB GitLab container registry from your computer.</p> <p>Note</p> <p>The registry_name,ID, and gitlab_group_name shown in this examples are for understanding and not meant for testing. Please use your GitLab registry name and ID for testing.</p> <pre><code>sudo docker push gitlab_registry_name:ID/gitlab_group_name/project_name:tag\n\nsudo docker push gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test\n</code></pre> <p></p> <p>In your GitLab's page container registry, refresh to view the <code>alpine</code> container is pushed to the registry.</p> <p></p> <p>Now lets pull the <code>alpine</code> container from GitLab's page container registry to your system. Before that, remove the previous image from the system, which already has a <code>test</code> tag, to avoid discrepancies.</p> <pre><code>sudo docker rmi -f image_id\n</code></pre> <p></p> <p></p> <p>In your GitLab's page container registry, copy the pull command from the <code>test</code> container registry, and use it to pull the docker container to your system. You can see the image is reflected in the image list.</p> <pre><code>sudo docker pull gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test\n</code></pre> <p></p> <p></p>"},{"location":"workflow_solutions/git/","title":"Git","text":""},{"location":"workflow_solutions/git/#introductory-guides","title":"Introductory Guides","text":"<p>Above all else, git is a software and document collaboration tool.</p> <p>It may be used to record and manage changes to projects. Git allows users to commit changes to branches in a repository. Those changes and branches are recorded and may be shared with other users who have clones of the project.</p> <p>Git can be thought of as a combination of a timeline and a tree. Git is a timeline because it records change history on a timeline. It is a tree because changes can be meaningfully grouped into branches, each of which is an independent set of related commits. The branches can fork from or merge into each other as changes are recorded along the timeline. An example of part of the state of the repository for this documentation can be seen below (visualized using VSCode and https://github.com/mhutchie/vscode-git-graph).</p> <p></p> <p>Each row is a single commit, or set of changes. The vertical threads in the \"Graph\" column are branches which can be seen to fork and merge with one another. Each branch tip is labeled with the branch name under the \"Description\" column, along with a message describing the commit. The \"Date\" column shows when a commit was made, \"Author\" by whom, and \"Commit\" a partial hash that uniquely identifies the commit.</p>"},{"location":"workflow_solutions/git/#educational-resources","title":"Educational Resources","text":"<p>The internet has many guides for using git. Rather than devise our own complete lesson plan for using git, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the git lesson, see https://swcarpentry.github.io/git-novice/.</p> <p>Denser and more complete documentation is available at https://git-scm.com/doc.</p>"},{"location":"workflow_solutions/git/#reference","title":"Reference","text":""},{"location":"workflow_solutions/git/#glossary","title":"Glossary","text":"<ul> <li>A repository is the largest unit of information that git keeps track of.<ul> <li>A useful model for a repository is a single coding or documentation project.</li> <li>A repository is composed of a collection of files, a working tree and index, and all of the change history associated with those things. Change history includes branches and commits.</li> <li>Repositories are decentralized, so that two people can work independently on parts of the same project and then merge their changes later.</li> <li>A local repository is a repository that is housed on the same machine you are working on.</li> <li>A remote repository or remote is a repository that is housed on a machine other than the one you are working on.</li> <li>Remotes are often housed on internet repository services like https://github.com and https://about.gitlab.com. UAB also maintains a private Gitlab instance at https://gitlab.rc.uab.edu.</li> </ul> </li> <li>The working tree is the structure used to model repository contents and history.</li> <li>The index contains changes since the most recent commit.</li> <li>The staging area, a subset of the index, contains changes ready to be committed.</li> <li>A branch is a single thread of commits made to the working tree. Branches are independent of each other until they are merged. A repository can have one or many branches. Branches can be set up to track remote branches, facilitating pushing and pulling.</li> <li>A commit is a collection of changes that have been made to the working tree. Commits have associated messages. Repositories and branches are structured collections of commits. A repository may have many commits, distributed among branches.</li> <li>A branch tip is the most recent commit on a branch.</li> <li>The HEAD is the most recent commit on the currently selected branch.</li> <li>Cloning means creating a complete, but independent, copy of a repository and its history.</li> <li>Fetching means retrieving changes in from a remote repository.</li> <li>Merging is the process of incorporating changes from a source branch into a target branch.</li> <li>Pulling is fetching followed by merging.</li> <li>Pushing means communicating changes out to a remote repository.</li> </ul>"},{"location":"workflow_solutions/git/#how-should-i-configure-git","title":"How should I configure git?","text":"<p>Good practice for configuring git includes adding your name and email globally on your local machine so that you received proper attribution when making changes to repositories.</p> <p>To add your name and email enter the following two lines, replacing <code>&lt;name&gt;</code> and <code>&lt;email&gt;</code> as appropriate.</p> <pre><code>git config --global user.name &lt;name&gt;\ngit config --global user.email &lt;email&gt;\n</code></pre> <p>More information is available at the official git webbook.</p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#how-do-i-obtain-git-repositories","title":"How do I obtain git repositories?","text":"<p>Obtaining repositories is how projects start, or how you might start working on someone else's repository. Substantially more information is available from the official git webbook</p>"},{"location":"workflow_solutions/git/#start-a-local-repository-init","title":"Start a Local Repository (Init)","text":"<p>Starting a local repository is also known as \"initializing\" a repository. It can be done with an empty folder, or a folder with existing code. Navigate to the folder and use the following command to initialize a repository.</p> <pre><code>git init\n</code></pre>"},{"location":"workflow_solutions/git/#forking","title":"Forking","text":"<p>Forking a repository is not a git concept, but a concept of remote repository hosting services like GitHub and GitLab. Forking is cloning a repository from another users account to your own account on GitHub or GitLab.</p> <p>GitHub fork documentation.</p> <p>GitLab fork documentation.</p>"},{"location":"workflow_solutions/git/#cloning","title":"Cloning","text":"<p>Cloning a repository means making an independent copy of a remote repository on your local machine. To clone a repository you will need the remote URL where the repository is available. Navigate to the directory where you would like the repository located. Use the following to create a new subdirectory containing the clone.</p> <pre><code>git clone &lt;remote-url&gt;\n</code></pre> <p>Note</p> <p>Clones may be made from GitHub and GitLab using either <code>https</code> or <code>ssh</code>. To use <code>ssh</code> you will need to set up SSH Keys.</p> <p>Note</p> <p>Best practice for the remote URL is to ensure it ends with <code>.git</code>. While the URL for this documentation's repository is https://github.com/uabrc/uabrc.github.io you should instead use <code>https://github.com/uabrc/uabrc.github.io.git</code> when cloning.</p> <p>Using the <code>.git</code> suffix is optional for GitHub, but required for GitLab.</p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#how-do-i-interact-with-remotes","title":"How do I interact with remotes?","text":"<p>A key part of git's usefulness is facilitating collaboration on code and documentation projects. Interacting with independent, remote copies of a repository is central to the purpose of git. Substantially more information is available at the official git webbook.</p>"},{"location":"workflow_solutions/git/#managing-remotes","title":"Managing remotes","text":"<ul> <li> <p>Check what remotes are available using <code>git remote -v</code></p> <p></p> </li> <li> <p>Add a new remote with <code>git remote add &lt;remote-name&gt; &lt;remote-url&gt;</code></p> <p></p> </li> <li> <p>Change the url of an existing remote with <code>git remote set-url &lt;remote-name&gt; &lt;new-remote-url&gt;</code></p> <p></p> </li> <li> <p>Remove a remote with <code>git remote remove &lt;remote-name&gt;</code></p> <p></p> </li> </ul> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#fetching-and-pulling","title":"Fetching and pulling","text":"<p>Fetching changes from remote repositories without incorporating them:</p> <ul> <li> <p>Check and download changes for all remotes using <code>git fetch --all</code>.</p> <p></p> </li> <li> <p>Check and download changes for a remote using <code>git fetch &lt;remote-name&gt;</code>.</p> <p></p> </li> </ul> <p>Pull (fetch and merge) changes from a branch using <code>git pull &lt;remote-name&gt; &lt;remote-branch-name&gt;.</code> These changes are merged to the current HEAD, so be sure to checkout the correct branch first.</p> <p></p> <p>Full documentation of <code>git fetch</code>.</p> <p>Full documentation of <code>git pull</code>.</p>"},{"location":"workflow_solutions/git/#pushing","title":"Pushing","text":"<p>Push changes to a remote branch on a remote repository using <code>git push &lt;remote-name&gt; &lt;remote-branch-name&gt;</code>.</p> <p></p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#how-do-i-record-changes","title":"How do I record changes?","text":"<p>The key part of git's function is recording changes. Substantially more information is available at the official git webbook.</p>"},{"location":"workflow_solutions/git/#checking-status-of-changes","title":"Checking status of changes","text":"<p>Use <code>git status</code>. It will show your current branch, staged files and unstaged files.</p> <p></p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#staging-and-committing-changes","title":"Staging and committing changes","text":"<p>Staging files is done with the <code>git add</code> command, committing files with <code>git commit</code>.</p> <pre><code>git add new_file.txt\ngit commit -m \"added new text file\"\n</code></pre> <p></p> <p>Full documentation of <code>git add</code>.</p> <p>Full documentation of <code>git commit</code>.</p>"},{"location":"workflow_solutions/git/#stop-tracking-files","title":"Stop tracking files","text":"<p>Use <code>git rm &lt;file&gt;</code> if the file has no changes. Despite the name this will not delete the file, only stop git from tracking any future changes. Specifically, it will stage a removal of the file from git tracking, which you will need to commit to stop tracking that file.</p> <p>If the file has changes:</p> <ul> <li>Use <code>git rm --cached &lt;file&gt;</code> to keep the changes</li> <li>Use <code>git rm -f &lt;file&gt;</code> to remove the changes</li> </ul> <p></p> <p></p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#ignore-files","title":"Ignore files","text":"<p>Place a file called <code>.gitignore</code> at the root directory of your repository. A modified form of Glob Syntax may be used, one pattern per line, to indicate files that should be ignored by git. Any files or directories matching a line in the <code>.gitignore</code> file will not be tracked, unless the file has already been committed to the working tree.</p> <ul> <li><code>*</code>: a wildcard meaning any number of any characters except for a slash</li> <li><code>**</code>: a wildcard for matching directories</li> <li><code>!</code>: negates a pattern, causing inclusion instead of exclusion</li> <li>The last pattern in the file takes precedence</li> </ul> <pre><code># ignores all files with extension .txt\n*.txt\n\n# includes file despite previous rule\n!included.txt\n\n# ignores all directories called test\n**/test/\n\n# ignores all directories called build in site\nsite/**/build\n\n# ignores all files in ouptut, but not in subdirectories\noutput/*\n\n# recursively ignores all files in output, including subdirectories\noutput/**\n</code></pre> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#how-do-i-manage-branches","title":"How do I manage branches?","text":""},{"location":"workflow_solutions/git/#listing-branches","title":"Listing branches","text":"<p>To list local branches use <code>git branch</code>.</p> <p></p> <p>To list remote branches use <code>git branch -r</code>.</p> <p></p>"},{"location":"workflow_solutions/git/#checking-out-existing-branches","title":"Checking out existing branches","text":"<p>To checkout an existing local branch use <code>git checkout &lt;branch-name&gt;</code>.</p> <p></p> <p>To checkout and track a remote branch use <code>git checkout --track &lt;remote-name&gt;/&lt;branch-name&gt;</code>.</p> <p></p>"},{"location":"workflow_solutions/git/#creating-new-branches","title":"Creating new branches","text":"<p>To create a new local branch use <code>git branch &lt;branch-name&gt;</code>.</p> <p></p> <p>To create and checkout a new local branch use <code>git checkout -b &lt;branch-name&gt;</code>.</p> <p></p> <p>Note</p> <p>Branches are always created from the HEAD.</p>"},{"location":"workflow_solutions/git/#deleting-branches","title":"Deleting branches","text":"<p>To delete a local branch use <code>git branch --delete &lt;branch-name&gt;</code>.</p> <p>Warning</p> <p>Deleting remote branches can be destructive if active pull requests depend on them. Be sure you are deleting the correct remote branch.</p> <p>To delete a remote branch use <code>git push &lt;remote-name&gt; --delete &lt;branch-name&gt;</code>.</p>"},{"location":"workflow_solutions/git/#merging-branches","title":"Merging branches","text":"<p>To merge branch <code>A</code> into branch <code>B</code>, first select branch <code>B</code> using <code>git checkout B</code>, then merge <code>A</code> into <code>B</code> using <code>git merge A</code>.</p> <p>Some merges may cause conflicts. Conflict resolution is part art, part science, and beyond the scope of this document. Several strategies exist to minimize conflict frequency and scope. The general gist requires making smaller but more frequent commits of complete units.</p> <p>More information on merging is available at the official git webbook.</p>"},{"location":"workflow_solutions/git/#how-do-i-reset-changes","title":"How do I reset changes?","text":"<p>Danger</p> <p><code>git reset</code> can be highly destructive if used improperly. If you are unsure then please do not use <code>git reset --hard</code>.</p> <p>Use <code>git reset &lt;mode&gt; &lt;commit&gt;</code>. There are several modes:</p> <ul> <li><code>--hard</code> is destructive<ul> <li>changes since <code>&lt;commit&gt;</code> in the working tree and index are discarded</li> <li>all untracked files \"in the way\" of those changes are deleted</li> </ul> </li> <li><code>--mixed</code> (default) is non-destructive<ul> <li>changes since <code>&lt;commit&gt;</code> in the index are tracked but not staged</li> </ul> </li> <li><code>--soft</code> is non-destructive<ul> <li>changes since <code>&lt;commit&gt;</code> in the index are staged</li> </ul> </li> </ul> <p>If <code>&lt;commit&gt;</code> is left empty then the most recent commit on the working branch is used. If you wish to specify a commit, then provide enough characters at the start of the commit hash (7 characters is usually enough for unique identification).</p> <p>Danger</p> <p>Using <code>git reset --hard</code> is destructive. All changes since the last commit are discarded and are most likely lost.</p> <p>Use <code>git reset --mixed</code> (default) or <code>git reset --soft</code> instead.</p> <p>More information is available at the official git webbook.</p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#how-do-i-tag-a-commit","title":"How do I tag a commit?","text":"<p>Use <code>git tag</code>. Tags are useful for tracking software versions or milestones in the history. There are two types, lightweight and annotated. Here we only discuss lightweight tags.</p> <ul> <li>Tag HEAD: <code>git tag &lt;tag-label&gt;</code></li> <li>Tag a specific commit by hash: <code>git tag &lt;tag-label&gt; &lt;commit-hash&gt;</code></li> <li>Push tag to remote: <code>git push &lt;remote-name&gt; &lt;tag-label&gt;</code></li> <li>Delete tag: <code>git tag -d &lt;tag-label&gt;</code></li> <li>Delete tag from remote: <code>git push origin --delete &lt;tag-label&gt;</code></li> </ul> <p>More information is available at the official git webbook.</p> <p>Full documentation.</p>"},{"location":"workflow_solutions/git/#help-something-went-wrong","title":"Help! Something went wrong","text":"<p>Stop. Breathe. Don't panic. It's likely that your changes are still cached by git. Don't make any additional changes or run any git commits as this may cause cached files to be deleted. If you have some experience with git, then chances are good you can recover your lost changes. If you do not have much experience with git or do not feel confident recovering, then please contact Support.</p> <p>If you wish to proceed without assistance, then please visit the following two pages depending on your situation.</p>"},{"location":"workflow_solutions/git/#i-need-to-undo-a-change-that-ive-made","title":"I need to undo a change that I've made","text":"<p>Warning</p> <p>Do not use <code>git reset --hard</code> if you are not sure of what you are doing!</p> <p>Please visit: https://wwarriner.github.io/gitfix/, read each card carefully, and answer the questions until you arrive at a solution. Do not use <code>git reset --hard</code> unless you are sure of what that command will do.</p> <p>More information is available at the official git webbook.</p>"},{"location":"workflow_solutions/git/#ive-lost-changes-i-made","title":"I've lost changes I made","text":"<p>The changes may be lost permanently, or they may be partially recoverable. Git has a garbage collector for old versions of files that is emptied periodically as new commands are run. If you have not run any commands since losing changes, the old versions should still be available.</p> <p>To recover the changes:</p> <ul> <li>In a terminal, navigate to the repository directory.</li> <li>Use the command <code>git fsck --lost-found</code>.</li> <li>Navigate to the subdirectory <code>.git/lost-found/other</code>.</li> <li>Highly recommended: make a backup of the <code>.git/lost-found/other</code> folder before proceeding.</li> </ul> <p>The recovery will only be partial. File content should all be recoverable. However, directory structure and file names will be lost. This solution will require considerable manual or scripted work to identify files and restore directory structure and file names.</p>"},{"location":"workflow_solutions/git_collaboration/","title":"How to Collaborate with Git","text":"<p>Git is a powerful tool for version control of software and other plain-text information. However, Git alone is not ideal for enabling and facilitating collaboration between many users working on the same research software project. Be sure to check the Important Note on Terms if you aren't familiar with Git.</p> <p>If you are here because you need to know how to get software from either instance, please see Obtaining Software below.</p> <p>If you are here because you need a place to collaborate with others on a software project, please see Collaborating below.</p>"},{"location":"workflow_solutions/git_collaboration/#important-note-on-terms","title":"Important Note on Terms","text":"<p>On most pages in this documentation, we use \"local\" to refer to your laptop or desktop computer, and \"remote\" to refer to Cheaha or a Cloud.rc virtual machine.</p> <p>When dealing with Git and repository hosting services like GitHub and GitLab we use remote to refer to repositories that are on a repository hosting site like GitHub or GitLab, and local to refer to repositories that are not on a repository hosting site.</p> <p>To summarize:</p> <ul> <li>Git, GitHub, GitLab context<ul> <li><code>local</code> - the repository on a computer where you work on your code (laptop, desktop, Cheaha)</li> <li><code>remote</code> - a remote server or service that stores code (github, gitlab, etc)</li> </ul> </li> <li>Cheaha and Cloud.rc context<ul> <li><code>local</code> - the machine (laptop, desktop) you use to access Cheaha or the Cloud.rc VM</li> <li><code>remote</code> - Cheaha or the Cloud.rc VM</li> </ul> </li> </ul>"},{"location":"workflow_solutions/git_collaboration/#for-obtaining-software","title":"For Obtaining Software","text":""},{"location":"workflow_solutions/git_collaboration/#cloning-from-github","title":"Cloning from GitHub","text":"<p>To do anything with GitHub, you will first need to navigate to their website https://github.com and create an account.</p> <p>To clone a repository, be sure you have the repository URL. Then, using <code>git</code> at a terminal, clone the repository using whatever settings are appropriate. GitHub repository pages look something like the page for this documentation, shown below.</p> <p></p> <p>You may also use the \"Code\" button on the page to see instructions for cloning the repository.</p> <p></p> <p>More in-depth instructions, including for SSH cloning, are provided at the official documentation.</p>"},{"location":"workflow_solutions/git_collaboration/#cloning-from-gitlab","title":"Cloning from GitLab","text":"<p>To do anything with our GitLab instance, you will first need to create an account. Please see our GitLab Account Management page.</p> <p>To clone a repository, be sure you have the repository URL. Then, using <code>git</code> at a terminal, clone the repository using whatever settings are appropriate. Be sure to append <code>.git</code> to the end of the repository or the clone will note be successful. For example, if the URL is <code>https://gitlab.rc.uab.edu/user/repository</code> then you will clone <code>https://gitlab.rc.uab.edu/user/repository.git</code>. GitLab repository pages look like the example shown below.</p> <p></p> <p>You may also use the \"Clone\" button on the page to see instructions for cloning the repository.</p> <p></p> <p>More in-depth instructions, including for SSH cloning, are provided at the official documentation.</p>"},{"location":"workflow_solutions/git_collaboration/#for-collaborating","title":"For Collaborating","text":"<p>GitHub and GitLab can both be used for software project management, and have helpful tools to facilitate group collaboration within projects and across multiple projects.</p> <p>Both services use organizations to manage projects across a team of people: GitHub docs page, GitLab docs page. Within a GitHub organization, people and repositories can be arranged into teams. GitLab allows arrangement of people and repositories with projects.</p> <p>An important feature, used extensively for this documentation's GitHub repository, is the issue tracker. Both GitHub and GitLab have per-repository issue trackers. Collaborators can create and manage issues, label them, and resolve them.</p>"},{"location":"workflow_solutions/git_collaboration/#how-do-i-choose-between-github-and-gitlab","title":"How do I Choose Between GitHub and GitLab?","text":"<ul> <li>Want to collaborate publicly and outside UAB? Consider using GitHub.</li> <li>Want your project private or internal to UAB? Consider using our GitLab instance.</li> </ul> <p>It is possible to collaborate publicly using GitLab, but there may be additional challenges. While external collaborators can see a public GitLab repository on our instance, they can't make any changes or create issues without a XIAS Account.</p> <p>It is possible to collaborate privately using GitHub with no additional hurdles, but if your project contains sensitive or protected information of any kinds, it should not be posted to GitHub, even in private repositories. Please consult with us via Support before</p>"},{"location":"workflow_solutions/git_collaboration/#good-practice-for-organizing-a-lab-space","title":"Good Practice for Organizing a Lab Space","text":"<p>Below is a bulleted list of good practices for organizing a lab space. Each bullet is followed by links to relevant GitHub and GitLab documentation pages, as appropriate.</p> <ul> <li>Have an organization for your lab space. GitHub GitLab<ul> <li>The organization should have its PI or PIs as owners, i.e., the owner role. GitHub GitLab</li> <li>Other trusted individuals can be made administrators as needed, to delegate important and sensitive tasks that require elevated permissions.</li> </ul> </li> <li>For each software project, create a repository within your organization. GitHub, GitLab<ul> <li>By default, organization members will have access at their assigned role level. These can be changed by managing roles and using teams effectively, if needed. For smaller labs this is often not necessary.</li> <li>The created repository is the central one for the organization and should not be changed directly.</li> </ul> </li> <li>For every individual, including owners and admins, work should be performed on a personal fork of the repository and then merged by submitting pull/merge requests.<ul> <li>Forks are copies of repositories made as a snapshot at the moment they are created. From that point on they are independent repositories with some features to facilitate collaborative workflows. GitHub GitLab</li> <li>Pull/merge requests allow individuals to contribute to a central repository. They allow reviewers to check the changes to ensure code quality, and to provide reviews or request changes. They are the primary means of controlling how code changes over time, and who is allowed to make those changes. GitHub GitLab</li> <li>See the Fork-Pull/Merge Request Workflow Section for more details on this valuable method of change management.</li> </ul> </li> </ul>"},{"location":"workflow_solutions/git_collaboration/#the-fork-pullmerge-request-workflow","title":"The Fork-Pull/Merge Request Workflow","text":"<p>The Fork-Pull/Merge Request workflow is a central concept to effective collaboration on individual repositories. It allows code owners and admins to effectively control how code changes, while giving accountability and credit to code maintainers and programmers. Every person working on a project has an effective means of working independently while being able to pull their changes together in a central location. It also neatly ties into issue tracking, which is discussed in the Issue Tracking Section.</p> <p>The workflow assumes a central repository already exists within an organization on either GitHub or GitLab. The workflow is written from the point of view of a new programmer who wants to work on the repository. The programmer must have a local machine where they will do their work and it must have Git installed.</p> <ul> <li>One-time setup<ol> <li>Fork an individual repository (downstream) from the central organization repository (upstream). GitHub GitLab</li> <li>Clone the downstream fork to the local machine where the programming will happen.</li> </ol> </li> <li>Workflow<ol> <li>Decide on a set of changes to make. Good practice is only working on one conceptual unit at a time. One feature, one bug fix, or one documentation page. Prefer fixing bugs before adding features.</li> <li>Synchronize your downstream fork with the upstream fork to minimize risk of merge conflicts. GitHub GitLab</li> <li>Pull the downstream fork main branch to your local clone main branch.</li> <li>Create a working branch for intended changes. Give it a short, descriptive name like <code>feature-add-button</code> or <code>fix-broken-link</code>.</li> <li>Checkout the working branch.</li> <li>Make changes to the code on the local machine using your preferred editor. Make small units of change at a time, try not to commit too much, but make sure your changes don't break the code. There is an art to this that comes with practice, but don't be afraid of trying.</li> <li>Commit those changes to the working branch. Keep making changes and committing until the set of changes is complete.</li> <li>When all needed changes have been made, push the working branch to your fork.</li> <li>Create a pull/merge request from the downstream working branch to the upstream main branch. GitHub GitLab</li> <li>Wait for reviews, make needed changes, and hopefully merging of your request.</li> </ol> </li> </ul> <p>Sometimes merging will be blocked because of a merge conflict. One programmer may make changes to code being worked on by another, and the two changes come into conflict. If this occurs, below are some steps that may help resolve the issue. In some cases, conflict resolution is straightforward, but in other cases thought will be necessary to disentangle what code should be kept, what should be discarded, and what should be modified.</p> <ul> <li>The downstream programmer should try synchronizing their fork, pulling it to their local main branch, and [merging] the main branch into their working branch. The conflict may still occur on their local machine, but they will be able to more easily see and test the effects of various conflict resolution attempts.</li> <li>Use a three-way diff program or editor which will let you see both sets of conflicting code, and facilitate making changes and selections. VSCode has a built-in three-way merge editor.</li> <li>Be sure everyone is using the same formatting rules in their editors. Sometimes spurious conflicts can occur as a result of inconsistent formatting.</li> <li>To minimize risk of conflict, don't have more than one programmer work on the same section of code if possible.</li> </ul>"},{"location":"workflow_solutions/git_collaboration/#effective-issue-tracking","title":"Effective Issue Tracking","text":"<p>Effective use of issue tracking can greatly reduce cognitive load and simplify code management. It gives a central location where users and maintainers can report bugs, make feature requests, and ask for clarifications on usage and documentation. These issues are tracked over time, can be labeled and organized, and closed and reopened. GitHub GitLab</p> <p>The typical issue lifecycle, at a high level, is something like below.</p> <ol> <li>Create an issue. GitHub GitLab</li> <li>Ask for clarifications and discuss as needed.</li> <li>Use the Fork-Pull/Merge Request Workflow to resolve the issue. In the Pull Request description, put the text <code>Fixes #...</code> where <code>...</code> should be replaced by the issue's number. When the request is merged, the issue will automatically be linked to the request and closed.</li> </ol>"},{"location":"workflow_solutions/git_collaboration/#common-scenarios","title":"Common Scenarios","text":""},{"location":"workflow_solutions/git_collaboration/#uploading-an-existing-code-folder","title":"Uploading an Existing Code Folder","text":"<p>The process for this has a few intricate steps that may be unfamiliar even to regular users of git, and has a few pitfalls.</p> <ol> <li>Use <code>git init</code> in the top-level code folder on the local machine, if it is not already a git repository. If it already is a repository, be sure the primary branch is called <code>main</code>. Use <code>git branch -m &lt;oldname&gt; main</code>.</li> <li>Create a repository on the remote server GitHub, GitLab</li> <li>Use <code>git remote add origin &lt;url&gt;</code> to add the remote URL to the local repository with the name <code>origin</code>.</li> <li>Verify the URL is correct with <code>git remote -v</code>. Fix it with <code>git remote set-url origin &lt;url&gt;</code> if needed.</li> <li>Checkout the main branch without <code>git checkout main</code>.</li> <li>Use <code>git pull origin main --allow-unrelated-histories</code> to combine the main branches of the remote and local repository, within your local repository.</li> <li>Use <code>git push origin main</code> to push the combined histories to the remote repository.</li> <li>Be sure to verify the repository looks good at the GitHub/GitLab repository page (depending on which you used).</li> </ol> <p>Note</p> <p><code>--allow-unrelated-histories</code> is necessary because Git considers the remote repository to be a completely distinct entity from the local repository. Their histories are unrelated.</p>"},{"location":"workflow_solutions/git_collaboration/#https-vs-ssh-access","title":"HTTPS vs SSH Access","text":"<p>For most beginners using Git, GitHub and GitLab, HTTPS (hypertext transfer protocol secure) is probably a sufficient method for accessing in early stages. It is the default mode of accessing GitHub and GitLab when using Git at the command line. HTTPS is less secure than SSH (secure shell). We recommend learning to use SSH as soon as possible to minimize security risks. Below are links to GitHub and GitLab documentation for using SSH.</p> <p>GitHub SSH documentation GitLab SSH documentation</p>"},{"location":"workflow_solutions/r_environments/","title":"R Projects and Environments","text":"<p>When working on multiple projects, it's likely that different sets of external analysis packages and their dependencies will be needed for each project. Managing these different projects is simple in something like Anaconda by creating a different virtual environment for each project, but this functionality is not fully built into RStudio by default.</p> <p>Instead, we suggest to take advantage of R Projects and the renv package to keep environments separate for each project you start.</p> <p>Important</p> <p>If you are planning to use the <code>renv</code> package for environment management, you should install it before creating a project.</p>"},{"location":"workflow_solutions/r_environments/#r-projects","title":"R Projects","text":"<p>RStudio and Hadley Wickham have written extensively on using Projects to organize different research projects, so please read through that documentation. These docs will give a condensed overview of how to set up Projects.</p>"},{"location":"workflow_solutions/r_environments/#creating-a-project","title":"Creating a Project","text":"<p>To begin, projects start at a parent or root directory within which most, if not all, files associated with that project should be stored. To create a project, find the Project dropdown at the top-right of RStudio, above the Environment window.</p> <p></p> <ol> <li> <p>Click New Project... This will open up a screen to select whether you want to create a new folder for your project, use an existing folder, or clone an existing Git repository. The following instructions assume you choose to create a new directory.</p> <p></p> </li> <li> <p>Next you will need to choose your project type. There are a number of different preset projects for R packages, Shiny applications, and different implementations of Quarto. The top option will open a generic project, but any of these options can be converted to any other type of project.</p> <p></p> </li> <li> <p>Finally, you will choose your project name, the location for the project directory, as well as choose whether you want to initialize a git repo for the project. In addition, you can choose to use <code>renv</code> for package dependency management. Please read more about <code>renv</code> here. It's highly suggested to use <code>renv</code> for future environment reproducibility.</p> <p></p> </li> </ol> <p>Afterwards, RStudio will reset, change the working directory to the project root, and create a .RProj file that controls the project settings. The project dropdown will have the newly created project name now.</p>"},{"location":"workflow_solutions/r_environments/#project-settings","title":"Project Settings","text":"<p>At this point, you can start writting scripts as normal, but it would be useful to change some of the settings for the projects to help with some RStudio performance and set up Git and renv.</p>"},{"location":"workflow_solutions/r_environments/#general-settings","title":"General Settings","text":"<p>Click the project dropdown again and select Project Options at the bottom. A window will appear with general settings. We advise to change the general settings to the following:</p> <p></p> <p>This will create a clean environment each time you open the project and does not save environment variables when exiting RStudio. In our experience, trying to automatically load variables from a previous session can cause RStudio to open very slowly or crash depending on the amount of data being used. All variables can be recreated by running your scripts or by purposefully saving and loading selected variables from other data files such as csv or RData.</p>"},{"location":"workflow_solutions/r_environments/#git-settings","title":"Git Settings","text":"<p>Another useful part of Projects is Git integration. Normally, you would need to manage git using the command line even though your development is in RStudio, but with Projects you can link a remote git repository to your Project. It then provides a graphical interface for creating branches, committing changes, and generally managing a remote code repository.</p> <p>Note</p> <p>To read more about how to get started with Git, please read our git documentation</p> <ol> <li> <p>To begin, you should create an empty repository either at Github or UAB's Gitlab where your project will be stored. this will open a new page with instructions on linking this remote repository with your local project. Keep these instructions open for later. A picture of the important piece can be seen below.</p> <p></p> </li> <li> <p>Then open the Git/SVN tab in the Project Settings. It will have no option set:</p> <p></p> </li> <li> <p>Click the Version Control dropdown menu and select Git. RStudio will ask you if you want to initialize a new git repository. Click Yes. Restart RStudio if it asks you to. Now a new Git tab will be available in the upper right pane</p> <p></p> </li> <li> <p>You will then need to add a link to the remote repository as the origin.</p> <ol> <li> <p>Click the More dropdown in the Files tab in the bottom right pane and select Open New Terminal Here</p> <p></p> </li> <li> <p>Copy the instructions for pushing an existing repository from the Git repo into your terminal. You can see an example of these instructions under step 1, and commands specific to your repository were given after you created your repository. Run these commands to link the local Project to the remote repository. Afterwards, the Origin field in the Git/SVN options will have changed to your remote repository address.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"workflow_solutions/r_environments/#project-paths","title":"Project Paths","text":"<p>Projects are designed to be portable and so use relative paths the vast majority of the time. This means any folder or file should be referenced using the top level of the project as the root directory. For example, if I wanted to load the <code>subj01.csv</code> file from the <code>data</code> directory in the top level of the project, I would do that using <code>read.csv('data/subj01.csv')</code>. Absolute paths will cause scripts to break if the project is moved to a different location within the filesystem or onto another computer entirely.</p> <p>As well, opening a project will automatically set the working directory to be at the top level of the project. It's inadvisable to the change the working directory location while a project is open.</p>"},{"location":"workflow_solutions/r_environments/#renv","title":"renv","text":"<p>Most, if not all, projects will use some combination of the thousands of packages available in R such as the <code>tidyverse</code>. The <code>renv</code> package helps manage all of your project's package dependencies as well as provide a way to easily share package environments with other researchers. This functionality is very similar to <code>conda</code> and <code>virtualenv</code> environments for Python users. More information about <code>renv</code> can be found on their site. It's suggested to keep a record of the packages your project uses with either <code>renv</code> or another tool for general reproducibility.</p> <p>In order to use <code>renv</code>, you should install it either through the package install tool in RStudio or using the <code>install.packages</code> command before creating an environment. If you already have an existing project but want to use <code>renv</code>,you can install it and manually intialize the environment useing <code>renv::init()</code>.</p> <p><code>renv</code> manages package versions and dependencies separately for each project. All downloaded packages are, by default, stored in <code>$HOME/.cache/R/renv/cache/</code> and symlinks are created to these folders within a created <code>renv</code> folder in the project directory. If multiple projects use different versions of the same package, each individual version is installed and kept separate from the other versions. If a package has already been installed for one project, any future projects that use that same package version will automatically add a symlink to the existing package install saving on package installation and compilation time.</p>"},{"location":"workflow_solutions/r_environments/#installing-packages","title":"Installing Packages","text":"<p>You can make a change to your package environment at any point using a variety of installation methods. <code>renv</code> keeps track of packages installed from the following locations:</p> <ol> <li>CRAN using <code>install.packages</code> or the RStudio package installation interface</li> <li>Bioconductor using <code>BiocManager::install()</code></li> <li>Github using <code>devtools::install_github()</code> or <code>remotes::install_github()</code></li> <li>Gitlab using <code>devtools::install_gitlab()</code> or <code>remotes::install_gitlab()</code></li> <li>Bitbucket</li> </ol>"},{"location":"workflow_solutions/r_environments/#saving-an-environment","title":"Saving an Environment","text":"<p>Once your environment is set to your satisfaction, you can save the state of the environment in an <code>renv.lock</code> file saved in the top level of your project directory using the <code>renv::snapshot()</code> command. This will save information such as the repository the packages was installed from (CRAN, github, etc.), the version, and the requirements. It will also store which version of R the project was using.</p>"},{"location":"workflow_solutions/r_environments/#loading-an-environment","title":"Loading an Environment","text":"<p>Once an environment has been saved to a lockfile, it can be loaded again using the <code>renv::restore()</code> command. This is useful in a number of situations. For instance, if you have installed a package that you are not happy with and do not want to keep, you can revert to the previous environment save state to remove that package and all of its dependencies. Another situation this is useful is when you want to rebuild the environment on another computer or when you are sharing your environment with another researcher.</p>"},{"location":"workflow_solutions/shell/","title":"Shell Reference","text":""},{"location":"workflow_solutions/shell/#introductory-guides","title":"Introductory Guides","text":"<p>The shell is a powerful tool, and with great power comes great responsibility. The following warnings are not intended to frighten, but to give a sense of respect for the power of shell commands.</p> <p>Most commands are perfectly safe, and often when they do something unexpected it can be fixed with some work. We will do our best to warn you of commands with greater potential for destruction, but no documentation is perfect.</p> <p>We are not responsible for accidental deletions or overwrites caused inadvertently, or otherwise, by any commands run by researchers. Be warned that directories, files and file contents that are deleted or overwritten cannot be restored by us under any circumstances. Researchers are responsible for maintaining backups of their files. If in doubt about a command please contact Contact Us for guidance.</p> <p>If you would prefer additional information or a tutorial to guide your use of the Shell, please see our page containing links to educational content on Shell and other tools you may need.</p>"},{"location":"workflow_solutions/shell/#educational-resources","title":"Educational Resources","text":"<p>The internet has thousands of guides for using the shell. Rather than devise our own complete lesson plan for using the shell, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the shell lesson, see https://swcarpentry.github.io/shell-novice/.</p> <p>There are also additional resources that aid in learning and verifying shell commands and scripts:</p> <ul> <li>Explain Shell: An educational tool providing detailed explanations of individual commands in relatively reasonably-plain English. This tool doesn't explain what a command does at a high level nor its purpose or intent, only the details of the parts making up the command.</li> <li>ShellCheck: An online tool for conducting static analysis checks on shell scripts. The Git repository for this tool can be found here and it can also be installed via Anaconda.</li> </ul> <p>At the shell prompt, you can also use the command <code>curl cheat.sh/&lt;command&gt;</code> to get a simple-to-understand explanation of what the command does and how to use it (see curl). Below is an example for the pwd command.</p> <p></p>"},{"location":"workflow_solutions/shell/#reference","title":"Reference","text":""},{"location":"workflow_solutions/shell/#command-concepts","title":"Command Concepts","text":"<p>Commands are entered at the prompt. The prompt can take many forms, typically something like one of the following. Common features are: (1) a prompt character, often the dollar sign <code>$</code>; (2) a caret to indicate where characters will be inserted when you type, typically a blinking underscore <code>_</code> or rectangle; (3) color to enhance meaning of various components.</p> <ul> <li> <p>Bash on Cheaha...</p> <p></p> </li> <li> <p>Git Bash on Windows Desktop...</p> <p></p> </li> <li> <p>Oh My Zsh on Debian...</p> <p></p> </li> </ul> <p>Commands take the form <code>command [optional] &lt;required&gt;</code>. The word <code>command</code> should be replaced with the literal name of the command, such as <code>pwd</code>, <code>ls</code> and <code>cd</code>, among many others.</p> <p>The text <code>[optional]</code> is for flags and inputs that are not required to run the command or that have default values. These flags can be useful for modifying the behavior or output of the command.</p> <p>The text <code>[required]</code> is for flags and inputs that are required to run the command. These must be supplied by the user or the command will not function or produce an error.</p> <p>Flags start with the character <code>-</code> as with the <code>-l</code> flag in <code>ls -l</code> (see ls). Flags that do not require input can be combined as <code>ls -al</code>. Flags that require input may not be combined as with the flags <code>-n</code> and <code>-m 2</code> in <code>grep -n -m 2 pattern textfile.txt</code> (see grep).</p> <p>All inputs are separated by the space character Space. If you wish to or must use a space character in an input, that input must be surrounded by quotation marks. Note that single quotes and double quotes have different behavior. Single quotes <code>''</code> interpret all characters between them literally. Double quotes <code>\"\"</code> interprets special characters. In most cases, especially with variable contents, double quotes <code>\"\"</code> are preferred.</p> <p>All commands are run in a process. By default, commands run at the shell prompt are run in the shell process, and wait for execution to stop before returning control to you. It is possible to regain control earlier in a number of ways.</p> <p>Warning</p> <p>Copying commands from rich-text sources, such as <code>.pdf</code>, Microsoft Office and webpages, can result in copying special or invisible unicode characters. These characters can cause commands to behave unexpectedly and can be difficult to diagnose. Instead, please try pasting your command into a plain-text editor, like notepad, before copying to the shell prompt.</p>"},{"location":"workflow_solutions/shell/#how-do-i-regain-control-of-the-prompt-while-a-command-is-running","title":"How do I regain control of the prompt while a command is running?","text":"<ul> <li>Running commands may be terminated using Ctrl+C. Pressing it once will request a graceful termination of the running command. Pressing it more than one will attempt to immediately kill the program.</li> <li>Open a new shell terminal and use that instead.</li> <li>Start the command as <code>command [optional] &lt;required&gt; &amp;</code>. Note the trailing ampersand character <code>&amp;</code>, which causes the command to be run asynchronously in the background.</li> </ul>"},{"location":"workflow_solutions/shell/#how-do-i-terminate-a-process-running-asynchronously","title":"How do I terminate a process running asynchronously?","text":"<p>Danger</p> <p>The commands listed here can cause loss of work by termination of incorrect processes if not used carefully.</p> <p>To kill a process running in another shell terminal or running in the background, use either <code>kill</code> or <code>pkill</code> together with an appropriate signal flag. The flag <code>-15</code> sends <code>SIGTERM</code> which will allow the program to terminate itself gracefully. The flag <code>-9</code> sends <code>SIGKILL</code> and will immediately terminate the process, in case <code>-15</code> is not working.</p> <ul> <li><code>kill &lt;signal&gt; &lt;pid&gt;</code> if you know the process id <code>&lt;pid&gt;</code> of the process. Use <code>ps -u &lt;username&gt;</code> to see your running processes. On Cheaha you can use <code>ps -u $USER</code> as a shortcut.</li> <li><code>pkill &lt;signal&gt; &lt;name-pattern&gt;</code> if you know the name of the process.</li> </ul> <p>Warning</p> <p>Using <code>pkill</code> requires carefully thinking of an appropriate name pattern. An incorrect name pattern can cause unwanted termination of processes that may be important to you. Process termination cannot be stopped or undone.</p>"},{"location":"workflow_solutions/shell/#special-escaped-characters","title":"Special (Escaped) Characters","text":"<p>Backslash <code>\\</code> is used to write literal versions of certain special characters. Backslash is also called the escape character, and the special characters are also called escape sequences. Character escape sequences are useful in situations where you need a representation of a character, instead of the result of pressing the corresponding key on your keyboard. For example, if you want to store a newline character in a string then you can't just press Enter. If you did, you would immediately execute the command before you finished the string. Instead you can type <code>\\n</code>, which is the escape sequence for a newline character.</p> <ul> <li><code>\\t</code> is interpreted as a tab character. The plain-text equivalent of pressing Tab.</li> <li><code>\\n</code> is interpreted as a newline character. The plain-text equivalent of pressing Enter.</li> <li><code>\\\\</code> is interpreted as a single backslash.</li> </ul> <p>At the shell prompt, double quotes <code>\"\"</code> interpret newline characters, while single quotes <code>''</code> do not.</p> <p>Below is an example file containing escaped characters (shown in nano), and its interpreted output (shown with echo).</p> <p></p> <p></p>"},{"location":"workflow_solutions/shell/#piping-and-command-chains","title":"Piping and Command Chains","text":"<p>Commands may be composed into chains using pipes with the pipe character <code>|</code>. For example, <code>ls -l | wc -l</code> counts the number of lines returned by <code>ls -l</code> (see ls and wc). Warning This construct does not accurately count the number of items in a directory, and is only for demonstration purposes. Do not use this exact command chain in practice.</p> <p></p>"},{"location":"workflow_solutions/shell/#redirects","title":"Redirects","text":"<p>Command inputs and outputs may be redirected with the characters <code>&lt;</code> for input and <code>&gt;</code> for output. Output redirects using <code>&gt;</code> overwrite the contents of existing files and are destructive. Using <code>&gt;&gt;</code> in place of <code>&gt;</code> appends contents to a file, rather than overwriting the contents.</p> <ul> <li>Inputs come from <code>STDIN</code> or <code>0</code>.</li> <li>Typical output is written to <code>STDOUT</code> or <code>1</code> and errors are written to <code>STDERR</code> or <code>2</code>.</li> <li> <p><code>ls -l 1&gt; dirlist</code> stores the directory listing to the file <code>dirlist</code>. In this case using <code>&gt;</code> is the same as <code>1&gt;</code>.</p> <p></p> </li> <li> <p><code>ls -l doesnotexist 1&gt; error 2&gt;&amp;1</code> stores the error message to the file <code>error</code>. The text <code>2&gt;&amp;1</code> means write <code>STDERR</code> to <code>STDOUT</code>.</p> <p></p> </li> <li> <p><code>wc -l 0&lt; lines</code> reads the contents of file <code>lines</code> and counts the number of lines. Note that for <code>wc</code> this is not needed, but may be needed for other commands. In this case using <code>&lt;</code> is the same as <code>0&lt;</code>.</p> <p></p> </li> </ul> <p>For more information on the commands used in the examples, see ls, wc and cat.</p> <p>Danger</p> <p>Output redirects using <code>&gt;</code> are destructive. The contents of the target file are immediately overwritten when the command is executed. It is not possible to recover the contents of the file under any circumstances. Researchers are responsible for maintaining backups of their files.</p>"},{"location":"workflow_solutions/shell/#path-concepts","title":"Path Concepts","text":"<ul> <li>The working directory is the directory you are currently in and may be identified using the command <code>pwd</code>.</li> <li>Dot <code>.</code> is a shortcut for the working directory. This is only used in some contexts.</li> <li>Double dot <code>..</code> is a shortcut for the immediate parent directory of whatever comes before it.</li> <li>Twiddle <code>~</code> is a shortcut for your home directory.</li> <li>Forwardslash <code>/</code> is the path to the root directory of the filesystem, which has no parent.</li> <li>Files and directory names starting with <code>.</code> are hidden.</li> <li> <p>Paths are formed of text-based directory names separated by <code>/</code></p> <ul> <li>Absolute paths start at the root directory, e.g. <code>/home/user/documents/</code>.</li> <li>Relative paths start at the working directory, e.g. <code>bin</code>.</li> </ul> <p>Below are examples of constructed paths tested with ls.</p> <p></p> </li> </ul>"},{"location":"workflow_solutions/shell/#glob-syntax","title":"Glob Syntax","text":"<ul> <li> <p>Glob is a shorthand syntax for dealing with many files and directories matching simple patterns.</p> <ul> <li>Question mark <code>?</code> matches a single character. <code>c?t</code> matches both <code>cat</code> and <code>cut</code>.</li> <li>Star <code>*</code> matches any string. <code>c*</code> matches <code>cat</code>, <code>cut</code>, and <code>clatter</code>. <code>*.png</code> matches all <code>png</code> files.</li> <li>Double star <code>**</code> matches any number of directories with any names. <code>**/*.png</code> matches all <code>png</code> files within any subdirectory of the working directory. This is not commonly used, but extremely useful for some applications.</li> </ul> <p>Below are examples of glob usage tested with ls.</p> <p></p> </li> </ul>"},{"location":"workflow_solutions/shell/#environment-concepts","title":"Environment Concepts","text":"<ul> <li> <p>Environment variables may be assigned by using <code>var=value</code> where <code>var</code> is the variable name and <code>value</code> is its value. Below is an example tested with echo.</p> <p></p> </li> <li> <p>Environment variable values may be expanded by using <code>\"${var}\"</code> where <code>var</code> is the variable name.</p> <ul> <li>On Cheaha...<ul> <li><code>\"${HOME}\"</code> expands to the path to your home directory.</li> <li><code>\"${USER_DATA}\"</code> expands to the path to your <code>data/user/&lt;username&gt;</code> directory.</li> <li><code>\"${USER}\"</code> expands to your user name.</li> </ul> </li> <li> <p>Always use double quotes around variables. DO use <code>\"${HOME}\"</code>, do NOT use <code>${HOME}</code>. Double quotes ensure that space characters in expanded values are handled appropriately. Below, note the error occurring without double quotes due to the spaces in the directory name. Double quoting the variable fixes the error. The commands used to test are ls, cd and pwd.</p> <p></p> </li> <li> <p>Expanding a variable that isn't defined returns an empty string and does not produce an error, but may cause unexpected behavior.</p> </li> </ul> </li> <li> <p>Environment variables may be expanded in paths and command arguments to save effort and time.</p> <ul> <li><code>\"${USER_DATA}/project/inputs\"</code></li> <li><code>command ${VARIABLE}</code></li> </ul> </li> </ul> <p>Warning</p> <p>Modifying, changing or overwriting existing environment variables while in a shell session can result in unexpected behavior. The environment can be reset to its default starting state by exiting the shell session and starting a new session.</p>"},{"location":"workflow_solutions/shell/#script-concepts","title":"Script Concepts","text":"<ul> <li>Scripts are a way to bundle many commands together and execute them in sequence.</li> <li>Scripts should start with the intended interpreter using a hash-bang like <code>#!/bin/bash</code>.<ul> <li>Most commonly <code>bash</code> is the intended interpreter on our systems.</li> <li>Other shell interpreters may be installed and used, but are not necessary.</li> </ul> </li> <li>To execute the script given by the hash-bang or she-bang (pronounced shih-bang) <code>#!</code>, use <code>./script.sh</code> in the folder containing the script. Executable permissions must be set to use a script this way, with <code>chmod u+x &lt;script-path&gt;</code>.</li> <li>To execute the script using a specific interpreter use <code>bash &lt;script-path&gt;</code>, or replace <code>bash</code> with your preferred interpreter. Beware that not all interpreters behave the same way. Executable permissions do not need to be set to use a script this way.</li> <li>Space-separated arguments may be passed to a script when executed in the same way as any other command.</li> </ul>"},{"location":"workflow_solutions/shell/#script-arguments","title":"Script Arguments","text":"<p>Arguments or parameters are passed to a command or script as a space separated list. Arguments may be referred to using numeric variables. The following list contains examples of variable references to arguments.</p> <ul> <li><code>\"${0}\"</code> is the execution path. If you use <code>./script.sh</code> then <code>\"${0}\"</code> will be <code>./script.sh</code>.</li> <li><code>\"${1}\"</code>, <code>\"${2}\"</code>, etc., are the first, second, etc., space separated variables. Calling <code>./script.sh hello world</code> will have <code>$1 = hello</code> and <code>$2 = world</code>. Many arguments may be passed this way.</li> <li><code>${@}</code> is all arguments except <code>\"${0}\"</code>. Important!! Note that double quotes <code>\"</code> are not used! Double quoting would bind all the arguments together. If you need to pass a group of arguments to another script, be cautious about using quotes.</li> <li><code>${@: 2}</code> is all arguments starting with <code>\"${2}\"</code>.</li> <li><code>${@: 2:2}</code> is the second and third argument.</li> <li><code>${@: -2}</code> is the last two arguments.</li> <li><code>${@: -2:1}</code> is the second to last argument only.</li> </ul> <p>Below is an example script file, hash-bang not shown, demonstrating how each argument variable works, and its interpreted output. The text editor nano is used to display the file and chmod is used to modify file permissions.</p> <p></p> <p></p> <p>Tip</p> <p>Using the shell requires some defensive techniques. Never use the space character Space in variables, directory names, file names, etc. Instead, only use letters, numbers and the underscore character <code>_</code>. In <code>bash</code> it is also allowed to use the hyphen character <code>-</code>, but this may not be portable to other shell interpreters. Names should only start with one or more letters or numbers.</p> <p>Do protect yourself from others who might use the space character Space by always double quoting your variables like <code>\"${var}\"</code> instead of <code>${var}</code> or <code>$var</code>.</p>"},{"location":"workflow_solutions/shell/#commands-for-solutions-to-common-problems","title":"Commands for Solutions to Common Problems","text":"<p>Below is a reference guide to various commands through the lens of problems to be solved.</p> <p>Note</p> <p>When you see words surrounded by angle brackets like <code>&lt;name&gt;</code>, you should not take that as a literal part of the command. In the case of <code>&lt;name&gt;</code> you would replace it with whatever name is appropriate.</p> <p>Important</p> <p>If you are using Cheaha and working with more than a few files or directories, or the files are large, please run your shell commands in a Job Context.</p> <p>Danger</p> <p>It is safest to assume that any command run at the shell cannot be undone. Be especially aware of the <code>rm</code> command, which is destructive. We do not maintain backups of any files, so once those files are removed or deleted they cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files.</p>"},{"location":"workflow_solutions/shell/#show-working-directory-pwd","title":"Show working directory (<code>pwd</code>)","text":"<p>Use <code>pwd</code>, which stands for present working directory.</p> <p></p>"},{"location":"workflow_solutions/shell/#list-files-and-directories-ls","title":"List files and directories (<code>ls</code>)","text":"<p>Below are common uses of <code>ls</code>, short for \"list\", used to display directory contents and examine details of files and directories. It may be used to check file permissions when using chmod.</p> <ul> <li> <p>Visible files only, list: <code>ls</code>. Multiple entries per line.</p> <p></p> </li> <li> <p>Visible files only, table: <code>ls -l</code>. One entry per line and shows permissions, size in bytes on disk, and timestamp.</p> <p></p> </li> <li> <p>Visible and hidden files, list <code>ls -a</code>. Same as <code>ls</code>, but has hidden files and directories and <code>.</code> and <code>..</code>. Example below is truncated to conserve page space.</p> <p></p> </li> <li> <p>Visible and hidden files, table <code>ls -al</code>. Same as <code>ls -l</code> but has hidden files and directories and <code>.</code> and <code>..</code>. Example below is truncated to conserve page space.</p> <p></p> </li> </ul>"},{"location":"workflow_solutions/shell/#examine-disk-usage-du","title":"Examine disk usage (<code>du</code>)","text":"<p>Use <code>du</code> to examine disk usage of files and directories. By default all values are given in bytes, use the flag <code>-h</code> to give values in <code>K</code>, <code>M</code>, <code>G</code> and <code>T</code> for kilobytes, megabytes, gigabytes and terabytes, respectively. Use the flag <code>-s</code> to summarize space used by directories. Below is an example of <code>du -sh</code>. Note that only directories with read permissions can be examined by <code>du</code>.</p> <p>When culling files to conserve storage space, it helps to find the largest files and directories. To find the ten largest, use the command <code>du -sh .[^.]* * | sort -hr | head -n10</code> in the top-level directory of your data. To better understand this command chain, see also sort, head and piping.</p>"},{"location":"workflow_solutions/shell/#change-working-directory-cd","title":"Change working directory (<code>cd</code>)","text":"<ul> <li>To change to a different directory, use <code>cd &lt;directory-path&gt;</code>.<ul> <li>The variable <code>&lt;path&gt;</code> may be relative, like <code>my/path</code>. This will move to the subdirectory <code>my/path</code> within the working directory. Relative directory paths may contain multiple <code>..</code> shortcuts to indicate parent directories.</li> <li>Or <code>&lt;path&gt;</code> may be absolute, like <code>/my/path</code> which will move to the directory <code>/my/path</code>, starting at the root of the filesystem. Recall the root directory is just <code>/</code>.</li> </ul> </li> <li>To move up to the parent of the working directory, use <code>cd ..</code>.</li> </ul> <p>Below are examples of <code>cd</code> usage, tested with ls.</p> <p></p>"},{"location":"workflow_solutions/shell/#copy-files-and-directories-cp","title":"Copy files and directories (<code>cp</code>)","text":"<p>Below are use cases with associated commands, and examples tested using ls.</p> <ul> <li> <p>Single file, change name: <code>cp -a &lt;source-file-path&gt; &lt;destination-file-path&gt;</code>.</p> <p></p> </li> <li> <p>Single file, keep name: <code>cp -a &lt;source-file-path&gt; &lt;destination-directory-path&gt;</code>.</p> </li> <li> <p>Directory: <code>cp -a &lt;source-directory-path&gt; &lt;destination-directory-path&gt;</code>.</p> <p></p> </li> <li> <p>Glob: <code>cp -a &lt;source-path-glob&gt; &lt;destination-directory-path&gt;</code>.</p> </li> </ul>"},{"location":"workflow_solutions/shell/#move-files-and-directories-mv","title":"Move files and directories <code>mv</code>","text":"<p>Below are use cases with associated commands, and examples tested using ls.</p> <ul> <li> <p>Single file, change name: <code>mv &lt;source-file-path&gt; &lt;destination-file-path&gt;</code>.</p> <p></p> </li> <li> <p>Single file, keep name: <code>mv &lt;source-file-path&gt; &lt;destination-directory-path&gt;</code>.</p> </li> <li> <p>Directory: <code>mv &lt;source-directory-path&gt; &lt;destination-directory-path&gt;</code>.</p> <p></p> </li> <li> <p>Glob: <code>mv &lt;source-path-glob&gt; &lt;destination-directory-path&gt;</code>.</p> </li> </ul>"},{"location":"workflow_solutions/shell/#delete-files-and-directories-rm-rmdir","title":"Delete files and directories (<code>rm</code>, <code>rmdir</code>)","text":"<p>Danger</p> <p>The <code>rm</code> command is destructive and cannot be undone. We do not maintain backups of any files, so files that are removed or deleted cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files.</p> <p>Below are use cases with associated commands, and examples tested using ls.</p> <ul> <li> <p>Single file: <code>rm &lt;file-path&gt;</code>.</p> <p></p> </li> <li> <p>Empty Directory: <code>rmdir &lt;directory-path&gt;</code>.</p> </li> <li> <p>Directory with Contents: <code>rm -r &lt;directory-path&gt;</code>.</p> <p></p> </li> <li> <p>Glob: <code>rm &lt;file-path-glob&gt;</code>.</p> </li> </ul> <p>Warning</p> <p>Careless use of the directory and glob forms of <code>rm</code> can lead to unwanted data loss. Be sure to double check your commands before executing.</p>"},{"location":"workflow_solutions/shell/#download-files-from-internet-sources-curl","title":"Download files from internet sources (curl)","text":"<p>Use <code>curl</code> to download files and webpages from internet sources. By default <code>curl</code> writes to <code>STDOUT</code>. If you wish to save the output to a file, use the <code>-o &lt;file-path&gt;</code> command. Note that <code>curl</code> does not transform, encode or decode the data in any way, and it is saved exactly as received from the supplied url.</p> <p>By far the most common usage is to download a file. To do so use <code>curl -o &lt;file-path&gt; &lt;url&gt;</code>, where <code>&lt;file-path&gt;</code> is the desired local path to save and <code>&lt;url&gt;</code> is the web address of the source data.</p>"},{"location":"workflow_solutions/shell/#create-directories-mkdir","title":"Create directories (<code>mkdir</code>)","text":"<p>Use <code>mkdir &lt;directory-name&gt;</code>. Example below is tested using ls.</p> <p></p>"},{"location":"workflow_solutions/shell/#create-files-touch","title":"Create files (<code>touch</code>)","text":"<p>Use <code>touch &lt;file-path&gt;</code>. Example below is tested using ls.</p> <p></p>"},{"location":"workflow_solutions/shell/#edit-plain-text-files-nano","title":"Edit plain-text files (<code>nano</code>)","text":"<p>Use <code>nano &lt;file-path&gt;</code>. If the file exists, it will be opened. If the file does not exist, it will be stored in memory until saved. If the contents are not saved they will be discarded and cannot be recovered. You may also use just <code>nano</code>, without a file path, to create a new empty file.</p> <p></p> <p>The character <code>^</code> is the Ctrl key, so <code>^y</code> would require pressing Ctrl+Y. To save use Ctrl+X to open the exit prompt.</p> <p></p> <p>Press Y for yes to bring up the save prompt.</p> <p></p> <p>If you opened <code>nano</code> using an existing file, or passed in a file path, then the <code>File Name to Write</code> prompt will be autofilled with the given file name. If not, you will need to enter it here. When you have the file name entered as desired, press Enter to save. Below is a demonstration of the shell with the new file listed using <code>ls</code>. Example below is tested using ls.</p> <p></p>"},{"location":"workflow_solutions/shell/#searching-for-text-in-nano","title":"Searching for text in <code>nano</code>","text":"<p>Use the key combination Ctrl+W to search for text. Follow the prompts as they appear to navigate.</p>"},{"location":"workflow_solutions/shell/#count-lines-words-and-characters-wc","title":"Count lines, words and characters (<code>wc</code>)","text":"<ul> <li>Lines: <code>wc -l &lt;file-path&gt;</code>.</li> <li>Words: <code>wc -w &lt;file-path&gt;</code>.</li> <li>Characters: <code>wc -m &lt;file-path&gt;</code>.</li> <li>All: <code>wc &lt;file-path&gt;</code>.</li> </ul> <p>Below are example outputs of a file named <code>newfile</code> containing the text <code>hello world</code> followed by a blank line. Note the file is redirected to <code>wc</code> from <code>STDIN</code>.</p> <p></p>"},{"location":"workflow_solutions/shell/#display-file-contents-cat-less","title":"Display file contents (<code>cat</code>, <code>less</code>)","text":"<p>Use <code>cat &lt;file-path&gt;</code> to display the contents of the file at <code>&lt;file-path&gt;</code>.</p> <p></p> <p>If the contents are too long to be displayed on one screen, you can pipe the output to <code>less</code> to add scrolling functionality by using <code>cat &lt;file-path&gt; | less</code>. Use the keyboard key Q to stop using <code>less</code>.</p> <p>Below are two images showing what <code>less</code> looks like. Note the last line of each image, which indicates you are using the <code>less</code> environment. The first image is an example where there is more text below the visible text. The second image is an example at the bottom of the text.</p> <p></p> <p></p>"},{"location":"workflow_solutions/shell/#examine-start-and-end-of-file-head-tail","title":"Examine start and end of file (<code>head</code>, <code>tail</code>)","text":"<p>To display only the first two lines use <code>head -n 2 &lt;file-path&gt;</code>. Use <code>tail</code> instead of <code>head</code> for the last two lines.</p> <p>Below is an image showing the use of <code>head</code> and <code>tail</code> on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks.</p> <p></p>"},{"location":"workflow_solutions/shell/#sort-file-contents-sort","title":"Sort file contents (<code>sort</code>)","text":"<ul> <li>Alphabetical: <code>sort &lt;file-path&gt;</code>.</li> <li>Numeric: <code>sort -n &lt;file-path&gt;</code>.</li> <li>Ignore case: <code>sort -i &lt;file-path&gt;</code>.</li> </ul> <p>Lines in the input file are returned in sorted order. The results are displayed in stdout. To write the sorted result to a file use <code>sort &lt;file-path&gt; &gt; &lt;new-file-path&gt;</code>.</p> <p>Below is an image showing the use of <code>head</code> and <code>tail</code> on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks.</p> <p></p> <p></p>"},{"location":"workflow_solutions/shell/#test-a-command-echo","title":"Test a command (<code>echo</code>)","text":"<p>Use <code>echo \"&lt;command&gt;\"</code> to see the expanded command without executing it. For example</p> <pre><code>echo \"cp -a $USER_DATA/mydir $HOME\"\n# prints\ncp -a /data/user/&lt;username&gt;/mydir /home/&lt;username&gt;\n</code></pre> <p>Use the <code>-e</code> flag if you need to interpret escaped characters such as <code>\\t</code> for tab or <code>\\n</code> for new line. The command being examined in the example below is cp</p> <p></p>"},{"location":"workflow_solutions/shell/#search-for-text-grep","title":"Search for text (<code>grep</code>)","text":"<p>Use <code>grep \"&lt;pattern&gt;\" \"&lt;file-path&gt;\"</code> to search for <code>&lt;pattern&gt;</code> in the file at <code>&lt;file-path&gt;</code>. Use the <code>-n</code> flag to display line numbers with results.</p> <p>Below is an example of <code>grep -n</code> on a file. The number at the start of the result line is the line number of the pattern match. The matched portion is shown in red while other text on the same line is shown in white. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. We are looking for the literal text \"echo\" within the file.</p> <p></p>"},{"location":"workflow_solutions/shell/#close-the-session-exit","title":"Close the session (<code>exit</code>)","text":"<p>Use <code>exit</code>.</p>"},{"location":"workflow_solutions/shell/#clear-the-shell-display-clear","title":"Clear the shell display (<code>clear</code>)","text":"<p>Use <code>clear</code>.</p>"},{"location":"workflow_solutions/shell/#where-is-a-command-located-which","title":"Where is a command located? (<code>which</code>)","text":"<p>Use <code>which &lt;command-name&gt;</code>. The command being searched for is ls.</p> <p></p>"},{"location":"workflow_solutions/shell/#what-does-a-command-do-whatis-man","title":"What does a command do? (<code>whatis</code>, <code>man</code>)","text":"<p>For builtin command and aliases there are two distinct options for learning more. The command being examined is ls.</p> <ul> <li>Use <code>whatis &lt;command-name&gt;</code> to get a brief summary of the command.</li> <li>Use <code>man &lt;command-name&gt;</code> to get the full help file in a <code>less</code> environment. Use <code>q</code> on your keyboard to exit <code>less</code>.</li> </ul> <p></p>"},{"location":"workflow_solutions/shell/#remotely-access-shell-on-other-machines-ssh","title":"Remotely access shell on other machines (<code>ssh</code>)","text":"<p>See our SSH Section for more detailed information.</p>"},{"location":"workflow_solutions/shell/#remotely-access-or-transfer-files-between-machines-sftp","title":"Remotely access or transfer files between machines (<code>sftp</code>)","text":"<p>See our Remote Access - Data Transfer Section for more detailed information.</p>"},{"location":"workflow_solutions/shell/#submit-and-manage-jobs-on-cheaha","title":"Submit and manage jobs on Cheaha","text":"<p>See our Slurm Section for more detailed information.</p>"},{"location":"workflow_solutions/shell/#manage-permissions-of-files-and-directores-chmod","title":"Manage permissions of files and directores (<code>chmod</code>)","text":"<p>Use <code>chmod</code> with the least permissions needed to accomplish a task.</p> <p>Permission management is an important part of managing access and control of files and directories.</p> <p>Danger</p> <p>Please carefully consider security when working in shared spaces like Cheaha. Setting private directories or files as readable by other users can inadvertently expose sensitive or protected information and may violate IT policy, FERPA, HIPAA or some combination.</p> <p>There are legitimate use cases for truly shared spaces. Please Contact Us if you need to share information with other users or collaborators and aren't sure of how to do so securely.</p>"},{"location":"workflow_solutions/shell/#what-permissions-do","title":"What Permissions Do","text":"<p>Setting the permissions of a file affect the contents of only that file. A read-only file can still be deleted by users with write permissions in its parent directory.</p> <ul> <li>Read permissions allow viewing and copying contents of a file.</li> <li>Write permissions allow changing the contents of a file, including deleting all of the contents.</li> <li>Execute permissions allow using the file as an executable. Helpful for scripts and compiled programs. Scripts and interpreted language files, like Python, must also have read permission set.</li> </ul> <p>Setting the permissions of directories affects what can be done with contained files and directories.</p> <ul> <li>Read permission allows the use of <code>ls</code> within the directory. <code>cp</code> may be used to copy files from the directory to somewhere else.</li> <li>Write permission allows creation of files and directories, as well as the use of <code>touch</code>, <code>mv</code> and <code>rm</code> on files and directories, within the directory. <code>cp</code> may be used to copy files into the directory.</li> <li>Executable permission allows setting the directory as working directory and the use of <code>cd</code> into the directory.</li> </ul> <p>Permissions are not inherited from their parent directory.</p>"},{"location":"workflow_solutions/shell/#how-to-check-permissions","title":"How to Check Permissions","text":"<p>Use <code>ls -ald &lt;path&gt;</code> to see the permissions on the file or directory at <code>&lt;path&gt;</code>. The <code>-d</code> flag lists directories instead of their contents.</p>"},{"location":"workflow_solutions/shell/#patterns-for-setting-permissions","title":"Patterns for Setting Permissions","text":"<p>Two separate patterns can be used to set or change permissions on files and directories. Either may be used, but they cannot be combined in a single use of <code>chmod</code>. In the example images below, the command ls is used to check permissions and the command cat is used to display the contents of the script.</p> <ol> <li> <p>Symbols</p> <p>The letter and symbol pattern is in the form <code>a=r</code>. There are three parts.</p> <ol> <li> <p>A collection of letters denoting who, e.g. <code>a</code> in this case. Multiple letters may be used.</p> <ul> <li><code>u</code> the owner of the file or directory</li> <li><code>g</code> the owner's group members</li> <li><code>o</code> users outside the owner's group</li> <li><code>a</code> all users (same as <code>ugo</code>).</li> </ul> </li> <li> <p>A symbol indicating how to change the permissions</p> <ul> <li><code>=</code> set permissions</li> <li><code>-</code> remove permissions</li> <li><code>+</code> add permissions</li> </ul> </li> <li> <p>A collection of letters denoting which permissions to change. Multiple letters may be used.</p> <ul> <li><code>r</code> read</li> <li><code>w</code> write (change the contents)</li> <li><code>x</code> execute</li> </ul> </li> </ol> <p>To add executable permission for only the owner <code>chmod u+x &lt;file-path&gt;</code>. Useful for custom scripts and compiled executables you will use directly from the command line. To set read-only permission for everyone use <code>chmod a=r &lt;file-path&gt;</code>.</p> <p>Note</p> <p>Using <code>=</code> to set permissions will both add and remove permissions. Using <code>a=r</code> will take away existing write and execute permissions.</p> <p>Below is an example of <code>chmod</code> used symbolically to set user execute permissions on a script. Note the error before permissions are set.</p> <p></p> </li> <li> <p>Numerals</p> <p>Bit mask patterns are in the form <code>755</code>. Each digit is the sum of three binary bits. The bits are</p> <ul> <li><code>4</code> read</li> <li><code>2</code> write</li> <li><code>1</code> execute</li> </ul> <p>The left digit is the owner's permissions. Middle digit is the owner's group. Right digit is users outside the owner's group.</p> <p>Setting <code>chmod 755</code> means the following:</p> <ul> <li>For the owner, set <code>4</code> read, <code>2</code> write and <code>1</code> execute. <code>4+2+1=7</code>.</li> <li>For the owner's group, set <code>4</code> read, <code>1</code> execute. <code>4+1=5</code>.</li> <li>For other users, set <code>4</code> read, <code>1</code> execute. <code>4+1=5</code>.</li> </ul> <p>Setting <code>755</code> is a common pattern for system-wide scripts. Because the <code>4</code> read and <code>1</code> executable bits are set for all users, it can be called from anywhere by any other script, and not just the owner. However, the <code>2</code> write bit is set only for the owner, so other users cannot modify the contents of the script.</p> <p>Below is an example of <code>chmod</code> used numerically to add execution permissions to all users. Note the error before permissions are set.</p> <p></p> </li> </ol>"},{"location":"workflow_solutions/shell/#examples","title":"Examples","text":"<ul> <li><code>chmod u+x script.sh</code> adds execute permission for you, to a script.</li> <li><code>chmod 755 script.sh</code> makes a script readable and executable by all, but only writeable by you. This is a common permission for non-sensitive files and directories.</li> <li><code>chmod 740 sensitive_directory</code> make a directory readable by you and your group, and writeable and executable by only you. Other users cannot delete files in this folder.</li> <li><code>chmod ug=r notes.txt</code> followed by <code>chmod o-rwx notes.txt</code> makes a file read-only for you and your group and removes all permissions for other users.</li> </ul>"},{"location":"workflow_solutions/shell/#manage-group-ownership-chgrp","title":"Manage group ownership (<code>chgrp</code>)","text":"<p>To change group ownership of files and directories use <code>chgrp</code>.</p> <p>The command <code>chgrp</code> may only be used on a file or directory if you own it, or if you are a member of its current group owner and a member of its new group owner.</p> <p>For single files use <code>chgrp &lt;new-group&gt; &lt;file&gt;</code>.</p> <p>To change a directory and all of its contents recursively use <code>chgrp -hR &lt;new-group&gt; &lt;file&gt;</code>. The <code>-h</code> flag will avoid walking through the targets of symbolic links.</p> <p>Warning</p> <p>When using <code>chgrp -R</code>, the default behavior is to walk through the contents of symbolic links. If this is not desired, use <code>-hR</code>.</p>"},{"location":"workflow_solutions/shell/#manage-researcher-access-to-files-and-directories-getfacl-setfacl","title":"Manage researcher access to files and directories (<code>getfacl</code>, <code>setfacl</code>)","text":"<p>Construction</p> <p>Under construction.</p>"},{"location":"workflow_solutions/using_anaconda/","title":"Anaconda","text":"<p>Python is a high level programming language that is widely used in many branches of science. As a result, many scientific packages have been developed in Python, leading to the development of a package manager called Anaconda. Anaconda is the standard in Python package management for scientific research.</p> <p>Benefits of Anaconda:</p> <ul> <li>Shareability: environments can be shared via human-readable text-based YAML files.</li> <li>Maintainability: the same YAML files can be version controlled using git.</li> <li>Repeatability: environments can be rebuilt using those same YAML files.</li> <li>Simplicity: dependency matrices are computed and solved by Anaconda, and libraries are pre-built and stored on remote servers for download instead of being built on your local machine.</li> <li>Ubiquity: nearly all Python developers are aware of the usage of Anaconda, especially in scientific research, so there are many resources available for learning how to use it, and what to do if something goes wrong.</li> </ul> <p>Anaconda can also install Pip and record which Pip packages are installed, so Anaconda can do everything Pip can, and more.</p> <p>Important</p> <p>If using Anaconda on Cheaha, please see our Anaconda on Cheaha page for important details and restrictions.</p>"},{"location":"workflow_solutions/using_anaconda/#using-anaconda","title":"Using Anaconda","text":"<p>Anaconda is a package manager, meaning it handles all of the difficult mathematics and logistics of figuring out exactly what versions of which packages should be downloaded to meet your needs, or inform you if there is a conflict.</p> <p>Anaconda is structured around environments. Environments are self-contained collections of researcher-selected packages. Environments can be changed out using a simple package without requiring tedious installing and uninstalling of packages or software, and avoiding dependency conflicts with each other. Environments allow researchers to work and collaborate on multiple projects, each with different requirements, all on the same computer. Environments can be installed from the command line, from pre-designed or shared YAML files, and can be modified or updated as needed.</p> <p>The following subsections detail some of the more common commands and use cases for Anaconda usage. More complete information on this process can be found at the Anaconda documentation. Need some hands-on experience, you can find instructions on how to install PyTorch and TensorFlow using Anaconda in this tutorial.</p> <p>Important</p> <p>If using Anaconda on Cheaha, please see our Anaconda on Cheaha page for important details and restrictions.</p>"},{"location":"workflow_solutions/using_anaconda/#create-an-environment","title":"Create an Environment","text":"<p>In order to create a basic environment with the default packages, use the <code>conda create</code> command:</p> <pre><code># create a base environment. Replace &lt;env&gt; with an environment name\nconda create -n &lt;env&gt;\n</code></pre> <p>If you are trying to replicate a pipeline or analysis from another person, you can also recreate an environment using a YAML file, if they have provided one. To replicate an environment using a YAML file, use:</p> <pre><code># replicate an environment from a YAML file named env.yml\nconda create -n &lt;env&gt; -f &lt;path/to/env.yml&gt;\n</code></pre> <p>By default, all of your conda environments are stored in <code>/home/&lt;user&gt;/.conda/envs</code>.</p>"},{"location":"workflow_solutions/using_anaconda/#activate-an-environment","title":"Activate an Environment","text":"<p>From here, you can activate the environment using either <code>source</code> or <code>conda</code>:</p> <pre><code># activate the virtual environment using source\nsource activate &lt;env&gt;\n\n# or using conda\nconda activate &lt;env&gt;\n</code></pre> <p>To know your environment has loaded, the command line should look like:</p> <pre><code>(&lt;env&gt;) [BlazerID@c0XXX ~]$\n</code></pre> <p>Once the environment is activated, you are allowed to install whichever python libraries you need for your analysis.</p>"},{"location":"workflow_solutions/using_anaconda/#install-packages","title":"Install Packages","text":"<p>To install packages using Anaconda, use the <code>conda install</code> command. The <code>-c</code> or <code>--channel</code> command can be used to select a specific package channel to install from. The <code>anaconda</code> channel is a curated collection of high-quality packages, but the very latest versions may not be available on this channel. The <code>conda-forge</code> channel is more open, less carefully curated, and has more recent versions.</p> <pre><code># install most recent version of a package\nconda install &lt;package&gt;\n\n# install a specific version\nconda install &lt;package&gt;=version\n\n# install from a specific conda channel\nconda install -c &lt;channel&gt; &lt;package&gt;&lt;=version&gt;\n</code></pre> <p>Generally, if a package needs to be downloaded from a specific conda channel, it will mention that in its installation instructions.</p>"},{"location":"workflow_solutions/using_anaconda/#installing-packages-with-pip","title":"Installing Packages with Pip","text":"<p>Some packages are not available through Anaconda. Often these packages are available via PyPI and thus using the Python built-in Pip package manager. Pip may also be used to install locally-available packages as well.</p> <p>Important</p> <p>Make sure <code>pip</code> is installed within the <code>conda</code> environment and use it for installing packages within the <code>conda</code> environment to prevent Pip related issues.</p> <pre><code># install most recent version of a package\npip install \\&lt;package\\&gt;\n\n# install a specific version, note the double equals sign\npip install \\&lt;package\\&gt;==version\n\n# install a list of packages from a text file\npip install -r packages.txt\n</code></pre>"},{"location":"workflow_solutions/using_anaconda/#finding-packages","title":"Finding Packages","text":"<p>You may use the Anaconda page to search for packages on Anaconda, or use Google with something like <code>&lt;package name&gt; conda</code>. To find packages in PyPI, either use the PyPI page to search, or use Google with something like <code>&lt;package name&gt; pip</code>.</p>"},{"location":"workflow_solutions/using_anaconda/#packages-for-jupyter","title":"Packages for Jupyter","text":"<p>For more information about using Anaconda with Jupyter, see the section Working with Anaconda Environments.</p>"},{"location":"workflow_solutions/using_anaconda/#update-packages-in-an-environment","title":"Update packages in an environment","text":"<p>To ensure packages and their dependencies are all up to date, it is a best practice to regularly update installed packages, and libraries in your activated environment.</p> <pre><code>conda update -\u2014all\n</code></pre>"},{"location":"workflow_solutions/using_anaconda/#deactivating-an-environment","title":"Deactivating an Environment","text":"<p>An environment can be deactivated using the following command.</p> <pre><code># Using conda\nconda deactivate\n</code></pre> <p>Anaconda may say that using <code>source deactivate</code> is deprecated, but environment will still be deactivated.</p> <p>Closing the terminal will also close out the environment.</p>"},{"location":"workflow_solutions/using_anaconda/#deleting-an-environment","title":"Deleting an Environment","text":"<p>To delete an environment, use the following command. Remember to replace <code>&lt;env&gt;</code> with the existing environment name.</p> <pre><code>conda env remove \u2014-name &lt;env&gt;\n</code></pre>"},{"location":"workflow_solutions/using_anaconda/#working-with-environment-yaml-files","title":"Working with Environment YAML Files","text":""},{"location":"workflow_solutions/using_anaconda/#exporting-an-environment","title":"Exporting an Environment","text":"<p>To easily share environments with other researchers or replicate it on a new machine, it is useful to create an environment YAML file. You can do this using:</p> <pre><code># activate the environment if it is not active already\nconda activate &lt;env&gt;\n\n# export the environment to a YAML file\nconda env export &gt; env.yml\n</code></pre>"},{"location":"workflow_solutions/using_anaconda/#creating-an-environment-from-a-yaml-file","title":"Creating an Environment from a YAML File","text":"<p>To create an environment from a YAML file <code>env.yml</code>, use the following command.</p> <pre><code>conda env create --file env.yml\n</code></pre>"},{"location":"workflow_solutions/using_anaconda/#sharing-your-environment-file","title":"Sharing your environment file","text":"<p>To share your environment for collaboration, there are primarily 3 ways to export environments, the below commands show how to create environment files that can be shared for replication. Remember to replace <code>&lt;env&gt;</code> with the existing environment name.</p> <ol> <li> <p>Cross-Platform Compatible</p> <pre><code>conda env export --from-history &gt; &lt;env&gt;.yml\n</code></pre> </li> <li> <p>Platform + Package Specific</p> <p>Create .yml file to share, replace <code>&lt;envname&gt;</code> (represents the name of your environment) and <code>&lt;env&gt;</code> (represents the name of the file you want to export) with preferred names for file.</p> <pre><code>conda env export &lt;envname&gt; &gt; &lt;env&gt;.yml\n</code></pre> </li> <li> <p>Platform + Package + Channel Specific</p> <pre><code>conda list \u2014-explicit &gt; &lt;env&gt;.txt\n# OR\nconda list \u2014-explicit &gt; &lt;env&gt;.yml\n</code></pre> </li> </ol>"},{"location":"workflow_solutions/using_anaconda/#replicability-versus-portability","title":"Replicability versus Portability","text":"<p>An environment with only <code>python 3.10.4</code>, <code>numpy 1.21.5</code> and <code>jinja2 2.11.2</code> installed will output something like the following file when <code>conda env export</code> is used. This file may be used to precisely replicate the environment as it exists on the machine where <code>conda env export</code> was run. Note that the versioning for each package contains two <code>=</code> signs. The code like <code>he774522_0</code> after the second <code>=</code> sign contains hyper-specific build information for the compiled libraries for that package. Sharing this exact file with collaborators may result in frustration if they do not have the exact same operating system and hardware as you, and they would not be able to build this environment. We would say that this environment file is not very portable.</p> <p>There are other portability issues:</p> <ul> <li>The <code>prefix: C:\\...</code> line is not used by <code>conda</code> in any way and is deprecated. It also shares system information about file locations which is potentially sensitive information.</li> <li>The <code>channels:</code> group uses <code>- defaults</code>, which may vary depending on how you or your collaborator has customized their Anaconda installation. It may result in packages not being found, resulting in environment creation failure.</li> </ul> <pre><code>name: test-env\nchannels:\n  - defaults\ndependencies:\n  - blas=1.0=mkl\n  - bzip2=1.0.8=he774522_0\n  - ca-certificates=2022.4.26=haa95532_0\n  - certifi=2021.5.30=py310haa95532_0\n  - intel-openmp=2021.4.0=haa95532_3556\n  - jinja2=2.11.2=pyhd3eb1b0_0\n  - libffi=3.4.2=h604cdb4_1\n  - markupsafe=2.1.1=py310h2bbff1b_0\n  - mkl=2021.4.0=haa95532_640\n  - mkl-service=2.4.0=py310h2bbff1b_0\n  - mkl_fft=1.3.1=py310ha0764ea_0\n  - mkl_random=1.2.2=py310h4ed8f06_0\n  - numpy=1.21.5=py310h6d2d95c_2\n  - numpy-base=1.21.5=py310h206c741_2\n  - openssl=1.1.1o=h2bbff1b_0\n  - pip=21.2.4=py310haa95532_0\n  - python=3.10.4=hbb2ffb3_0\n  - setuptools=61.2.0=py310haa95532_0\n  - six=1.16.0=pyhd3eb1b0_1\n  - sqlite=3.38.3=h2bbff1b_0\n  - tk=8.6.11=h2bbff1b_1\n  - tzdata=2022a=hda174b7_0\n  - vc=14.2=h21ff451_1\n  - vs2015_runtime=14.27.29016=h5e58377_2\n  - wheel=0.37.1=pyhd3eb1b0_0\n  - wincertstore=0.2=py310haa95532_2\n  - xz=5.2.5=h8cc25b3_1\n  - zlib=1.2.12=h8cc25b3_2\nprefix: C:\\Users\\user\\Anaconda3\\envs\\test-env\n</code></pre> <p>To make this a more portable file, suitable for collaboration, some planning is required. Instead of using <code>conda env export</code> we can build our own file. Create a new file called <code>env.yml</code> using your favorite text editor and add the following. Note we've only listed exactly the packages we installed, and their version numbers, only. This allows Anaconda the flexibility to choose dependencies which do not conflict and do not contain unusable hyper-specific library build information.</p> <pre><code>name: test-env\nchannels:\n  - anaconda\ndependencies:\n  - jinja2=2.11.2\n  - numpy=1.21.5\n  - python=3.10.4\n</code></pre> <p>This is a much more readable and portable file suitable for sharing with collaborators. We aren't quite finished though! Some scientific packages on the <code>conda-forge</code> channel, and on other channels, can contain dependency errors. Those packages may accidentally pull a version of a dependency that breaks their code.</p> <p>For example, the package <code>markupsafe</code> made a not-backward-compatible change (a breaking change) to their code between <code>2.0.1</code> and <code>2.1.1</code>. Dependent packages expected <code>2.1.1</code> to be backward compatible, so their packages allowed <code>2.1.1</code> as a substitute for <code>2.0.1</code>. Since Anaconda chooses the most recent version allowable, package installs broke. To work around this for our environment, we would need to modify the environment to \"pin\" that package at a specific version, even though we didn't explicitly install it.</p> <pre><code>name: test-env\nchannels:\n  - anaconda\ndependencies:\n  - jinja2=2.11.2\n  - markupsafe=2.0.1\n  - numpy=1.21.5\n  - python=3.10.4\n</code></pre> <p>Now we can be sure that the correct versions of the software will be installed on our collaborator's machines.</p> <p>Note</p> <p>The example above is provided only for illustration purposes. The error has since been fixed, but the example above really happened and is helpful to explain version pinning.</p>"},{"location":"workflow_solutions/using_anaconda/#good-practice-for-finding-software-packages-on-anaconda","title":"Good Practice for Finding Software Packages on Anaconda","text":"<p>Finding Anaconda software packages involves searching through the available \u201cChannels\u201d and repositories to locate the specific packages that contain functions that you need for your environment. Channels are Anaconda's way of organizing packages. Channels instruct Anaconda where to look for packages when installation is to be done. The following are Anaconda Channels that are readily used to house majority of the packages used in scientific research. Anaconda, Conda-Forge, BioConda, other Channels also exist. If you want more information on Anaconda Channels please see their docs.</p> <p>In the sections below, you will see information on how to find key packages you intend to use, ensure the packages are up-to-date, figure out the best way to install them, and finally compose an environment file for portability and replicability.</p>"},{"location":"workflow_solutions/using_anaconda/#step-by-step-guide-to-finding-anaconda-software-packages","title":"Step-by-Step Guide to Finding Anaconda Software Packages","text":"<p>If we find the package at one of the Channel sources mentioned above, we can check the Platform version to ensure it is either\u00a0\"noarch\"\u00a0(if available) or\u00a0linux. After noting the version, we can click the \"source\" or \"repo\" link (if available) or \"homepage\". Then we try to find the latest version. For a package found on GitHub, click \"Releases\" on the right-hand side. Verify that the latest Release is the same as, or very close to, the version on Anaconda or PyPI. If so, the package is being maintained on Anaconda/PyPI and suitable for use. Note the exact software name, version, and channel (if not on PyPI). We prefer searching using the following methods, and usually have the most success in the order listed below.</p> <ul> <li> <p>Using Google: You may already be familiar with the exact Anaconda package name you require. In the event this is not the case, a simple web engine search with key words usually finds the package. For example, a web search for an Anaconda package would be something along the lines of \u201cAnaconda package for <code>Generic Topic Name</code>\u201d. Your search results, should return popular package names related to the topic you have searched for. In the sections below, there is an attempt to provide a detailed step-by-step guide on how to find Anaconda packages using \u201cnumpy\u201d as an example.</p> </li> <li> <p>Anaconda Cloud: Anaconda Cloud is the primary source for finding Anaconda packages. You can visit Anaconda Cloud and use the search bar to find the package you need. For example, when you get the package name from your web search (using numpy). You will enter name of the package in the search bar as shown below.</p> </li> </ul> <p></p> <p>Review results of your search, it is advised to use \u201cArtifacts\u201d that are compatible with the platform you are working with, as well as have the most \u201cFavorites\u201d and \u201cDownloads\u201d numbers. Click on the portion that contains the name of the package (highlighted 3 in the image below). 1 highlights the Artifact, Favorite and Downloads numbers, the selection 2 highlights the Channel where this package is stored.</p> <p></p> <p>Follow the installation instructions you see in the image below.</p> <p></p> <ul> <li>Using the Conda Search Command: You can use the <code>conda search &lt;package_name&gt;</code> command directly in your terminal to find packages. Replace <code>&lt;package_name&gt;</code> with the package you would like to search for. To do this on Cheaha, make sure to <code>module load Anaconda3</code> first, and follow the instructions to activate an environment. Then do <code>conda search numpy</code>. You should get a long list of numpy packages. Review this output, but take note of the highlighted portions in the image. The section with a red selection shows the numpy versions that are available, The section with a blue selection shows the channel where each numpy version is stored.</li> </ul> <p></p> <p>You can then install numpy with a specific version and from a specific channel with.</p> <pre><code>    conda install -c conda-forge numpy=2.0.0rc2\n</code></pre> <ul> <li> <p>Using Specific Channels: You can also get packages using specific Anaconda Channels listed below.</p> <ul> <li> <p>Anaconda Main Channel: The default channel provided by Anaconda, Inc. Visit Anaconda</p> </li> <li> <p>Conda-Forge: A community-driven channel with a wide variety of packages.Visit Conda-Forge</p> </li> <li> <p>Bioconda: A channel specifically for bioinformatics packages. Visit Bioconda</p> </li> </ul> </li> </ul> <p>You can specify a channel in your search, and it will show you a list of the packages available in that channel using <code>conda search -c &lt;channel_name&gt; &lt;package_name&gt;</code>, remember to replace  and  with the channel and package names you are searching for respectively. <pre><code>    conda search -c conda-forge numpy\n</code></pre> <p>If we find the package at one of these sources, we check the Platform version to ensure it is either\u00a0noarch\u00a0(if available) or\u00a0linux for it to work on Cheaha (\"noarch\" is usually preferred for the sake of portability). Noting the version, we can click the \"source\" or \"repo\" link (if available) or \"homepage\". Then we try to find the latest version. For a package found on GitHub, click \"Releases\" on the right-hand side. Verify that the latest Release is the same as, or very close to, the version on Anaconda or PyPI. If so, the package is being maintained on Anaconda/PyPI and suitable for use. Note the exact software name, version, and channel (if not on PyPI).</p> <p></p> <p>If we don't find a package using Google, or the Anaconda/PyPI pages are out of date, then it may become very hard to use the software in an Anaconda environment. It is possible to try installing a git repository using\u00a0pip, but care must be taken to choose the right commit or tag. You can find more info here. To search for a git repository try:</p> <ol> <li>github \"name\".</li> <li>gitlab \"name\".</li> </ol> <p>Remember to replace name with name of Anaconda package.</p> <p>Note</p> <p>There are issues with out-of-date software. It may have bugs that have since been fixed and so makes for less reproducible science. Documentation may be harder to find if it isn't also matched to the software version. Examining the README.md file for instructions may provide some good information on installing the package. You can also reach out to us for support in installing a package.</p> <p>When we have a complete list of Anaconda packages and Channels, then we can create an environment from scratch with all the dependencies included. For Anaconda packages, add one line to\u00a0dependencies\u00a0for each software. For PyPI packages add\u00a0- pip:\u00a0under\u00a0dependencies. Then under - pip:add <code>==</code> to pin the version, see below. The advantage to using an environment file is that it can be stored with your project in GitHub or GitLab, giving it all the benefits of version control.</p> <pre><code>name: test-env\ndependencies:\n  - anaconda::matplotlib=3.8.4  # Pinned version from anaconda channel\n  - conda-forge::python=3.10.4  # Pinned version from conda-forge channel\n  - pip\n  - pip:\n    - numpy==1.26.4  # Pinned version for pip\n    - git+https://github.com/user/repo.git  # Example of installing from a Git repo\n    - http://insert_package_link_here  # For URL links\n</code></pre> <p>For git repos, add them under <code>- pip:</code> based on examples here. See the section Replicability versus Portability for more information.</p> <p>The above configuration is only for illustration purposes, to show how channels and dependencies can be used. It is best to install all of your packages from conda channels, to avoid your environment breaking. Only packages that are unavailable via conda, should be installed via pip. If you run into challenges please contact us.</p>"},{"location":"workflow_solutions/using_anaconda/#key-things-to-remember","title":"Key Things To Remember","text":"<ol> <li> <p>Exploring Package Documentation: For each package, check the documentation to understand its features, version history, and compatibility. Documentation can often be found on the Anaconda Cloud package page under the \"Documentation\" or \"Homepage\" link shared above in this tutorial.</p> </li> <li> <p>Regularly consider updating your environment file to manage dependencies and maintain compatible software environments. Also newer software tends to resolve older bugs, consequently improving the state of science.</p> </li> <li> <p>Verify Package Version and Maintenance: Ensure you are getting the latest version of the package that is compatible with your environment. Verify that the package is actively maintained by checking the source repository (e.g., GitHub, GitLab). Look for recent commits, releases, and issue resolutions. The concepts of version pinning and semantic versioning, explain this in detail.</p> </li> </ol>"},{"location":"workflow_solutions/using_anaconda/#version-pinning","title":"Version Pinning","text":"<p>Version pinning in Anaconda environments involves specifying exact versions of packages to ensure consistency and compatibility. This practice is crucial for reproducibility, as it allows environments to be reproduced exactly, a critical component in research and collaborative projects. Version pinning also aids stability, by preventing unexpected changes that could break your environment, code or analysis. This practice also maintains compatibility between different packages that rely on specific dependencies. To implement version pinning, you can create a YAML file that lists the exact versions of all installed packages or specify versions directly when creating or updating environments using Conda commands.</p>"},{"location":"workflow_solutions/using_anaconda/#semantic-versioning","title":"Semantic Versioning","text":"<p>Semantic versioning is a versioning scheme using a three-part format (MAJOR.MINOR.PATCH) to convey the significance of changes in a software package. In Anaconda environments, it plays a role in managing compatibility, version pinning, dependency resolution, and updating packages. The MAJOR version indicates incompatible API changes, i.e. same software package but operation and interaction are mostly different from what you are accustomed to in the previous version. The MINOR version adds backward-compatible functionality, i.e. same version of software package but now contains new features and functionality. Operations and interactions are still mostly the same. While PATCH version includes backward-compatible bug fixes, i.e. same major and minor versions now have a slight change, perhaps a bug or some small change, still same features, operations and interactions, just the minor bug fix. Using semantic versioning helps maintain consistency and compatibility by ensuring that updates within the same major version are compatible, and by allowing precise control when specifying package versions.</p> <p>In practice, updating a Major version of a package may break your workflow, but may increase software reliability, stability and fix bugs affecting your science. Changing the major version may also introduce new bugs, these concerns and some others are some of the tradeoffs that have to be taken into consideration. Semantic versioning helps with managing Anaconda environments by facilitating precise version pinning and dependency resolution. For instance, you can pin specific versions using Conda commands or specify version ranges to ensure compatibility as shown in the examples above. Semantic versioning also informs upgrade strategies, letting us know when to upgrade packages based on the potential impact of changes. By leveraging semantic versioning, you can maintain stable and consistent environments, which is essential for smooth research workflows.</p>"},{"location":"workflow_solutions/using_anaconda/#good-software-development-practice","title":"Good Software Development Practice","text":"<p>Building on the example above, we can bring in good software development practices to ensure we don't lose track of how our environment is changing as we develop our software or our workflows. If you've ever lost a lot of hard work by accidentally deleting an important file, or forgetting what changes you've made that need to be rolled back, this section is for you.</p> <p>Efficient software developers live the mantra \"Don't repeat yourself\". Part of not repeating yourself is keeping a detailed and meticulous record of changes made as your software grows over time. Git is a way to have the computer keep track of those changes digitally. Git can be used to save changes to environment files as they change over time. Remember that each time your environment changes to commit the output of Exporting your Environment to a repository for your project.</p>"},{"location":"workflow_solutions/using_workflow_managers/","title":"Workflow Managers","text":"<p>Construction</p> <p>This page is a stub and is under construction.</p>"},{"location":"workflow_solutions/using_workflow_managers/#snakemake","title":"Snakemake","text":""},{"location":"workflow_solutions/using_workflow_managers/#pegasus","title":"Pegasus","text":""},{"location":"workflow_solutions/using_workflow_managers/#nextflow","title":"Nextflow","text":""}]}